{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "TIqpNgepFxVj",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Telco Customer Churn Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - (Manas Nayan Mukherjee) Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The **Telco Customer Churn Prediction** project is designed to address one of the most pressing challenges faced by telecom companies today: customer churn.\n",
        "\n",
        "**Customer churn**, or attrition, refers to the phenomenon where customers terminate their services, either by switching to a competitor or by discontinuing use altogether. For telecom companies, churn can lead to substantial revenue loss and a reduced customer base. Identifying customers at risk of churn and understanding the factors that contribute to it has thus become a key priority for businesses aiming to improve customer retention and ensure sustained growth.\n",
        "\n",
        "In this project, we aim to build a predictive model capable of identifying which customers are most likely to churn. By analyzing historical customer data, we seek to understand the underlying patterns and behaviors that lead to churn, such as customer demographics, service usage, payment methods, and interactions with customer service. The ultimate goal is to enable the telecom company to take proactive measures—such as targeted retention campaigns or personalized offers—to reduce churn and increase customer loyalty.\n",
        "\n",
        "The dataset used in this project consists of over **7,000 customer records** with various attributes such as demographics, account information, subscription details, and historical interaction data. Some of the key features in the dataset include customer tenure, contract type, monthly charges, payment methods, and reasons for churn. Additionally, the dataset also provides a churn label, which indicates whether a customer has left the company or remained.\n",
        "\n",
        "\n",
        "### Key Objectives\n",
        "\n",
        "1. **Data Exploration and Preprocessing**:\n",
        "   - The first phase of the project focuses on exploring the dataset and understanding its structure. This includes data cleaning to ensure there are no missing or inconsistent values and feature engineering to create new features that could improve model accuracy. The data preprocessing phase also involves encoding categorical variables, handling outliers, and scaling numerical features to ensure the models can learn effectively from the data.\n",
        "   \n",
        "2. **Data Visualization**:\n",
        "   - Visualization is crucial in uncovering hidden patterns in the data. A range of charts, including bar plots, histograms, and heatmaps, will be used to visualize relationships between various features and churn. The analysis will help in identifying critical factors that influence churn, such as **contract type**, **monthly charges**, and **tenure**. The correlation heatmap will reveal which variables have the strongest relationships with churn, providing insights that can guide business decisions.\n",
        "\n",
        "3. **Modeling and Evaluation**:\n",
        "   - Various machine learning models, such as **Logistic Regression**, **Random Forest Classifier**, and **XGBoost**, will be implemented to predict customer churn. These models will be evaluated based on performance metrics like **accuracy**, **precision**, **recall**, and **F1 score** to identify the most effective model for this particular dataset. Cross-validation techniques will be employed to ensure that the models generalize well to unseen data.\n",
        "\n",
        "4. **Hypothesis Testing**:\n",
        "   - In this phase, hypotheses based on insights from data exploration and visualization will be tested using statistical methods. For instance, we may hypothesize that customers with month-to-month contracts are more likely to churn, or that customers with high monthly charges are at a higher risk of leaving. Appropriate statistical tests (like **Z-tests** and **t-tests**) will be conducted to validate or reject these hypotheses. For example:\n",
        "      - **Hypothesis Testing 1**: A Z-Test was used to obtain the P-Value. Based on the results, the Null Hypothesis was rejected, indicating that customers who are not churning do not have an average monthly charge of $80 or more. This suggests that even though these customers are not churning, their average monthly charges are below $80, possibly pointing to pricing preferences or service selections that align with lower costs.\n",
        "      - **Hypothetical Statement 2**: A Z-Test was used to obtain the P-value. The Z-Test was chosen because the sample size for the senior citizen group is sufficiently large, and the data distribution is approximately normal. Based on the Z-score calculated, the P-value was derived, leading to the rejection of the null hypothesis. This indicates that the churn rate for senior citizens is significantly higher than the hypothesized 30%.\n",
        "      - **Hypothetical Statement 3**: A T-Test was used to obtain the P-value. The T-Test is suitable when comparing the sample mean to a population mean, especially when the data is not perfectly normal or when the sample size is moderate. In this analysis, the churn rate for customers with a tenure greater than 40 months was compared to the hypothesized population mean churn rate of 25%. The T-Test helped determine whether there is a significant difference between the observed sample mean and the hypothesized population mean, leading to the rejection of the null hypothesis.\n",
        "\n",
        "5. **Feature Engineering**:\n",
        "   - Feature engineering is an essential step to enhance the performance of machine learning models. The project will explore different techniques to handle missing values, encode categorical features, and scale numerical data. However, it is important to note that **Principal Component Analysis (PCA)** is generally used for dimensionality reduction when there are a large number of features. If our dataset has a manageable number of features, PCA may not be necessary and could be excluded from the feature engineering process.\n",
        "\n",
        "6. **Handling Imbalanced Data**:\n",
        "   - One of the key challenges in churn prediction is the **class imbalance** in the dataset, where the number of non-churning customers typically outweighs the number of churning customers. Several techniques, such as **SMOTE** (Synthetic Minority Over-sampling Technique), will be used to balance the dataset and ensure that the models can effectively predict both classes.\n",
        "\n",
        "7. **Business Impact**:\n",
        "   - The insights derived from this analysis will provide telecom companies with actionable strategies to minimize churn. By predicting customer behavior, companies can launch targeted retention efforts for high-risk customers, improve customer service, offer personalized pricing, and ultimately increase the customer lifetime value (CLTV). These strategies not only help in retaining valuable customers but also contribute to long-term profitability.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This project seeks to apply advanced machine learning techniques and data analytics to solve a critical business problem in the telecom industry. By predicting churn and identifying its key drivers, the model can empower businesses to take proactive measures to retain customers, enhance service offerings, and reduce revenue loss. Through this approach, telecom companies can shift from a reactive customer retention strategy to a more proactive and data-driven approach, ultimately leading to improved customer satisfaction and business outcomes.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Problem Overview**\n",
        "\n",
        "\n",
        "In the competitive telecom industry, customer churn poses a significant challenge, with annual churn rates often reaching 15-25%. Customers have the freedom to choose from multiple service providers, making it critical for companies to retain their customer base. Studies show that acquiring new customers is 5-10 times more expensive than retaining existing ones, emphasizing the importance of customer retention strategies.\n",
        "\n",
        "For a fictional telco company operating in California, understanding and predicting customer churn is essential to maintaining profitability and ensuring sustained growth. By analyzing churn patterns and identifying the factors influencing customer behavior, the company can take proactive measures to reduce churn rates and improve customer satisfaction."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objective**\n",
        "\n",
        "This project focuses on analyzing customer behavior and identifying patterns that lead to churn, enabling companies to implement proactive retention strategies.\n",
        "\n",
        "The analysis of the Telco Customer Churn dataset highlights some key features that can be great tools to help businesses retain valuable customers.\n",
        "\n",
        "We use parameters like Tenure Months, Churn Values, Churn Labels, Total Charges, Monthly Charges, Payment Reasons, and CLTV (Customer Lifetime Value) to gain deep insights behind the reasons for customer churn, allowing businesses to develop data-driven strategies to better retain customers.\n"
      ],
      "metadata": {
        "id": "J5gthOedAt27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information\n",
        "\n",
        "A fictional telco company that provided home phone and Internet services to 7043 customers in California in Q3.\n",
        "\n",
        "This dataset is detailed in: [Telco Customer Churn](https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2019/07/11/telco-customer-churn-1113)\n",
        "\n",
        "Downloaded from: [IBM Data and AI Accelerators](https://community.ibm.com/accelerators/?context=analytics&query=telco%20churn&type=Data&product=Cognos%20Analytics)\n",
        "\n",
        "There are several related datasets as documented in: [Base Samples for IBM Cognos Analytics](https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2018/09/12/base-samples-for-ibm-cognos-analytics)\n"
      ],
      "metadata": {
        "id": "VfdPSpgKBRwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Tips** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Entire project has well-structured, formatted, and commented code which makes it more readable.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code is there in the project for a user friendly appearance.     \n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic have proper comments so that any one can read and understand it properly.\n",
        "\n",
        "4. You may add as many number of charts you want but I have used 15 charts and for every chart used the following format.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```      \n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "*   Will the gained insights help creating a positive business impact?\n",
        "*   Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "[ Important : - I have done the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        "]\n",
        "\n",
        "5. Further added ml algorithms for model creation. Making each and every algorithm, in the following format:\n",
        "\n",
        "*   Explained the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "*   Elavulation of improvement? Noted down the improvement with updates Evaluation metric Score Chart.\n",
        "*   Explained each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**\n",
        "\n",
        "Let's gather all the essential tools and libraries we'll need for our **Telco Customer Churn Prediction** project. Each library has its **unique strengths**, and together they'll help us **analyze data**, **visualize patterns**, and build **robust predictive models**."
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pandas: Used for data manipulation and analysis, such as reading datasets, data cleaning, and exploratory data analysis (EDA).\n",
        "import pandas as pd  # Importing Pandas to handle and analyze data effectively\n",
        "\n",
        "# 2. NumPy: Used for numerical computations, array manipulation, and mathematical operations.\n",
        "import numpy as np  # Importing NumPy for efficient numerical operations\n",
        "\n",
        "# 3. Matplotlib: A plotting library for creating static, animated, and interactive visualizations. Used to create basic plots like histograms, bar charts, and scatter plots.\n",
        "import matplotlib.pyplot as plt  # Importing Matplotlib for basic plotting and visualizations\n",
        "\n",
        "# 4. Seaborn: A data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
        "import seaborn as sns  # Importing Seaborn for more advanced and attractive visualizations\n",
        "\n",
        "# 5. Scikit-learn: A machine learning library that includes tools for model training, evaluation, and preprocessing.\n",
        "from sklearn.model_selection import train_test_split  # Used for splitting the data into training and testing sets\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder  # Used for feature scaling and encoding categorical variables\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Used for evaluating model performance\n",
        "from sklearn.ensemble import RandomForestClassifier  # Example machine learning model for churn prediction\n",
        "from sklearn.linear_model import LogisticRegression  # Example model for churn prediction\n",
        "from sklearn.metrics import roc_auc_score  # For evaluating model performance using AUC (Area Under the Curve)\n",
        "from sklearn.feature_selection import VarianceThreshold  # Used to remove features with low variance that are not likely to contribute to the predictive power of the model\n",
        "from sklearn.preprocessing import OneHotEncoder  # Used to convert categorical variables into a format that can be provided to machine learning algorithms to do a better job in prediction\n",
        "from sklearn.compose import ColumnTransformer  # Used to apply different preprocessing steps to different subsets of features within a dataset\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold  # Used for hyperparameter tuning and repeated cross-validation\n",
        "from sklearn.model_selection import cross_val_score  # Returns an array of evaluation metrics (e.g., accuracy, precision, recall) computed for each CV fold\n",
        "from sklearn import metrics  # Used to evaluate machine learning models, Implement scores, losses, and utility functions and to quantify prediction quality\n",
        "from sklearn.model_selection import ParameterGrid  # Used to generate all possible combinations of hyperparameters. It's useful for performing exhaustive grid search over specified parameter values\n",
        "\n",
        "# 6. XGBoost: A high-performance machine learning library for gradient boosting, used for classification tasks, and often provides better performance on structured data.\n",
        "import xgboost as xgb  # Importing XGBoost for powerful gradient boosting models\n",
        "from xgboost import XGBClassifier  # Specifically importing the XGBClassifier for classification tasks\n",
        "\n",
        "# 7. Statsmodels: A library used for statistical models and hypothesis testing. It helps with regression analysis, testing for statistical significance, and more.\n",
        "import statsmodels.api as sm  # Importing Statsmodels for statistical analysis and hypothesis testing\n",
        "\n",
        "# 8. Imbalanced-learn: A library that provides tools for handling imbalanced datasets, such as oversampling, undersampling, and generating synthetic data.\n",
        "from imblearn.over_sampling import SMOTE  # Used for handling imbalanced datasets by oversampling the minority class\n",
        "\n",
        "# 9. TensorFlow/Keras (Optional): Libraries for deep learning models, if you decide to use neural networks for churn prediction.\n",
        "import tensorflow as tf  # Importing TensorFlow for deep learning models\n",
        "from tensorflow.keras.models import Sequential  # Used to define a deep learning model\n",
        "from tensorflow.keras.layers import Dense  # Layers for creating a deep neural network\n",
        "\n",
        "# 10. Plotly (Optional): A visualization library for interactive plots, used to create dashboards or advanced visualizations.\n",
        "import plotly.express as px  # Used for creating interactive visualizations\n",
        "\n",
        "# 11. Missingno (Optional): A visualization library to understand missing values in the dataset.\n",
        "import missingno as msno  # Used for visualizing the pattern of missing data\n",
        "\n",
        "# 12. Statsmodels: Provides functions for calculating Variance Inflation Factor (VIF) to detect multicollinearity in a set of features.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  # Calculating VIF to detect multicollinearity\n",
        "\n",
        "# 13. Shap: (SHapley Additive exPlanations) is used for model interpretability.\n",
        "import shap  # Importing SHAP for explaining machine learning model outputs\n",
        "!pip install shap  # Installing the SHAP library in your Python environment\n",
        "\n",
        "# 14. SciPy: A library used for scientific and technical computing. It helps with statistical calculations and optimization tasks.\n",
        "from scipy import stats  # Used for statistical tests and calculations\n",
        "\n",
        "# Importing the math module for mathematical calculations such as square root and power functions\n",
        "import math  # Performing essential mathematical calculations\n",
        "\n",
        "# Importing the norm module from scipy.stats to handle normal distribution functions\n",
        "from scipy.stats import norm  # Calculating probabilities and critical values for the normal distribution\n",
        "\n",
        "# 15. Model Saving and Loading Libraries: Used for saving the trained machine learning models to files and loading them for future use\n",
        "import pickle  # The pickle module is used to serialize (save) the trained model object into a .pkl file format\n",
        "import joblib  # The joblib module provides a similar functionality but is more efficient for large numpy arrays"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting Excel to CSV**\n",
        "\n",
        "**(Optional, but a smart move!) CSV files are more universally accepted and easier to handle compared to Excel files. They load faster and make data processing a breeze. Let's convert that Excel file into a CSV format to streamline our work!**"
      ],
      "metadata": {
        "id": "5ap4WgnvObeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from an Excel file\n",
        "# We're using pandas to read the Excel file into a DataFrame\n",
        "excel_dataset = pd.read_excel('/content/Telco_customer_churn.xlsx')  # Load the Excel file into a DataFrame\n",
        "print(\"Excel file loaded successfully!\")  # Confirm that the Excel file has been loaded\n",
        "\n",
        "# Convert the DataFrame to a CSV file\n",
        "# Saving the DataFrame as a CSV file for easier data handling and quicker loading in future steps\n",
        "excel_dataset.to_csv('/content/Telco_customer_churn.csv', index=False)  # Save the DataFrame as a CSV file\n",
        "print(\"Excel file converted to CSV successfully!\")  # Confirm that the file has been converted and saved"
      ],
      "metadata": {
        "id": "H6_gVjbssy6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Loading**\n",
        "\n",
        "**Let's continue our journey by loading the dataset we converted to CSV. This step is crucial as it brings our data into the workspace, ready for exploration and analysis.**"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the CSV file\n",
        "# Using pandas to read the CSV file into a DataFrame\n",
        "dataset = pd.read_csv('/content/Telco_customer_churn.csv')  # Read the CSV file into a DataFrame\n",
        "\n",
        "# Confirm that the dataset has been imported successfully\n",
        "print(\"Dataset imported successfully!\")  # Inform that the dataset has been successfully loaded"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset First View**\n",
        "\n",
        "**Now, let's take a peek at the first few rows of our dataset. This will give us a quick snapshot of its structure and contents, helping us understand what we're working with.**"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "# This step helps you get an initial understanding of the structure and contents of the dataset\n",
        "print(\"First five rows of the dataset:\")  # Inform that we are displaying the first few rows\n",
        "dataset_head = dataset.head()  # Get the first five rows of the dataset\n",
        "display(dataset_head)  # Display the first five rows of the dataset"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "1. **Customer Demographics and Location:**\n",
        "   - The dataset includes details like `CustomerID`, `Gender`, and location information (`Country`, `State`, `City`, `Zip Code`, `Latitude`, `Longitude`).\n",
        "   - All customers in this sample are from Los Angeles, California.\n",
        "\n",
        "2. **Subscription and Billing Information:**\n",
        "   - The `Contract` type for these customers is \"Month-to-month\".\n",
        "   - All customers use `Paperless Billing`.\n",
        "   - Various `Payment Methods` are used, including \"Mailed check\", \"Electronic check\", and \"Bank transfer (automatic)\".\n",
        "\n",
        "3. **Financial Details:**\n",
        "   - `Monthly Charges` and `Total Charges` vary among customers, indicating different usage patterns and service levels.\n",
        "   - `CLTV` (Customer Lifetime Value) estimates the long-term value of each customer.\n",
        "\n",
        "4. **Churn Information:**\n",
        "   - The `Churn Label` and `Churn Value` show that all customers in this sample have churned.\n",
        "   - `Churn Score` and `Churn Reason` provide insights into the likelihood and reasons for customer churn. Some reasons include \"Competitor made better offer\", \"Moved\", and \"Competitor had better devices\".\n",
        "\n",
        "These initial observations help us understand the dataset's structure and the type of information it contains. This data is crucial for analyzing customer churn and developing strategies to retain customers."
      ],
      "metadata": {
        "id": "E-FomKEIu2Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Rows & Columns count**\n",
        "\n",
        "**Let's determine the size and dimensionality of our dataset. Knowing the number of rows and columns gives us a sense of the scale we’re working with.**"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of rows and columns in the dataset\n",
        "# Using the shape attribute from pandas to understand the dataset's dimensions\n",
        "rows, columns = dataset.shape  # Get the number of rows and columns in the dataset\n",
        "print(f\"The dataset contains {rows} rows and {columns} columns.\")  # Print the number of rows and columns"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Information**\n",
        "\n",
        "**Let's delve into a summary of our dataset. This step is essential as it provides a comprehensive overview, including the number of entries, column names, data types, and non-null values.**"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dataset information\n",
        "# Using the info() method to get a concise summary of the DataFrame\n",
        "dataset_info = dataset.info()  # Get the dataset summary, including data types and non-null values"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "1. **General Overview:**\n",
        "   - The dataset contains 7043 entries (rows) and 33 columns.\n",
        "   - The dataset is loaded into a pandas DataFrame.\n",
        "\n",
        "2. **Column Details:**\n",
        "   - The columns include a mix of data types:\n",
        "     - `object` (string-like data) for categorical features.\n",
        "     - `int64` and `float64` for numerical features.\n",
        "\n",
        "3. **Non-Null Values:**\n",
        "   - Most columns have 7043 non-null values, indicating no missing data in those columns.\n",
        "   - The `Churn Reason` column has significantly fewer non-null values (1869), suggesting many missing entries in this specific column.\n",
        "\n",
        "4. **Key Columns:**\n",
        "   - **Identification and Location:** Columns like `CustomerID`, `Zip Code`, `Latitude`, and `Longitude` provide unique identifiers and location details for each customer.\n",
        "   - **Demographics and Subscription:** Columns like `Gender`, `Senior Citizen`, `Partner`, and `Dependents` offer demographic details.\n",
        "   - **Service Details:** Columns like `Phone Service`, `Internet Service`, `Contract`, and `Payment Method` describe the services used by customers.\n",
        "   - **Financial Information:** Columns like `Monthly Charges`, `Total Charges`, and `CLTV` give insights into financial aspects.\n",
        "   - **Churn Information:** `Churn Label`, `Churn Value`, `Churn Score`, and `Churn Reason` are crucial for understanding customer churn behavior.\n",
        "\n",
        "This detailed overview gives a clear understanding of the data structure, which is essential for further analysis and model building.\n"
      ],
      "metadata": {
        "id": "fl2mbwNBxKiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Duplicate Values**\n",
        "\n",
        "**Next, let’s check for any duplicate entries in our dataset. Identifying and removing duplicate rows is essential to ensure the accuracy and quality of our data.**"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate entries in the dataset\n",
        "# Using the duplicated() method to find any duplicate rows\n",
        "duplicate_count = dataset.duplicated().sum()  # Count the number of duplicate rows in the dataset\n",
        "print(f\"The dataset contains {duplicate_count} duplicate entries.\")  # Print the count of duplicate entries"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Missing Values/Null Values**\n",
        "\n",
        "**Let's identify any missing or null values in our dataset. Knowing which columns have missing data helps us address these gaps and maintain the integrity of our analysis.**"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing or null values in the dataset\n",
        "# Using the isnull() method to find missing values and summing them up\n",
        "missing_values = dataset.isnull().sum()  # Count the number of missing values in each column\n",
        "print(\"Missing values in each column:\")  # Inform that we are displaying missing values count\n",
        "print(missing_values)  # Print the count of missing values for each column"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "1. **General Overview:**\n",
        "   - The dataset contains 7043 entries and 33 columns.\n",
        "\n",
        "2. **Missing Values:**\n",
        "   - Most columns have 0 missing values, indicating a high level of data completeness.\n",
        "   - The `Churn Reason` column has a significant number of missing values (5174 out of 7043), which suggests that many customers did not provide a reason for their churn.\n",
        "\n",
        "3. **Columns without Missing Values:**\n",
        "   - Key columns such as `CustomerID`, `Count`, `Country`, `State`, `City`, `Zip Code`, `Lat Long`, `Latitude`, `Longitude`, `Gender`, `Senior Citizen`, `Partner`, `Dependents`, `Tenure Months`, `Phone Service`, `Multiple Lines`, `Internet Service`, `Online Security`, `Online Backup`, `Device Protection`, `Tech Support`, `Streaming TV`, `Streaming Movies`, `Contract`, `Paperless Billing`, `Payment Method`, `Monthly Charges`, `Total Charges`, `Churn Label`, `Churn Value`, `Churn Score`, and `CLTV` do not have any missing values.\n",
        "\n",
        "This analysis ensures that most of your data is complete, but the missing values in the `Churn Reason` column will need to be addressed in your data preprocessing steps.\n"
      ],
      "metadata": {
        "id": "Bd4G-LgyxrIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Missing Values Using a Heatmap**\n",
        "\n",
        "**Let's create a visual representation of the missing values in our dataset. This will help us easily identify patterns and areas where data is missing.**"
      ],
      "metadata": {
        "id": "H_CHdwxhTcCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the missing values using a heatmap\n",
        "# This step helps in identifying the pattern of missing values across different columns\n",
        "\n",
        "plt.figure(figsize=(12, 6))  # Set the figure size for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "sns.heatmap(dataset.isnull(), cbar=False, cmap='viridis', yticklabels=False)  # Create a heatmap to visualize missing values\n",
        "# Parameters explained:\n",
        "# - dataset.isnull(): Generates a boolean DataFrame where True indicates a missing value\n",
        "# - cbar=False: Removes the color bar for simplicity\n",
        "# - cmap='viridis': Sets the color map to 'viridis' for better visualization\n",
        "# - yticklabels=False: Hides the y-axis labels for a cleaner look\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "plt.title('Heatmap of Missing Values in the Dataset')  # Add a title to the heatmap\n",
        "plt.show()  # Display the heatmap\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What did you know about your dataset?**"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset provided is sourced from IBM and represents a fictional telecommunications company in California that offers home phone and Internet services. Our objective is to analyze customer churn and uncover the insights behind it.\n",
        "\n",
        "### Understanding Churn Prediction\n",
        "\n",
        "Churn prediction involves analyzing the likelihood of a customer abandoning a product or service. The goal is to understand the factors leading to churn and take proactive measures to retain customers before they decide to leave.\n",
        "\n",
        "### Dataset Overview\n",
        "\n",
        "The dataset consists of **7043 rows** and **33 columns**. Here are some key observations:\n",
        "\n",
        "#### Data Completeness:\n",
        "- **No Duplicate Entries**: There are no duplicate entries in the dataset, ensuring that each row represents a unique customer record.\n",
        "- **High Level of Data Completeness**: Most columns do not have missing values, indicating a high level of data completeness.\n",
        "\n",
        "#### Visual Analysis of Missing Values:\n",
        "- **Heatmap Insights**: A heatmap visualization confirms that the dataset is mostly complete, except for the `Churn Reason` column, which has significant missing values (**5174 out of 7043**). This suggests that many customers did not provide a reason for their churn.\n",
        "\n",
        "These observations confirm the dataset's integrity, making it well-suited for further analysis and model building to understand and predict customer churn."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***\n",
        "\n",
        "**Let's dive into our dataset by identifying all the available variables and generating a statistical summary of the numeric columns. This will give us valuable insights into the data's distribution, central tendency, and spread.**"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the columns in the dataset\n",
        "# This step helps in identifying all the variables available for analysis\n",
        "\n",
        "dataset_columns = dataset.columns  # Get the column names from the dataset\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html\n",
        "\n",
        "print(\"Dataset Columns:\")  # Print a message indicating we are displaying the columns\n",
        "print(dataset_columns)  # Display the column names"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate a Statistical Summary**\n",
        "\n",
        "### **Let's gain insights into the distribution, central tendency, and spread of our numeric data by generating a statistical summary. This step helps us understand the key statistical properties of our dataset**"
      ],
      "metadata": {
        "id": "6Z35kGaPfN0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a statistical summary of the numeric columns in the dataset\n",
        "# This step provides insight into the distribution, central tendency, and spread of the data\n",
        "\n",
        "dataset_description = dataset.describe()  # Generate a summary of the numeric columns\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html\n",
        "\n",
        "print(\"Statistical Summary of the Dataset:\")  # Print a message indicating we are displaying the statistical summary\n",
        "print(dataset_description)  # Display the statistical summary"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "1. **General Overview:**\n",
        "   - The dataset contains 7043 entries and 33 columns.\n",
        "   \n",
        "2. **Count:**\n",
        "   - Each customer record is counted once, confirming no duplicate entries.\n",
        "\n",
        "3. **Geographic Data:**\n",
        "   - `Zip Code` ranges from 90001 to 96161.\n",
        "   - `Latitude` ranges from approximately 32.56 to 41.96 degrees.\n",
        "   - `Longitude` ranges from approximately -124.30 to -114.19 degrees.\n",
        "\n",
        "4. **Tenure Months:**\n",
        "   - The tenure ranges from 0 to 72 months, with a mean of approximately 32.37 months.\n",
        "   - The 25th, 50th, and 75th percentiles are 9, 29, and 55 months, respectively.\n",
        "\n",
        "5. **Monthly Charges:**\n",
        "   - Monthly charges range from 18.25 to 118.75 dollars, with a mean of approximately 64.76 dollars.\n",
        "   - The standard deviation is 30.09 dollars, indicating variability in customer bills.\n",
        "   \n",
        "6. **Churn Metrics:**\n",
        "   - `Churn Value`: This binary column indicates whether a customer has churned (1) or not (0).\n",
        "   - `Churn Score`: The scores range from 5 to 100, with a mean of approximately 58.70, indicating the likelihood of churn.\n",
        "   - `CLTV` (Customer Lifetime Value): Ranges from 2003 to 6500, with an average of around 4400.30. This metric estimates the total revenue a business can reasonably expect from a customer.\n",
        "\n",
        "These descriptive statistics provide a snapshot of the dataset's numeric columns, helping to understand the distribution, central tendency, and variability of the data. This is crucial for identifying patterns and anomalies that may influence customer churn.\n"
      ],
      "metadata": {
        "id": "qgJLKlcEzxb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variables Description**"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **CustomerID**: A unique ID that identifies each customer.\n",
        "- **Count**: A value used in reporting/dashboarding to sum up the number of customers in a filtered set.\n",
        "- **Country**: The country of the customer’s primary residence.\n",
        "- **State**: The state of the customer’s primary residence.\n",
        "- **City**: The city of the customer’s primary residence.\n",
        "- **Zip Code**: The zip code of the customer’s primary residence.\n",
        "- **Lat Long**: The combined latitude and longitude of the customer’s primary residence.\n",
        "- **Latitude**: The latitude of the customer’s primary residence.\n",
        "- **Longitude**: The longitude of the customer’s primary residence.\n",
        "- **Gender**: The customer’s gender: Male, Female.\n",
        "- **Senior Citizen**: Indicates if the customer is 65 or older: Yes, No.\n",
        "- **Partner**: Indicates if the customer has a partner: Yes, No.\n",
        "- **Dependents**: Indicates if the customer lives with any dependents: Yes, No. Dependents could be children, parents, grandparents, etc.\n",
        "- **Tenure Months**: Indicates the total number of months that the customer has been with the company by the end of the quarter specified above.\n",
        "- **Phone Service**: Indicates if the customer subscribes to home phone service with the company: Yes, No.\n",
        "- **Multiple Lines**: Indicates if the customer subscribes to multiple telephone lines with the company: Yes, No.\n",
        "- **Internet Service**: Indicates if the customer subscribes to Internet service with the company: No, DSL, Fiber Optic, Cable.\n",
        "- **Online Security**: Indicates if the customer subscribes to an additional online security service provided by the company: Yes, No.\n",
        "- **Online Backup**: Indicates if the customer subscribes to an additional online backup service provided by the company: Yes, No.\n",
        "- **Device Protection**: Indicates if the customer subscribes to an additional device protection plan for their Internet equipment provided by the company: Yes, No.\n",
        "- **Tech Support**: Indicates if the customer subscribes to an additional technical support plan from the company with reduced wait times: Yes, No.\n",
        "- **Streaming TV**: Indicates if the customer uses their Internet service to stream television programming from a third-party provider: Yes, No. The company does not charge an additional fee for this service.\n",
        "- **Streaming Movies**: Indicates if the customer uses their Internet service to stream movies from a third-party provider: Yes, No. The company does not charge an additional fee for this service.\n",
        "- **Contract**: Indicates the customer’s current contract type: Month-to-Month, One Year, Two Year.\n",
        "- **Paperless Billing**: Indicates if the customer has chosen paperless billing: Yes, No.\n",
        "- **Payment Method**: Indicates how the customer pays their bill: Bank Withdrawal, Credit Card, Mailed Check.\n",
        "- **Monthly Charges**: Indicates the customer’s current total monthly charge for all their services from the company.\n",
        "- **Total Charges**: Indicates the customer’s total charges, calculated to the end of the quarter specified above.\n",
        "- **Churn Label**: Yes = the customer left the company this quarter. No = the customer remained with the company. Directly related to Churn Value.\n",
        "- **Churn Value**: 1 = the customer left the company this quarter. 0 = the customer remained with the company. Directly related to Churn Label.\n",
        "- **Churn Score**: A value from 0-100 that is calculated using the predictive tool IBM SPSS Modeler. The model incorporates multiple factors known to cause churn. The higher the score, the more likely the customer will churn.\n",
        "- **CLTV**: Customer Lifetime Value. A predicted CLTV is calculated using corporate formulas and existing data. The higher the value, the more valuable the customer. High-value customers should be monitored for churn.\n",
        "- **Churn Reason**: A customer’s specific reason for leaving the company. Directly related to Churn Category.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check Unique Values for each variable**\n",
        "\n",
        "**Let's identify the diversity of values in each variable by checking for unique values in each column. This will give us insight into the variety within our dataset.**"
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for unique values in each column\n",
        "# Using the nunique() method to find the number of unique values in each column\n",
        "\n",
        "unique_counts = dataset.nunique()  # Count the number of unique values in each column\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html\n",
        "\n",
        "print(\"Unique values in each column:\")  # Inform that we are displaying the unique counts\n",
        "print(unique_counts)  # Display the unique counts for each column"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "- **CustomerID**: Each customer has a unique ID, with 7043 unique values.\n",
        "- **Count, Country, State**: These columns have only one unique value, which makes them constant across the dataset.\n",
        "- **City, Zip Code, Lat Long, Latitude, Longitude**: These columns have many unique values, reflecting the diverse geographic locations of customers.\n",
        "- **Gender, Senior Citizen, Partner, Dependents, Phone Service, Paperless Billing**: These columns have 2 unique values each, indicating binary categories.\n",
        "- **Multiple Lines, Internet Service, Online Security, Online Backup, Device Protection, Tech Support, Streaming TV, Streaming Movies, Contract**: These columns have 3 unique values each, representing various service options and customer preferences.\n",
        "- **Payment Method**: There are 4 unique values, showing different methods customers use to pay their bills.\n",
        "- **Tenure Months**: There are 73 unique values, indicating the variability in the duration of customer tenure.\n",
        "- **Monthly Charges**: There are 1585 unique values, reflecting the range of customer billing amounts.\n",
        "- **Total Charges**: With 6531 unique values, this column shows the diversity in the total amounts billed to customers over time.\n",
        "- **Churn Label, Churn Value**: These columns have 2 unique values each, representing whether a customer has churned or not.\n",
        "- **Churn Score**: There are 85 unique values, indicating different levels of churn risk scores assigned to customers.\n",
        "- **CLTV**: There are 3438 unique values, reflecting the variation in the predicted customer lifetime value.\n",
        "- **Churn Reason**: This column has 20 unique values, representing different reasons why customers might churn.\n",
        "\n",
        "**These insights help in understanding the diversity and distribution of values in each variable, which is crucial for further analysis and model building.**\n"
      ],
      "metadata": {
        "id": "47FW-_iq0age"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***\n",
        "\n",
        "### Understanding Data Wrangling\n",
        "\n",
        "#### What is Data Wrangling?\n",
        "Data wrangling, also known as data munging, is the process of cleaning, transforming, and organizing raw data into a usable format for analysis. This involves a series of steps including data cleaning, structuring, enriching, validating, and integrating data from multiple sources. The goal of data wrangling is to prepare the data for easy and effective analysis.\n",
        "\n",
        "#### Relevance of Data Wrangling\n",
        "- **Data Quality**: Ensures that the data is clean, consistent, and reliable, which is essential for accurate analysis.\n",
        "- **Efficiency**: Streamlines the data preparation process, saving time and effort in the long run.\n",
        "- **Data Integration**: Helps in combining data from various sources, providing a comprehensive view for analysis.\n",
        "- **Adaptability**: Makes data adaptable for various analytical techniques and tools, enhancing its usability.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Accuracy**: Properly wrangled data leads to more accurate and reliable models.\n",
        "- **Performance**: Improves the performance of machine learning models by providing high-quality, consistent data.\n",
        "- **Interpretability**: Clean and well-structured data makes it easier to interpret the results of the analysis.\n",
        "- **Robustness**: Enhances the robustness of models by eliminating noise and inconsistencies in the data.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Data Wrangling Techniques in Data Science](https://towardsdatascience.com/data-wrangling-techniques-for-data-scientists-50ca5e4f6d18).\n"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Wrangling Code**\n",
        "\n",
        "**Let's clean and prepare our dataset for analysis by converting data types, handling missing values, and dropping unnecessary columns.**"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'Total Charges' to numeric since it's currently an object\n",
        "# This conversion ensures that 'Total Charges' is treated as a numeric value for analysis\n",
        "dataset['Total Charges'] = pd.to_numeric(dataset['Total Charges'], errors='coerce')  # Convert 'Total Charges' to numeric\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html\n",
        "\n",
        "# Checking for missing values after conversion\n",
        "# This step helps to identify any missing values introduced during the conversion\n",
        "missing_values_after_conversion = dataset['Total Charges'].isnull().sum()  # Count missing values in 'Total Charges' after conversion\n",
        "print(f\"Missing values in 'Total Charges' after conversion: {missing_values_after_conversion}\")  # Print the count of missing values\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html\n",
        "\n",
        "# Imputing missing values in 'Total Charges' with the median\n",
        "# Using the median to fill missing values minimizes the impact of outliers and ensures data integrity\n",
        "dataset['Total Charges'].fillna(dataset['Total Charges'].median(), inplace=True)  # Fill missing values with the median\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
        "\n",
        "# Dropping unnecessary columns\n",
        "# Removing columns that are not relevant for the analysis to simplify the dataset\n",
        "columns_to_drop = ['CustomerID', 'Lat Long', 'Churn Reason']  # Specify columns to drop\n",
        "dataset_cleaned = dataset.drop(columns=columns_to_drop, axis=1)  # Drop the specified columns\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
        "\n",
        "# Verifying the dataset after cleaning\n",
        "# This step provides an overview of the dataset's structure after cleaning\n",
        "dataset_cleaned.info()  # Display the dataset information\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Insights from Data Wrangling**"
      ],
      "metadata": {
        "id": "noC1corEWPiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Insights from data wrangling:\")\n",
        "print(\"- The 'Total Charges' column was converted to numeric, and any missing values introduced during the conversion were replaced with the median.\")\n",
        "print(\"- Columns like 'CustomerID', 'Lat Long', and 'Churn Reason' were removed as they do not directly contribute to the analysis.\")"
      ],
      "metadata": {
        "id": "r_nl1jGvWNjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Manipulations Performed**\n",
        "\n",
        "#### Handling Missing Values:\n",
        "- **Total Charges**: Converted from object to numeric, with non-numeric entries treated as missing values and replaced with the median to maintain data integrity.\n",
        "\n",
        "#### Column Removal:\n",
        "- **CustomerID**: A unique identifier with no predictive value.\n",
        "- **Lat Long**: Spatial data irrelevant for churn analysis since `Latitude` and `Longitude` are already present separately.\n",
        "- **Churn Reason**: Available only for churned customers, thus not useful for predictive modeling due to significant missing values.\n",
        "\n",
        "#### Data Type Conversion:\n",
        "- **Total Charges**: Converted from object to float after handling non-numeric entries to ensure accurate numerical analysis.\n",
        "\n",
        "#### Categorical Encoding:\n",
        "- **Binary Columns** (e.g., `Gender`, `Senior Citizen`, `Partner`, `Dependents`): Encoded as 0 and 1.\n",
        "- **Multi-category Columns** (e.g., `Contract`, `Payment Method`, `Internet Service`): One-hot encoded for effective modeling.\n",
        "\n",
        "#### Outlier Analysis:\n",
        "- **Monthly Charges** and **Total Charges**: Checked for outliers; no extreme anomalies were identified.\n",
        "\n",
        "#### Verification Steps:\n",
        "- Ensured no duplicate records were present in the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Insights Found**\n",
        "\n",
        "#### High Churn Groups:\n",
        "- Customers with month-to-month contracts and electronic check payments are more prone to churn.\n",
        "#### Tenure Impact: - Short-tenure customers (<12 months) are at higher risk of churn, emphasizing the need for early engagement strategies.\n",
        "#### Service Gaps: - Lack of tech support or online security services correlates with higher churn rates.\n",
        "#### Financial Influence: - Higher `Monthly Charges` contribute to churn, particularly for customers utilizing multiple add-on services.\n",
        "#### Demographic Trends: - Senior citizens and customers with paperless billing exhibit slightly higher churn rates.\n",
        "\n",
        "These manipulations and insights provide a valuable foundation for building predictive models and strategizing interventions to reduce churn.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 ((Churn Percentage - Donut Chart))"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate churn percentage\n",
        "# Determine the number of churned vs. non-churned customers\n",
        "churn_counts = dataset['Churn Label'].value_counts()  # Count the number of occurrences of each class (churned and non-churned)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "labels = churn_counts.index  # Labels for the pie chart (churned and non-churned)\n",
        "sizes = churn_counts.values  # Values representing each slice of the pie (count of churned and non-churned)\n",
        "\n",
        "# Create a donut chart\n",
        "fig, ax = plt.subplots(figsize=(8, 6))  # Create a figure and a set of subplots, adjusting the size for clarity\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
        "\n",
        "ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor': 'white'})  # Create the pie chart\n",
        "# Parameters explained:\n",
        "# - sizes: The values representing each slice of the pie\n",
        "# - labels: Labels for the pie chart\n",
        "# - autopct: String used to label the pie slices with their numeric value\n",
        "# - startangle: The starting angle of the pie chart\n",
        "# - wedgeprops: Dictionary of arguments passed to the wedge objects to draw a white edge color\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html\n",
        "\n",
        "# Add a circle in the middle to transform the pie chart into a donut chart\n",
        "centre_circle = plt.Circle((0, 0), 0.70, fc='white')  # Create a white circle at the center for the donut effect\n",
        "fig.gca().add_artist(centre_circle)  # Add the center circle to the figure\n",
        "\n",
        "plt.title(\"Churn Percentage (Donut Chart)\")  # Add a title to the chart\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits neatly\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "plt.show()  # Display the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The donut chart is visually appealing and effectively communicates the proportion of churned vs. non-churned customers. It provides an easy way to understand the overall churn percentage at a glance."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that 26.5% of customers have churned, while 73.5% have not. This indicates a significant opportunity to reduce churn by focusing on the 26.5% of customers who are at risk of leaving."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the churn percentage helps prioritize customer retention strategies. If churn is high, businesses might implement loyalty programs or improve their services to retain customers.\n",
        "\n",
        "High churn rates signal potential revenue loss and customer dissatisfaction. This insight highlights areas needing immediate improvement, such as service quality or pricing."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 (Gender Distribution - Bar Chart)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate gender counts\n",
        "# Determine the number of male vs. female customers\n",
        "gender_counts = dataset['Gender'].value_counts()  # Count the number of male and female customers\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(8, 6))  # Create a figure and adjust the size for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "sns.barplot(x=gender_counts.index, y=gender_counts.values, palette='viridis')  # Create a bar plot with a specified color palette\n",
        "# Parameters explained:\n",
        "# - x: The x-axis labels (gender)\n",
        "# - y: The y-axis values (count of each gender)\n",
        "# - palette: Color palette for the bars\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
        "\n",
        "plt.title(\"Gender Distribution\")  # Add a title to the chart\n",
        "plt.xlabel(\"Gender\")  # Label the x-axis\n",
        "plt.ylabel(\"Count\")  # Label the y-axis\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "plt.show()  # Display the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is an effective way to visually compare the counts of different categories, in this case, gender. It clearly shows the distribution and allows for easy comparison between the two groups."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that the gender distribution in the dataset is relatively balanced, with a slightly higher number of males compared to females. This indicates that the dataset does not have a significant gender imbalance."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the balanced gender distribution helps ensure that any gender-specific strategies or decisions are based on a representative dataset. Marketing campaigns or product developments can be tailored to appeal to both genders equally.\n",
        "\n",
        "There are no insights from this chart that would lead to negative growth, as the balanced gender distribution suggests that the business can cater to a diverse audience without the risk of alienating one gender."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 (Churn Rate by Contract Type - Stacked Bar Chart)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for the stacked bar chart\n",
        "plt.figure(figsize=(10, 7))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Group the data by 'Contract' and 'Churn Label', and calculate the size of each group\n",
        "contract_churn = dataset.groupby(['Contract', 'Churn Label']).size().unstack()  # Group data and calculate group sizes\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html\n",
        "\n",
        "# Plot the stacked bar chart\n",
        "contract_churn.plot(kind='bar', stacked=True, color=['#66c2a5', '#fc8d62'], figsize=(10, 6))  # Create a stacked bar plot\n",
        "# Parameters explained:\n",
        "# - kind: Type of plot to generate\n",
        "# - stacked: Whether to stack the bars\n",
        "# - color: Color scheme for the bars\n",
        "# - figsize: Size of the figure\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Churn Rate by Contract Type\")  # Add a title to the chart\n",
        "plt.xlabel(\"Contract Type\")  # Label the x-axis\n",
        "plt.ylabel(\"Customer Count\")  # Label the y-axis\n",
        "\n",
        "# Rotate the x-ticks for better readability\n",
        "plt.xticks(rotation=0)  # Set the rotation of x-ticks to 0 degrees\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xticks.html\n",
        "\n",
        "# Add legend\n",
        "plt.legend(title=\"Churn Status\", labels=[\"No Churn\", \"Churn\"])  # Add a legend to differentiate churn status\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart is an excellent choice to compare the churn rate across different contract types.\n",
        "\n",
        "It visually emphasizes the proportion of customers who churn or stay for each contract type (Month-to-month, One year, Two year)."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution of churn across different contract types, helping to identify which contract types have higher churn rates. For example, month-to-month contracts might show a higher churn rate compared to one-year or two-year contracts."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the churn rate by contract type can inform targeted retention strategies. If certain contract types have higher churn rates, businesses can investigate the reasons and take specific actions to improve customer retention for those contracts.\n",
        "\n",
        "There are no insights from this chart that would lead to negative growth, as addressing high churn rates in specific contract types can help reduce overall churn and improve customer satisfaction."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 (Churn Rate by Payment Method - Horizontal Bar Chart)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for the horizontal bar chart\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Group the data by 'Payment Method' and calculate the mean churn rate, then sort the values\n",
        "payment_churn = dataset.groupby('Payment Method')['Churn Value'].mean().sort_values() * 100\n",
        "# Explanation: Group the data by 'Payment Method', calculate the mean churn rate for each group, sort the values, and convert to percentage\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.mean.html\n",
        "\n",
        "# Plot the horizontal bar chart\n",
        "payment_churn.plot(kind='barh', color='#8da0cb')  # Create a horizontal bar plot with a specified color\n",
        "# Parameters explained:\n",
        "# - kind: Type of plot to generate ('barh' for horizontal bar plot)\n",
        "# - color: Color of the bars\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Churn Rate by Payment Method\")  # Add a title to the chart\n",
        "plt.xlabel(\"Churn Rate (%)\")  # Label the x-axis\n",
        "plt.ylabel(\"Payment Method\")  # Label the y-axis\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart is an excellent choice for comparing the churn rates across different payment methods. It clearly shows the churn rate for each payment method and allows for easy comparison."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the churn rates for different payment methods, highlighting that electronic check payments have the highest churn rate (approximately 45%), while automatic bank transfers and credit cards have the lowest churn rates (approximately 10%)."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the churn rate by payment method can inform targeted interventions. If certain payment methods are associated with higher churn rates, businesses can investigate the reasons and take specific actions to address the issues. This can help reduce overall churn and improve customer satisfaction.\n",
        "\n",
        "There are no insights from this chart that would lead to negative growth, as addressing high churn rates for specific payment methods can help retain customers and enhance their experience."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 (Monthly Charges Distribution - Histogram)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for the histogram\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Plot the histogram with Kernel Density Estimate (KDE)\n",
        "sns.histplot(dataset['Monthly Charges'], bins=30, kde=True, color='#66c2a5')  # Create a histogram with 30 bins and a KDE line\n",
        "# Parameters explained:\n",
        "# - bins: Number of bins in the histogram\n",
        "# - kde: Whether to include a Kernel Density Estimate (KDE) line\n",
        "# - color: Color of the bars\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Distribution of Monthly Charges\")  # Add a title to the chart\n",
        "plt.xlabel(\"Monthly Charges\")  # Label the x-axis\n",
        "plt.ylabel(\"Frequency\")  # Label the y-axis\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is a great way to visualize the distribution of a continuous variable like monthly charges. It provides a clear view of the frequency of different charge ranges and helps identify patterns such as skewness or the presence of multiple peaks."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution of monthly charges among customers. The most frequent monthly charge is around 20, with a frequency of approximately 1200. There are several peaks and valleys in the distribution, indicating variability in the monthly charges, which range from around 20 to 120."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the distribution of monthly charges can help the business identify pricing trends and customer segments. For example, if many customers have higher charges, the company might explore premium services or products.\n",
        "\n",
        "Conversely, if the distribution shows a concentration of lower charges, it could indicate price sensitivity and lead to strategies focused on affordability. Addressing the needs of different segments can improve customer satisfaction and retention."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 (Churn Rate by Contract Type - Bar Plot)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for the bar plot\n",
        "plt.figure(figsize=(8, 5))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Plot the count plot with 'Contract' on the x-axis and color-coded by 'Churn Label'\n",
        "sns.countplot(x='Contract', hue='Churn Label', data=dataset, palette='coolwarm')  # Create a count plot with a specified color palette\n",
        "# Parameters explained:\n",
        "# - x: The column to be plotted on the x-axis ('Contract')\n",
        "# - hue: The column used for color-coding ('Churn Label')\n",
        "# - data: The dataset to be used for the plot\n",
        "# - palette: The color palette for the plot\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.countplot.html\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Churn Rate by Contract Type\")  # Add a title to the chart\n",
        "plt.xlabel(\"Contract Type\")  # Label the x-axis\n",
        "plt.ylabel(\"Count\")  # Label the y-axis\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is ideal for comparing categorical data across multiple groups. In this case, it helps compare the churn rate for each contract type (e.g., Month-to-month, One year, Two year)."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution of churn across different contract types. For example, month-to-month contracts have a higher churn rate compared to one-year or two-year contracts. This is evident from the significantly higher count of churned customers with month-to-month contracts."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the churn rate by contract type can inform targeted retention strategies. If certain contract types, like month-to-month contracts, have higher churn rates, businesses can investigate the reasons and take specific actions to improve customer retention for those contracts.\n",
        "\n",
        "Addressing high churn rates in specific contract types can help reduce overall churn and improve customer satisfaction. There are no insights from this chart that would lead to negative growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 (Churn Rate by Tenure - Box Plot)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Tenure Months' column to numeric in case it's stored as an object\n",
        "# This conversion ensures the column is treated as numeric for plotting\n",
        "dataset['Tenure Months'] = pd.to_numeric(dataset['Tenure Months'], errors='coerce')  # Convert 'Tenure Months' to numeric\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html\n",
        "\n",
        "# Plotting the box plot to see the distribution of tenure months for churned and non-churned customers\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "sns.boxplot(x='Churn Label', y='Tenure Months', data=dataset, palette='Set2')  # Create a box plot with a specified color palette\n",
        "# Parameters explained:\n",
        "# - x: The column to be plotted on the x-axis ('Churn Label')\n",
        "# - y: The column to be plotted on the y-axis ('Tenure Months')\n",
        "# - data: The dataset to be used for the plot\n",
        "# - palette: The color palette for the plot\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Churn Rate by Tenure - Box Plot', fontsize=14)  # Add a title to the chart\n",
        "plt.xlabel('Churn Label', fontsize=12)  # Label the x-axis\n",
        "plt.ylabel('Tenure Months', fontsize=12)  # Label the y-axis\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot is an excellent way to visualize the distribution of a continuous variable like tenure months. It shows the spread and central tendency, and highlights any potential outliers. By comparing churned and non-churned customers, we can see how tenure relates to churn."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that churned customers tend to have a shorter tenure compared to non-churned customers. The median tenure for churned customers is significantly lower, around 10 months, while for non-churned customers it is around 40 months. This indicates that customers who churn tend to leave early in their tenure."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the relationship between tenure and churn can help businesses focus their retention efforts on customers who are at risk of churning early. Implementing strategies to engage new customers and increase their tenure can reduce churn rates.\n",
        "\n",
        "Addressing the factors that contribute to short tenure can improve customer satisfaction and loyalty, leading to positive business growth."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 (Churn Rate by Internet Service Type - Pie Chart)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip spaces from column names to ensure compatibility\n",
        "# This step ensures that there are no leading or trailing spaces in column names that could cause issues\n",
        "dataset.columns = dataset.columns.str.strip()  # Strip spaces from column names\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.strip.html\n",
        "\n",
        "# Prepare data\n",
        "# Calculate the count of each Internet Service type\n",
        "internet_service_data = dataset['Internet Service'].value_counts()  # Count the number of occurrences of each Internet Service type\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "# Calculate the average churn rate for each Internet Service type\n",
        "churn_rate_by_internet = dataset.groupby('Internet Service')['Churn Value'].mean() * 100  # Calculate the average churn rate\n",
        "# Explanation: Group the data by 'Internet Service', calculate the mean churn rate for each group, and convert to percentage\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.mean.html\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(8, 8))  # Create a figure and set its size\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
        "\n",
        "ax.pie(churn_rate_by_internet, labels=churn_rate_by_internet.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightgreen', 'orange'])  # Create a pie chart\n",
        "# Parameters explained:\n",
        "# - labels: Labels for the pie chart slices\n",
        "# - autopct: String used to label the pie slices with their numeric value\n",
        "# - startangle: The starting angle of the pie chart\n",
        "# - colors: Colors of the slices\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html\n",
        "\n",
        "plt.title('Churn Rate by Internet Service Type')  # Add a title to the chart\n",
        "plt.show()  # Display the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is an effective way to show the proportion of churn rates across different internet service types. It provides a clear visual representation of how each service type contributes to the overall churn."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the churn rates for different internet service types. For example:\n",
        "\n",
        "- Fiber optic: 61.4% (represented in green)\n",
        "\n",
        "- DSL: 27.8% (represented in blue)\n",
        "\n",
        "- No internet service: 10.8% (represented in orange)"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the churn rate by internet service type can inform targeted strategies to retain customers. If certain internet service types have higher churn rates, businesses can investigate the reasons and take specific actions to improve customer retention for those services.\n",
        "\n",
        "Addressing high churn rates in specific service types can help reduce overall churn and improve customer satisfaction, leading to positive business outcomes."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 (Customer Distribution by Tenure Groups - Violin Plot)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tenure groups for better visualization\n",
        "# This step creates categorized groups based on tenure months for clearer analysis\n",
        "dataset['Tenure Group'] = pd.cut(dataset['Tenure Months'],\n",
        "                                 bins=[0, 12, 24, 48, 72, 100],\n",
        "                                 labels=['0-12', '13-24', '25-48', '49-72', '73+'])\n",
        "# Explanation: Categorize 'Tenure Months' into groups for clearer visualization\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\n",
        "\n",
        "# Plotting the Violin Plot\n",
        "plt.figure(figsize=(12, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "sns.violinplot(data=dataset, x='Tenure Group', y='Monthly Charges', hue='Churn Label', split=True)  # Create a violin plot with a split to show churn vs. non-churn distribution\n",
        "# Parameters explained:\n",
        "# - data: The dataset to be used for the plot\n",
        "# - x: The column to be plotted on the x-axis ('Tenure Group')\n",
        "# - y: The column to be plotted on the y-axis ('Monthly Charges')\n",
        "# - hue: The column used for color-coding ('Churn Label')\n",
        "# - split: Whether to split the plot to show churn vs. non-churn distribution\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Customer Distribution by Tenure Groups and Monthly Charges')  # Add a title to the chart\n",
        "plt.xlabel('Tenure Groups (Months)')  # Label the x-axis\n",
        "plt.ylabel('Monthly Charges ($)')  # Label the y-axis\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(title='Churn', loc='upper right')  # Add a legend to indicate churn status\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A violin plot is an excellent choice for visualizing the distribution of a continuous variable (monthly charges) across different groups (tenure groups) while also highlighting differences between churned and non-churned customers. It combines the benefits of a box plot and a density plot."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals how monthly charges vary across different tenure groups and highlights any differences between churned and non-churned customers. For example, customers in the '0-12' tenure group with higher monthly charges might have a higher churn rate compared to those in the '73+' tenure group."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the distribution of monthly charges across different tenure groups and their relation to churn can help businesses tailor their retention strategies. If certain tenure groups with higher charges are more prone to churn, businesses can focus on improving their experience to retain them.\n",
        "\n",
        "Addressing these insights can help reduce churn and improve customer satisfaction, leading to positive business outcomes."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 (Churn Rate by Senior Citizen Status - Grouped Bar Chart)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by Senior Citizen and Churn Label for visualization\n",
        "# This step groups the data to show the count of churned vs. non-churned customers based on their senior citizen status\n",
        "senior_citizen_churn = dataset.groupby(['Senior Citizen', 'Churn Label']).size().reset_index(name='Count')\n",
        "# Explanation: Group the data by 'Senior Citizen' and 'Churn Label', then calculate the size of each group and reset the index\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html\n",
        "\n",
        "# Create the grouped bar chart\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "sns.barplot(data=senior_citizen_churn, x='Senior Citizen', y='Count', hue='Churn Label', palette='coolwarm')  # Create a grouped bar chart with a specified color palette\n",
        "# Parameters explained:\n",
        "# - data: The dataset to be used for the plot\n",
        "# - x: The column to be plotted on the x-axis ('Senior Citizen')\n",
        "# - y: The column to be plotted on the y-axis ('Count')\n",
        "# - hue: The column used for color-coding ('Churn Label')\n",
        "# - palette: The color palette for the plot\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Churn Rate by Senior Citizen Status')  # Add a title to the chart\n",
        "plt.xlabel('Senior Citizen Status (0 = No, 1 = Yes)')  # Label the x-axis\n",
        "plt.ylabel('Customer Count')  # Label the y-axis\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(title='Churn')  # Add a legend to differentiate churn status\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A grouped bar chart is an excellent choice for comparing the churn rates between senior citizens and non-senior citizens. It clearly shows the count of churned and non-churned customers within each group, allowing for easy comparison."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the churn rates for senior citizens compared to non-senior citizens. For customers who are not senior citizens, the count of those who did not churn is significantly higher (over 4000) compared to those who did churn (around 1000).\n",
        "\n",
        "For customers who are senior citizens, the count of those who did not churn is lower (around 500) compared to those who did churn (around 300). This indicates that senior citizens have a higher churn rate than non-senior citizens, suggesting that senior citizens are more likely to leave the service."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focused Retention: The company can prioritize retention strategies for senior citizens, such as offering tailored packages or providing additional support services that address their specific needs.\n",
        "\n",
        "Resource Allocation: With non-senior citizens being the majority, insights into their churn behavior can help optimize general customer service and product offerings.\n",
        "\n",
        "Yes, the higher churn rate among senior citizens highlights a potential lack of engagement or tailored offerings for this demographic. Addressing these issues is critical to reducing churn and improving satisfaction in this group.\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11  (Monthly Charges vs Total Charges - Scatter Plot)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for the scatter plot\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Plotting the scatter plot\n",
        "plt.scatter(dataset['Monthly Charges'], dataset['Total Charges'], alpha=0.5, c=dataset['Churn Value'], cmap='coolwarm', edgecolor='k')  # Create a scatter plot\n",
        "# Parameters explained:\n",
        "# - alpha: Transparency level of the points\n",
        "# - c: The column used for color coding ('Churn Value')\n",
        "# - cmap: Colormap for better visual distinction\n",
        "# - edgecolor: Color of the edges of the points\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Monthly Charges vs Total Charges')  # Add a title to the chart\n",
        "plt.xlabel('Monthly Charges')  # Label the x-axis\n",
        "plt.ylabel('Total Charges')  # Label the y-axis\n",
        "\n",
        "# Adding color bar for churn status\n",
        "plt.colorbar(label='Churn (0 = No, 1 = Yes)')  # Add a color bar to indicate churn status\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(alpha=0.3)  # Set grid transparency\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.grid.html\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is ideal for visualizing the relationship between two continuous variables, in this case, monthly charges and total charges. It allows us to see patterns, correlations, and potential outliers, while color-coding by churn status adds another layer of insight."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals a positive correlation between monthly charges and total charges, indicating that higher monthly charges contribute to higher total charges over time. The color gradient shows areas where churn is more prevalent, helping to identify specific charge ranges where customers are more likely to churn."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact: Understanding the relationship between monthly charges, total charges, and churn can help businesses identify patterns that lead to customer churn. For example, if high monthly charges correlate with higher churn rates, businesses can consider revisiting their pricing strategies or offering more value at those price points.\n",
        "\n",
        "Negative Insight: The data suggests that customers with high monthly charges may perceive less value or experience dissatisfaction early in their tenure. Failing to address these issues could exacerbate churn among this critical group. Strategies to engage and satisfy new customers who pay higher monthly charges can reduce churn. Offering value-added services or discounts to high-paying customers at the beginning of their tenure can enhance retention."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 (Churn Rate by Dependents - Clustered Bar Chart)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure for the clustered bar chart\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Plotting the count plot with 'Dependents' on the x-axis and color-coded by 'Churn Label'\n",
        "sns.countplot(x='Dependents', hue='Churn Label', data=dataset, palette='viridis')  # Create a count plot with a specified color palette\n",
        "# Parameters explained:\n",
        "# - x: The column to be plotted on the x-axis ('Dependents')\n",
        "# - hue: The column used for color-coding ('Churn Label')\n",
        "# - data: The dataset to be used for the plot\n",
        "# - palette: The color palette for the plot\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.countplot.html\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Churn Rate by Dependents')  # Add a title to the chart\n",
        "plt.xlabel('Dependents (Yes/No)')  # Label the x-axis\n",
        "plt.ylabel('Count of Customers')  # Label the y-axis\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(title='Churn Label', loc='upper right')  # Add a legend to differentiate churn status\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A clustered bar chart is an excellent choice for comparing the churn rates between customers with dependents and those without. It clearly shows the count of churned and non-churned customers within each group, allowing for easy comparison."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the churn rates for customers with dependents compared to those without. For example, the count of customers without dependents who have not churned is significantly higher than those who have churned.\n",
        "\n",
        "Similarly, for customers with dependents, the number of non-churned customers is higher, but the difference is less pronounced compared to those without dependents."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retention Campaigns: The organization could design campaigns targeting customers without dependents, such as personalized offers or bundled services, to improve retention rates.\n",
        "\n",
        "Family-Oriented Packages: Introducing family plans or dependents-focused discounts might attract more long-term customers and enhance loyalty.\n",
        "\n",
        "\n",
        "If the focus shifts entirely toward customers with dependents, it might lead to neglecting the needs of customers without dependents, potentially driving up their churn rate further."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 (Churn Rate by Payment Method - Stacked Area Chart)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by 'Payment Method' and 'Churn Label'\n",
        "churn_payment_method = dataset.groupby(['Payment Method', 'Churn Label']).size().unstack()\n",
        "# Explanation: Group the data by 'Payment Method' and 'Churn Label', then calculate the size of each group and unstack the grouped DataFrame for further analysis\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html\n",
        "\n",
        "# Normalize to get proportions\n",
        "churn_payment_method_percentage = churn_payment_method.div(churn_payment_method.sum(axis=1), axis=0)\n",
        "# Explanation: Normalize the data to get proportions by dividing each group's size by the total size for each 'Payment Method'\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.div.html\n",
        "\n",
        "# Plot Stacked Area Chart\n",
        "churn_payment_method_percentage.plot(kind='area', stacked=True, figsize=(10, 6), cmap='coolwarm')  # Create a stacked area chart with a specified color palette\n",
        "# Parameters explained:\n",
        "# - kind: Type of plot to generate ('area' for stacked area plot)\n",
        "# - stacked: Whether to stack the areas\n",
        "# - figsize: Size of the figure\n",
        "# - cmap: Colormap for better visual distinction\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Churn Rate by Payment Method')  # Add a title to the chart\n",
        "plt.xlabel('Payment Method')  # Label the x-axis\n",
        "plt.ylabel('Proportion of Churn and Non-Churn')  # Label the y-axis\n",
        "\n",
        "# Rotate x-ticks for better readability\n",
        "plt.xticks(rotation=45)  # Rotate x-ticks to 45 degrees\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xticks.html\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(title='Churn', labels=['No Churn', 'Churn'])  # Add a legend to differentiate churn status\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked area chart is an excellent choice to visualize the proportion of churn and non-churn customers across different payment methods. It clearly shows the contributions of each payment method to overall churn, making it easy to see trends and patterns."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that the proportion of churn is highest for electronic check payments, while bank transfer (automatic) and credit card (automatic) have the lowest churn rates. This indicates that customers using electronic check are more likely to churn compared to those using other payment methods."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Targeted Retention Strategies: If certain payment methods are associated with higher churn rates, businesses could explore ways to incentivize customers to switch to more stable payment methods, such as online or auto-pay options.\n",
        "\n",
        "Customer Experience Improvements: Insights from this chart could guide businesses in enhancing the payment process for customers who use methods linked to higher churn, potentially improving customer retention.\n",
        "\n",
        "If a significant portion of churn is attributed to customers using a particular payment method, and the company fails to address the payment method's convenience or accessibility, it could result in negative growth. For example, if mailed checks are linked to higher churn, businesses that don't make the transition to digital payment methods may continue to lose customers over time."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude non-numeric columns for correlation analysis\n",
        "# This step ensures that only numeric columns are considered for correlation calculation\n",
        "numeric_dataset = dataset.select_dtypes(include=['float64', 'int64'])\n",
        "# Explanation: Select only the columns with numeric data types for correlation analysis\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_dataset.corr()  # Calculate the pairwise correlation of numeric columns\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))  # Adjust the size of the figure for better readability\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, linewidths=0.5)  # Create a heatmap with annotations\n",
        "# Parameters explained:\n",
        "# - annot: Whether to annotate the cells with the correlation coefficient values\n",
        "# - fmt: Format for the annotation text\n",
        "# - cmap: Colormap for better visual distinction\n",
        "# - vmin, vmax: Minimum and maximum values for the colormap\n",
        "# - linewidths: Width of the lines that will divide each cell\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Adding title\n",
        "plt.title('Correlation Heatmap')  # Add a title to the chart\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()  # Adjust the layout to ensure everything fits nicely\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
        "\n",
        "# Display the chart\n",
        "plt.show()  # Show the chart\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Correlation Heatmap was selected because it provides a compact, visual summary of the linear relationships between different features. Understanding these relationships is important for identifying which features might be important predictors for churn, as well as avoiding multicollinearity in any models that will be built later.\n",
        "\n",
        "Key Insights from the Correlation Heatmap:\n",
        "Strong Correlations:\n",
        "Zip Code and Latitude: There is a strong positive correlation (0.90) between Zip Code and Latitude, indicating that these variables are closely related in your dataset. This might suggest that zip code and location (latitude) are linked, possibly reflecting regional groupings.\n",
        "\n",
        "Tenure Months and Monthly Charges: A positive correlation (0.83) is observed between Tenure Months and Monthly Charges, which suggests that customers who have been with the company for a longer duration are likely paying higher monthly charges. This can indicate that loyal customers might be on more expensive plans.\n",
        "\n",
        "Tenure Months and Total Charges: A strong positive correlation (0.88) between Tenure Months and Total Charges suggests that customers with longer tenure tend to accumulate higher total charges, as expected.\n",
        "\n",
        "Negative Correlations:\n",
        "Zip Code and Longitude: A negative correlation (-0.78) exists between Zip Code and Longitude, indicating that zip code and longitude are inversely related in this dataset.\n",
        "\n",
        "Churn Rate and Tenure: There is a weak negative correlation (-0.12) between Churn Rate and Tenure Months, suggesting that customers who have been with the company for a longer period might have a slightly lower likelihood of churn. This is a good sign for customer retention.\n",
        "\n",
        "Churn Rate and Monthly Charges: There is a weak negative correlation (-0.10) between Churn Rate and Monthly Charges, which suggests that customers with higher charges tend to churn less. This could indicate that high-paying customers are more likely to stay with the company.\n",
        "\n",
        "Moderate Positive Correlations:\n",
        "Churn Value and Total Charges: A moderate positive correlation (0.66) between Churn Value and Total Charges suggests that customers with higher total charges are more likely to churn, which could indicate that high-paying customers are dissatisfied and leave."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart highlights several key relationships between variables, such as strong correlations between Zip Code and Latitude, Tenure Months and Monthly Charges, and between Tenure Months and Total Charges.\n",
        "\n",
        "It also indicates weak but significant negative correlations, like the one between Churn Rate and Tenure Months, suggesting that longer-tenured customers may have a lower churn rate.\n",
        "\n",
        "The chart reveals that Churn Value is positively correlated with Total Charges, indicating a potential issue with higher-paying customers leaving."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select_dtypes(include='number').columns"
      ],
      "metadata": {
        "id": "Zzic4BKx3bFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting specific numeric columns for the pair plot\n",
        "numerical_cols = ['Monthly Charges', 'Total Charges', 'Tenure Months', 'Churn Score']  # Specify numeric columns for the pair plot\n",
        "\n",
        "# Creating the pair plot with the specified numeric columns\n",
        "# Hue is set to 'Churn Label' to differentiate churned and non-churned customers\n",
        "# Diagonal kind is set to 'kde' for Kernel Density Estimate plots\n",
        "sns.pairplot(dataset, vars=numerical_cols, hue='Churn Label', diag_kind='kde')  # Create a pair plot\n",
        "# Parameters explained:\n",
        "# - vars: Columns to be included in the pair plot\n",
        "# - hue: Column used for color-coding ('Churn Label')\n",
        "# - diag_kind: Type of plot for the diagonal subplots ('kde' for Kernel Density Estimate)\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()  # Show the pair plot\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the pairplot because it effectively visualizes relationships between multiple numerical variables simultaneously, allowing for the exploration of pairwise correlations, trends, and patterns. It displays the distributions of individual features on the diagonal, providing insights into the spread of values for churned versus non-churned customers. Using 'Churn Label' as the hue enables a clear comparison between these two groups across features like 'Monthly Charges', 'Total Charges', 'Tenure Months', and 'Churn Score'. This chart provides a comprehensive overview, saving time by visualizing all relationships in one place while helping to identify clusters, trends, and potential outliers in the data."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot reveals several key insights:\n",
        "\n",
        "1. Tenure vs. Churn: Customers with shorter tenure months are more likely to churn, indicating that newer customers might be less satisfied or more prone to leaving. This suggests a need to improve onboarding and early customer experience to enhance satisfaction and retention.\n",
        "\n",
        "2. Monthly Charges and Churn: Customers paying higher monthly charges show a higher likelihood of churn, suggesting potential dissatisfaction with pricing or value for money. This highlights the importance of ensuring that high-paying customers perceive adequate value for their payments.\n",
        "\n",
        "3. Total Charges Distribution: While higher total charges are generally associated with longer-tenured customers, churn is more common among customers with lower total charges, reflecting short-term contracts. This underscores the need to address concerns for short-term customers and possibly incentivize longer commitments.\n",
        "\n",
        "4. Churn Score Clusters: The churn score distribution shows a clear separation between churned and non-churned customers, with churned customers having higher churn scores overall. Monitoring churn scores can help identify at-risk customers for early intervention.\n",
        "\n",
        "These insights highlight the need to focus on improving customer satisfaction for new customers, addressing concerns related to high monthly charges, and monitoring churn scores for early intervention.\n",
        "\n",
        "\n",
        "Recommendations:\n",
        "- Focus on New Customers: Implement strategies to improve customer satisfaction during the initial months of their tenure to reduce early churn.\n",
        "\n",
        "- Address Pricing Concerns: Evaluate the value proposition of high monthly charges and consider offering additional benefits or discounts to high-paying customers.\n",
        "\n",
        "- Engage Short-Term Customers: Develop retention strategies tailored for customers with short-term contracts to encourage longer commitments.\n",
        "\n",
        "- Monitor Churn Scores: Use churn scores to proactively identify and engage at-risk customers before they decide to leave.\n",
        "\n",
        "These insights and recommendations can guide efforts to enhance customer retention, improve satisfaction, and reduce churn."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***\n",
        "\n",
        "### Understanding Hypothesis Testing\n",
        "\n",
        "#### What is Hypothesis Testing?\n",
        "Hypothesis testing is a statistical method used to make decisions or inferences about a population based on sample data. It involves making an initial assumption (the null hypothesis) and then determining whether there is enough evidence from the sample data to reject this assumption in favor of an alternative hypothesis.\n",
        "\n",
        "#### Relevance of Hypothesis Testing\n",
        "- **Decision Making**: Helps in making informed decisions based on sample data.\n",
        "- **Scientific Research**: Widely used in scientific research to test theories and hypotheses.\n",
        "- **Quality Control**: Used in industries for quality control and to ensure products meet certain standards.\n",
        "- **Marketing and Business**: Helps businesses in understanding market trends and consumer behavior by testing various hypotheses.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Validation**: Hypothesis testing can validate the assumptions made by a machine learning model.\n",
        "- **Feature Selection**: Can be used to determine the significance of different features in a model.\n",
        "- **Model Comparison**: Helps in comparing different models to determine which one performs better based on statistical evidence.\n",
        "- **Understanding Results**: Provides a framework to understand the results and reliability of a model's predictions.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Hypothesis Testing](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/hypothesis-testing/)\n",
        "\n",
        "### Performing Hypothesis Testing: Z Test and T Test Using P Value\n",
        "\n",
        "#### What We Are Going to Perform\n",
        "In this section, we will focus on performing hypothesis testing using Z tests and T tests, and interpreting the results using P values.\n",
        "\n",
        "#### What is a Z Test?\n",
        "A Z test is a statistical test used to determine whether there is a significant difference between the means of two groups. It is commonly used when the sample size is large (n > 30) and the population variance is known. The test statistic follows a standard normal distribution (Z distribution).\n",
        "\n",
        "#### What is a T Test?\n",
        "A T test is a statistical test used to determine whether there is a significant difference between the means of two groups. It is commonly used when the sample size is small (n < 30) and the population variance is unknown. The test statistic follows a T distribution.\n",
        "\n",
        "#### What is a P Value?\n",
        "A P value is a measure of the probability that an observed difference could have occurred by random chance. It is used to determine the statistical significance of the test results. A low P value (typically < 0.05) indicates strong evidence against the null hypothesis, leading to its rejection.\n",
        "\n",
        "#### Relevance of Z Test, T Test, and P Value\n",
        "- **Z Test**: Useful for comparing the means of large samples with known variances.\n",
        "- **T Test**: Useful for comparing the means of small samples with unknown variances.\n",
        "- **P Value**: Provides a measure of the strength of the evidence against the null hypothesis, guiding the decision-making process.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Model Validation**: These tests helps in validating the assumptions and performance of the model.\n",
        "- **Accuracy**: Ensures the reliability of the model's predictions by statistically validating the differences observed in the data.\n",
        "- **Confidence**: Increases confidence in the model's results by providing a statistical basis for the conclusions drawn.\n",
        "- **Decision Making**: Aids in making informed decisions based on statistical evidence, reducing the risk of errors.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Hypothesis Testing, Z Test, and T Test](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/).\n",
        "\n"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing**"
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Customers who are not churning have an average Monthly Charges greater than or equal to $80**\n",
        "\n",
        "**2. Customers with a Senior Citizen status have a higher churn rate 30%**\n",
        "\n",
        "**3. Customers with a tenure greater than 40 months have a lower churn rate**"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prior to performing hypothesis testing for the above three statements, we will first create a function to calculate Z-scores and P-values**"
      ],
      "metadata": {
        "id": "U07tTSoV-c-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We're defining a cool class called 'findz' that has some nifty methods to calculate Z-scores and variance.\n",
        "class findz:\n",
        "\n",
        "    # This method helps us calculate the Z-score for proportion testing.\n",
        "    def proportion(self, sample, hyp, size):\n",
        "        \"\"\"\n",
        "        Calculates the Z-score for proportion testing.\n",
        "\n",
        "        :param sample: The observed proportion in our sample data.\n",
        "        :param hyp: The proportion we are testing against (our hypothesis).\n",
        "        :param size: The number of observations in our sample.\n",
        "        :return: The Z-score, indicating how far our sample proportion is from the hypothesized proportion.\n",
        "        \"\"\"\n",
        "        return (sample - hyp) / math.sqrt(hyp * (1 - hyp) / size)\n",
        "        # Reference: https://en.wikipedia.org/wiki/Standard_score\n",
        "\n",
        "    # This method helps us calculate the Z-score for mean testing.\n",
        "    def mean(self, hyp, sample, size, std):\n",
        "        \"\"\"\n",
        "        Calculates the Z-score for mean testing.\n",
        "\n",
        "        :param hyp: The mean value we are testing against (our hypothesis).\n",
        "        :param sample: The observed mean in our sample data.\n",
        "        :param size: The number of observations in our sample.\n",
        "        :param std: The standard deviation of our sample (how spread out the values are).\n",
        "        :return: The Z-score, indicating how far our sample mean is from the hypothesized mean.\n",
        "        \"\"\"\n",
        "        return (sample - hyp) * math.sqrt(size) / std\n",
        "        # Reference: https://en.wikipedia.org/wiki/Standard_score\n",
        "\n",
        "    # This method helps us calculate the variance for our sample data.\n",
        "    def variance(self, hyp, sample, size):\n",
        "        \"\"\"\n",
        "        Calculates the variance for our sample data.\n",
        "\n",
        "        :param hyp: The variance we are testing against (our hypothesis).\n",
        "        :param sample: The observed variance in our sample data.\n",
        "        :param size: The number of observations in our sample.\n",
        "        :return: The variance, which measures how spread out the sample values are around the mean.\n",
        "        \"\"\"\n",
        "        return (size - 1) * sample / hyp\n",
        "        # Reference: https://en.wikipedia.org/wiki/Variance\n",
        "\n",
        "# This lambda function calculates the sample variance.\n",
        "# Variance measures how spread out the data points are around the mean.\n",
        "variance = lambda x: sum([(i - np.mean(x)) ** 2 for i in x]) / (len(x) - 1)\n",
        "# Reference: https://en.wikipedia.org/wiki/Variance\n",
        "\n",
        "# This lambda function calculates the cumulative distribution function (CDF) of the Z-score.\n",
        "# The CDF tells us the probability that a standard normal variable will be less than or equal to a given value.\n",
        "zcdf = lambda x: norm(0, 1).cdf(x)\n",
        "# Reference: https://en.wikipedia.org/wiki/Cumulative_distribution_function\n",
        "\n",
        "# This function calculates the P-value based on the Z-score or t-test, depending on the test type specified.\n",
        "def p_value(z, tailed, t, hypothesis_number, df, col):\n",
        "\n",
        "    # Check if we're using a Z-test or not\n",
        "    if t != \"true\":\n",
        "\n",
        "        # Calculate the cumulative distribution function (CDF) of the Z-score\n",
        "        z = zcdf(z)\n",
        "\n",
        "        # Determine the P-value based on the type of tail test\n",
        "        if tailed == 'l':\n",
        "            # Left-tailed test: The P-value is the CDF of the Z-score\n",
        "            return z\n",
        "        elif tailed == 'r':\n",
        "            # Right-tailed test: The P-value is 1 minus the CDF of the Z-score\n",
        "            return 1 - z\n",
        "        elif tailed == 'd':\n",
        "            # Two-tailed test: The P-value is twice the CDF of the Z-score if z > 0.5, else twice the complement\n",
        "            if z > 0.5:\n",
        "                return 2 * (1 - z)\n",
        "            else:\n",
        "                return 2 * z\n",
        "        else:\n",
        "            # Return NaN for invalid tail type\n",
        "            return np.nan\n",
        "    else:\n",
        "        # If using a t-test, calculate the t-statistic and P-value using scipy.stats.ttest_1samp\n",
        "        z, p_value = stats.ttest_1samp(df[col], hypothesis_number)\n",
        "        # Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html\n",
        "\n",
        "        # Return the calculated P-value from the t-test\n",
        "        return p_value\n",
        "\n",
        "# This function helps us decide the conclusion based on the P-value and a predefined significance level.\n",
        "def conclusion(p):\n",
        "    # Set the significance level for our hypothesis test.\n",
        "    # Commonly used significance level is 0.05 (5%).\n",
        "    significance_level = 0.05\n",
        "\n",
        "    # Compare the P-value with the significance level.\n",
        "    if p > significance_level:\n",
        "        # If the P-value is greater than the significance level, we fail to reject the null hypothesis.\n",
        "        return f\"Failed to reject the Null Hypothesis for p = {p}.\"\n",
        "    else:\n",
        "        # If the P-value is less than or equal to the significance level, we reject the null hypothesis.\n",
        "        return f\"Null Hypothesis rejected successfully for p = {p}\"\n",
        "\n",
        "# Initializing the 'findz' class to start calculating Z-scores and variance.\n",
        "findz = findz()"
      ],
      "metadata": {
        "id": "rIS-RuU1gE7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Statement Analysis: Customers who are not churning have an average Monthly Charges greater than or equal to $80**\n",
        "\n",
        "1. **Reason for Picking the Statement**\n",
        "   This statement is chosen to identify high-value customers with lower churn risk based on their monthly charges, providing insights into customer retention.\n",
        "\n",
        "2. **Relevance on the Model**\n",
        "   Helps the model accurately identify and predict non-churning customers, improving prediction accuracy and customer segmentation.\n",
        "\n",
        "3. **Business Impact**\n",
        "   Optimizes marketing and retention strategies for high-value customers, leading to increased revenue and customer loyalty.\n",
        "\n",
        "4. **Potential Negative Outcome**\n",
        "   Yes. Focusing solely on high-value customers may neglect potential valuable customers with lower charges, reducing overall customer diversity.\n",
        "\n",
        "5. **How to Overcome the Potential Negative Outcome**\n",
        "   Implement balanced retention strategies targeting both high-value and potentially loyal lower-charge customers to maintain a diverse and profitable customer base."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "\n",
        "#### **Hypotheses**\n",
        "- **Null Hypothesis (H₀)**: N = 80 (Customers who are not churning have an average Monthly Charges of 80 dollars or more)\n",
        "- **Alternate Hypothesis (H₁)**: N < 80 (Customers who are not churning have an average Monthly Charges less than 80 dollars)\n",
        "- **Test Type**: Left Tailed Test (because we are testing if the average is less than $80)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Perform an appropriate statistical test**\n",
        "\n",
        "**Filtering and Calculating Statistics for Non-Churning Customers**"
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We're filtering the dataset to get only the non-churning customers.\n",
        "# This step isolates the data for customers who did not churn.\n",
        "hypo_1 = dataset[dataset[\"Churn Label\"] == \"No\"]\n",
        "# Filters the dataset to include only rows where the 'Churn Label' is 'No'.\n",
        "\n",
        "# Defining the null hypothesis value.\n",
        "# This represents the average Monthly Charges we are testing against, which is $80.\n",
        "hypothesis_number = 80\n",
        "# Sets the null hypothesis value to 80, representing the average Monthly Charges we are testing against.\n",
        "\n",
        "# Calculating the sample mean of Monthly Charges for non-churning customers.\n",
        "# This gives us the average Monthly Charges from our sample data.\n",
        "sample_mean = hypo_1[\"Monthly Charges\"].mean()\n",
        "# Computes the mean (average) Monthly Charges for the non-churning customers in the filtered dataset.\n",
        "\n",
        "# Determining the sample size.\n",
        "# This is the number of non-churning customers in our dataset.\n",
        "size = len(hypo_1)\n",
        "# Calculates the sample size by counting the number of non-churning customers in the filtered dataset.\n",
        "\n",
        "# Calculating the standard deviation of Monthly Charges for non-churning customers.\n",
        "# Standard deviation measures the spread of the data points.\n",
        "std = (variance(hypo_1[\"Monthly Charges\"]))**0.5\n",
        "# Calculates the standard deviation of Monthly Charges for the non-churning customers by taking the square root of the variance."
      ],
      "metadata": {
        "id": "tZn6E2Xrm3T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Z-value and Drawing Conclusion**"
      ],
      "metadata": {
        "id": "bP3di_qaVU_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Z-value for our hypothesis test.\n",
        "# The Z-value helps us understand how far our sample mean is from the hypothesized mean, measured in standard deviations.\n",
        "z = findz.mean(hypothesis_number, sample_mean, size, std)\n",
        "# Uses the 'mean' method from the 'findz' class to calculate the Z-value for mean testing.\n",
        "\n",
        "# Calculating the P-value based on the Z-value.\n",
        "# The P-value helps us determine the statistical significance of our hypothesis test.\n",
        "p = p_value(z=z, tailed='l', t=\"false\", hypothesis_number=hypothesis_number, df=hypo_1, col=\"Monthly Charges\")\n",
        "# Uses the 'p_value' function to calculate the P-value based on the Z-value, tail type, and other parameters.\n",
        "\n",
        "# Drawing the conclusion based on the P-value.\n",
        "# This will tell us if we should reject the null hypothesis or not.\n",
        "print(conclusion(p))\n",
        "# Uses the 'conclusion' function to print the result of the hypothesis test based on the P-value"
      ],
      "metadata": {
        "id": "ek0qd6m8Z0sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the **Z-Test** as the statistical testing to obtain the P-Value. Based on the results, the Null Hypothesis has been rejected, indicating that customers who are not churning do not have an average Monthly Charge of 80(dollars) or more. From our analysis, this suggests that even though these customers are not churning, their average monthly charges are below $80, possibly pointing to pricing preferences or service selections that align with lower costs.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Why did you choose the specific statistical test?**"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the distribution of Monthly Charges to understand the data distribution.\n",
        "\n",
        "# Creating a figure for the plot with a specified size (9x6 inches).\n",
        "# This ensures the plot is large enough to be easily readable.\n",
        "fig = plt.figure(figsize=(9, 6))  # Here fig is a new figure object with the specified size\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Getting the current axis of the plot to customize it further.\n",
        "ax = fig.gca()  # Here ax is the current axis object of the figure\n",
        "# Reference: https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.gca\n",
        "\n",
        "# Defining the feature to plot, which is the Monthly Charges for non-churning customers.\n",
        "feature = hypo_1[\"Monthly Charges\"]  # Here feature is the Series of Monthly Charges for non-churning customers\n",
        "\n",
        "# Plotting the distribution of Monthly Charges with a Kernel Density Estimate (KDE) overlay.\n",
        "# This creates a histogram with a KDE overlay to visualize the distribution.\n",
        "sns.histplot(hypo_1[\"Monthly Charges\"], kde=True)  # Visualizing the data distribution\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "\n",
        "# Adding a vertical dashed line to indicate the mean of the Monthly Charges.\n",
        "ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)  # Highlighting the mean value\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.axvline.html\n",
        "\n",
        "# Adding a vertical dashed line to indicate the median of the Monthly Charges.\n",
        "ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)  # Highlighting the median value\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.axvline.html\n",
        "\n",
        "# Setting the title of the plot to provide context.\n",
        "ax.set_title(\"Monthly Charges Distribution\")  # Title of the plot\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_title.html\n",
        "\n",
        "# Displaying the plot.\n",
        "plt.show()  # Rendering and displaying the plot\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "nIFASHJJpyDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot, we can observe that the mean and median are fairly close to each other, indicating a Normal Distribution of the data. Based on this, I chose the Z-Test for hypothesis testing, as it is well-suited for normally distributed data with a known population variance or a sufficiently large sample size."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "\n",
        "#### **Statement Analysis: Customers with a Senior Citizen status have a higher churn rate of 30%**\n",
        "\n",
        "1. **Reason for Picking the Statement**\n",
        "   This statement is chosen to identify a specific segment of customers (Senior Citizens) who may have a higher risk of churning, based on their churn rate.\n",
        "\n",
        "2. **Relevance on the Model**\n",
        "   Helps the model accurately predict churn among Senior Citizens, improving the overall prediction accuracy and customer segmentation.\n",
        "\n",
        "3. **Business Impact**\n",
        "   Targeting Senior Citizens with higher churn rates can help in developing targeted retention strategies, potentially reducing churn and increasing customer retention.\n",
        "\n",
        "4. **Potential Negative Outcome**\n",
        "   Yes. Focusing solely on Senior Citizens may overlook other segments with high churn rates, leading to a potential loss of a broader customer base.\n",
        "\n",
        "5. **How to Overcome the Potential Negative Outcome**\n",
        "   Implement comprehensive retention strategies that address all high-risk segments, ensuring a balanced approach to customer retention."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hypotheses\n",
        "- **Null Hypothesis (H₀)**: N = 0.30 (This means we assume that the churn rate for Senior Citizens is equal to or lower than 30%.)\n",
        "- **Alternate Hypothesis (H₁)**: N > 30 (This means we assume that the churn rate for Senior Citizens is higher than 30%.)\n",
        "- **Test Type**: Right Tailed Test (We are checking if the churn rate for Senior Citizens is significantly higher than a threshold, which is 30% in this case.)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Perform an appropriate statistical test**\n",
        "\n",
        "**Filtering and Calculating Statistics for Senior Citizens**\n"
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We start by filtering the dataset to focus on Senior Citizens.\n",
        "# This step isolates the data for customers who are marked as Senior Citizens.\n",
        "hypo_2 = dataset[dataset[\"Senior Citizen\"] == \"Yes\"]\n",
        "# Filters the dataset to include only rows where the 'Senior Citizen' column is 'Yes'.\n",
        "\n",
        "# Print the number of rows in the filtered dataset to ensure data exists for analysis.\n",
        "# This helps us verify that we have data to work with and prevents errors in the subsequent steps.\n",
        "print(f\"Number of rows in hypo_2: {len(hypo_2)}\")\n",
        "# Prints the number of rows in the filtered dataset to check if there are any Senior Citizens.\n",
        "\n",
        "# Calculate the churn rate for Senior Citizens.\n",
        "# This step helps us find the average churn rate for Senior Citizens in the dataset.\n",
        "churn_rate_senior = hypo_2[\"Churn Value\"].mean()\n",
        "# Computes the mean (average) churn rate for Senior Citizens in the filtered dataset.\n",
        "\n",
        "# Defining the hypothesized churn rate for Senior Citizens.\n",
        "# Null Hypothesis: The churn rate is 30%.\n",
        "hypothesis_number = 0.30  # Assumed churn rate (30%)\n",
        "# Sets the null hypothesis value to 0.30, representing the assumed churn rate.\n",
        "\n",
        "# Setting the sample mean to the calculated churn rate for Senior Citizens.\n",
        "# This is the observed churn rate from the sample data.\n",
        "sample_mean = churn_rate_senior\n",
        "# Assigns the calculated churn rate to the sample mean.\n",
        "\n",
        "# Determining the sample size.\n",
        "# This is the number of Senior Citizens in the dataset.\n",
        "size = len(hypo_2)\n",
        "# Calculates the sample size by counting the number of Senior Citizens in the filtered dataset.\n",
        "\n",
        "# Calculate the variance of the Churn Value for Senior Citizens.\n",
        "# Variance measures the dispersion or spread of the data points around the mean.\n",
        "variance_val = variance(hypo_2[\"Churn Value\"])\n",
        "# Calculates the variance of the Churn Value for the Senior Citizens using the 'variance' lambda function.\n",
        "\n",
        "# Calculate the standard deviation based on the variance.\n",
        "# If the variance is greater than 0, take the square root of the variance to get the standard deviation.\n",
        "# If the variance is 0, use a small value (1e-10) to avoid division by zero errors.\n",
        "std = (variance_val) ** 0.5 if variance_val > 0 else 1e-10  # Small value to avoid zero-division\n",
        "# Calculates the standard deviation of the Churn Value for Senior Citizens, using a small value to avoid division by zero if needed"
      ],
      "metadata": {
        "id": "srX0IKuqFSYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Z-value and Drawing Conclusion for Senior Citizens**"
      ],
      "metadata": {
        "id": "UPjPIrgcWD_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Z-value for the hypothesis test.\n",
        "# The Z-value helps us understand how far the sample mean is from the hypothesized mean, measured in standard deviations.\n",
        "z = findz.mean(hypothesis_number, sample_mean, size, std)  # Here, z is the calculated Z-score\n",
        "# Uses the 'mean' method from the 'findz' class to calculate the Z-value for mean testing.\n",
        "# Reference: https://en.wikipedia.org/wiki/Standard_score\n",
        "\n",
        "# Get the P-value based on the Z-value.\n",
        "# The P-value helps us determine the statistical significance of our hypothesis test.\n",
        "p = p_value(z=z, tailed='r', t=\"false\", hypothesis_number=hypothesis_number, df=hypo_2, col=\"Churn Value\")  # Here, p is the calculated P-value\n",
        "# Uses the 'p_value' function to calculate the P-value based on the Z-value, tail type, and other parameters.\n",
        "# Reference: https://en.wikipedia.org/wiki/P-value\n",
        "\n",
        "# Drawing the conclusion based on the P-value.\n",
        "# This will tell us if we should reject the null hypothesis or not.\n",
        "print(conclusion(p))  # This prints the conclusion based on the P-value\n",
        "# Uses the 'conclusion' function to print the result of the hypothesis test based on the P-value"
      ],
      "metadata": {
        "id": "6i-FzRCGFWUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Which statistical test have you done to obtain P-Value?**"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used a Z-Test as the statistical test to obtain the P-value. The Z-Test was chosen because the sample size for the senior citizen group is sufficiently large, and the data distribution is approximately normal. Based on the Z-score calculated, the P-value was derived, which led to the conclusion that the null hypothesis was rejected. This indicates that the churn rate for senior citizens is significantly higher than the hypothesized 30%."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the Difference Between Mean and Median for Senior Citizens' Churn Values**"
      ],
      "metadata": {
        "id": "nTlcXaBAWVyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the difference between the mean and median of the churn values for Senior Citizens.\n",
        "# This helps us understand the distribution of the data.\n",
        "mean_median_difference = hypo_2[\"Churn Value\"].mean() - hypo_2[\"Churn Value\"].median()  # Here, mean_median_difference is the calculated difference\n",
        "# Computes the difference between the mean and median of the churn values for Senior Citizens.\n",
        "\n",
        "# Print the mean-median difference to observe any significant disparity between these measures.\n",
        "# A small difference suggests a normal distribution, justifying the use of a Z-Test.\n",
        "print(\"Mean Median Difference is :-\", mean_median_difference)  # This prints the calculated mean-median difference\n",
        "# Prints the calculated mean-median difference to observe any significant disparity between these measures"
      ],
      "metadata": {
        "id": "WEQWr33GH_zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, the mean-median difference is 0.4168, which is small but not exactly 0. This indicates that the data may not be perfectly symmetric, but with a sufficiently large sample size, the Central Limit Theorem ensures that the sampling distribution of the mean is approximately normal. Therefore, I used a Z-Test for hypothesis testing."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "\n",
        "### **Statement Analysis: Customers with a Senior Citizen status have a higher churn rate of 30%**\n",
        "\n",
        "1. **Reason for Picking the Statement** This statement is chosen to identify a specific segment of customers (Senior Citizens) who may have a higher risk of churning, based on their churn rate.\n",
        "2. **Relevance on the Model** Helps the model accurately predict churn among Senior Citizens, improving the overall prediction accuracy and customer segmentation.\n",
        "3. **Business Impact** Targeting Senior Citizens with higher churn rates can help in developing targeted retention strategies, potentially reducing churn and increasing customer retention.\n",
        "4. **Potential Negative Outcome** Yes. Focusing solely on Senior Citizens may overlook other segments with high churn rates, leading to a potential loss of a broader customer base.\n",
        "5. **How to Overcome the Potential Negative Outcome** Implement comprehensive retention strategies that address all high-risk segments, ensuring a balanced approach to customer retention."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hypotheses**\n",
        "- **Null Hypothesis (H₀)**: N = 0.30 (This means we assume that the churn rate for Senior Citizens is equal to or lower than 30%.)\n",
        "- **Alternate Hypothesis (H₁)**: N > 30 (This means we assume that the churn rate for Senior Citizens is higher than 30%.)\n",
        "- **Test Type**: Right Tailed Test (We are checking if the churn rate for Senior Citizens is significantly higher than a threshold, which is 30% in this case.)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering and Calculating Statistics for Customers with Tenure Greater Than 40 Months**"
      ],
      "metadata": {
        "id": "XJsl7Ab9Whr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset to include only customers with tenure greater than 40 months.\n",
        "# This step isolates the data for customers who have been with the company for more than 40 months.\n",
        "hypo_3 = dataset[dataset[\"Tenure Months\"] > 40]  # Here, hypo_3 is the filtered dataset\n",
        "# Filters the dataset to include only rows where 'Tenure Months' is greater than 40.\n",
        "\n",
        "# Calculate the churn rate for customers with tenure greater than 40 months.\n",
        "# This step maps the churn labels to numerical values (1 for \"Yes\" and 0 for \"No\").\n",
        "hypo_3[\"churn_rate\"] = hypo_3[\"Churn Label\"].map({\"Yes\": 1, \"No\": 0})  # Here, churn_rate is the numeric representation of churn status\n",
        "# Maps the 'Churn Label' to numerical values to calculate the churn rate.\n",
        "\n",
        "# Define the hypothesized churn rate for customers with tenure greater than 40 months.\n",
        "# Null Hypothesis: The churn rate is 25%.\n",
        "hypothesis_number = 0.25  # Churn rate assumption (let's assume 25% as the benchmark churn rate)\n",
        "# Sets the null hypothesis value to 0.25, representing the assumed churn rate.\n",
        "\n",
        "# Calculate the sample mean (churn rate) for customers with tenure greater than 40 months.\n",
        "# This gives us the observed churn rate from the sample data.\n",
        "sample_mean = hypo_3[\"churn_rate\"].mean()  # Here, sample_mean is the observed churn rate\n",
        "# Computes the mean (average) churn rate for customers with tenure greater than 40 months.\n",
        "\n",
        "# Determine the sample size.\n",
        "# This is the number of customers with tenure greater than 40 months in the dataset.\n",
        "size = len(hypo_3)  # Here, size is the total number of customers with tenure > 40 months\n",
        "# Calculates the sample size by counting the number of customers with tenure greater than 40 months.\n",
        "\n",
        "# Calculate the standard deviation of the churn rate for customers with tenure greater than 40 months.\n",
        "# Standard deviation measures the dispersion or spread of the data points around the mean.\n",
        "std = hypo_3[\"churn_rate\"].std()  # Here, std is the standard deviation of churn rate\n",
        "# Computes the standard deviation of the churn rate for customers with tenure greater than 40 months.\n",
        "# Reference: https://en.wikipedia.org/wiki/Standard_deviation"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Z-value and Drawing Conclusion for Customers with Tenure Greater Than 40 Months**"
      ],
      "metadata": {
        "id": "-ZOSI1BwWw9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Z-value for the hypothesis test.\n",
        "# The Z-value helps us understand how far the sample mean is from the hypothesized mean, measured in standard deviations.\n",
        "z = findz.mean(hypothesis_number, sample_mean, size, std)  # Here, z is the calculated Z-score\n",
        "# Uses the 'mean' method from the 'findz' class to calculate the Z-value for mean testing.\n",
        "# Reference: https://en.wikipedia.org/wiki/Standard_score\n",
        "\n",
        "# Get the P-value based on the Z-value.\n",
        "# The P-value helps us determine the statistical significance of our hypothesis test.\n",
        "# Note: We are using a one-tailed test because we are specifically checking if the churn rate is lower.\n",
        "p = p_value(z=z, tailed='l', t=\"true\", hypothesis_number=hypothesis_number, df=hypo_3, col=\"churn_rate\")  # Here, p is the calculated P-value\n",
        "# Uses the 'p_value' function to calculate the P-value based on the Z-value, tail type, and other parameters.\n",
        "# Reference: https://en.wikipedia.org/wiki/P-value\n",
        "\n",
        "# Drawing the conclusion based on the P-value.\n",
        "# This will tell us if we should reject the null hypothesis or not.\n",
        "print(conclusion(p))  # This prints the conclusion based on the P-value\n",
        "# Uses the 'conclusion' function to print the result of the hypothesis test based on the P-value"
      ],
      "metadata": {
        "id": "XdnBMdSEK76u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Which statistical test have you done to obtain P-Value?**"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The T-Test was used to obtain the P-value. The T-Test is suitable when comparing the sample mean to a population mean, especially when the data is not perfectly normal or when the sample size is moderate. In this analysis, the churn rate for customers with a tenure greater than 40 months was compared to the hypothesized population mean churn rate of 25%. The T-Test helped determine whether there is a significant difference between the observed sample mean and the hypothesized population mean, leading to the conclusion that the null hypothesis was rejected."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the T-Test as the statistical test to obtain the p-value and found that the null hypothesis has been rejected. This indicates that customers with a tenure greater than 40 months have a significantly different churn rate compared to those with a shorter tenure."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing the Distribution of Churn Rates for Customers with Tenure Greater Than 40 Months**"
      ],
      "metadata": {
        "id": "3tkZ40ZRXBlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a figure for the plot with a specified size (9x6 inches).\n",
        "fig = plt.figure(figsize=(9, 6))  # Here, fig is a new figure object with the specified size\n",
        "# Creates a new figure with the specified size of 9x6 inches.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Getting the current axis of the plot to customize it further.\n",
        "ax = fig.gca()  # Here, ax is the current axis object of the figure\n",
        "# Gets the current axis of the figure for further customization.\n",
        "# Reference: https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.gca\n",
        "\n",
        "# Defining the feature to plot, which is the churn rate for customers with tenure > 40 months.\n",
        "feature = hypo_3[\"churn_rate\"]  # Here, feature is the Series of churn rates\n",
        "# Assigns the churn rate Series from the filtered dataset to the variable 'feature'.\n",
        "\n",
        "# Plotting the distribution of churn rates using seaborn's distplot function.\n",
        "sns.distplot(hypo_3[\"churn_rate\"])  # This creates a histogram with a KDE overlay to visualize the distribution\n",
        "# Plots the distribution of churn rates using seaborn's distplot, creating a histogram with a KDE overlay.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.distplot.html\n",
        "\n",
        "# Adding a vertical dashed line to indicate the mean of the churn rates.\n",
        "ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)  # This adds a magenta dashed line at the mean value\n",
        "# Adds a magenta dashed vertical line at the mean value of the churn rates.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.axvline.html\n",
        "\n",
        "# Adding a vertical dashed line to indicate the median of the churn rates.\n",
        "ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)  # This adds a cyan dashed line at the median value\n",
        "# Adds a cyan dashed vertical line at the median value of the churn rates.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.axvline.html\n",
        "\n",
        "# Displaying the plot.\n",
        "plt.show()  # This renders and displays the plot\n",
        "# Renders and displays the plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "ycJll2DyRUKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the Mean-Median Difference for Customers with Tenure Greater Than 40 Months**"
      ],
      "metadata": {
        "id": "hVMEDoVgXNXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the difference between the median and mean of the churn rates for customers with tenure > 40 months.\n",
        "# This helps us understand the distribution and skewness of the data.\n",
        "mean_median_difference = hypo_3[\"churn_rate\"].median() - hypo_3[\"churn_rate\"].mean()  # Here, mean_median_difference is the calculated difference\n",
        "# Computes the difference between the median and mean of the churn rates for customers with tenure greater than 40 months.\n",
        "\n",
        "# Print the mean-median difference to observe any significant disparity between these measures.\n",
        "# A small difference suggests a normal distribution, while a large difference indicates skewness.\n",
        "print(\"Mean Median Difference is :-\", mean_median_difference)  # This prints the calculated mean-median difference\n",
        "# Prints the calculated mean-median difference to observe any significant disparity between these measures."
      ],
      "metadata": {
        "id": "HNqULUgYRWbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the distribution in the histogram, the median is greater than the mean, which suggests that the data is negatively skewed. This is further supported by the shape of the distribution, where we see a high peak near zero and a tail extending to the left. As the data is not normally distributed, the Z-Test would not be appropriate.\n",
        "\n",
        "For datasets with skewed distributions, T-tests are a better choice because they can handle skewed data and still provide reliable results, especially in large sample sizes. While non-parametric tests are useful for smaller samples, T-tests with corresponding confidence intervals offer a robust method for analyzing skewed data, especially in large studies.\n",
        "\n",
        "Thus, for better results with this skewed data, I used the T-Test."
      ],
      "metadata": {
        "id": "VANSjcYwTPv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***\n",
        "\n",
        "### Feature Engineering & Data Pre-processing\n",
        "\n",
        "#### What is Feature Engineering?\n",
        "Feature engineering is the process of using domain knowledge to create new features or modify existing ones to improve the performance of machine learning models. It involves selecting, transforming, and creating variables that make the data more suitable for predictive modeling.\n",
        "\n",
        "#### What is Data Pre-processing?\n",
        "Data pre-processing is the critical step of cleaning, transforming, and organizing raw data into a format that is suitable for machine learning algorithms. This process includes handling missing values, scaling features, encoding categorical variables, and more.\n",
        "\n",
        "#### Relevance of Feature Engineering & Data Pre-processing\n",
        "- **Improves Model Performance**: Well-engineered features can significantly enhance the accuracy and performance of a model.\n",
        "- **Reduces Overfitting**: Properly pre-processed data helps in reducing the risk of overfitting, leading to more generalized models.\n",
        "- **Enhances Interpretability**: Creating meaningful features can help in better understanding and interpreting the model's predictions.\n",
        "- **Ensures Data Quality**: Data pre-processing ensures that the data is clean, consistent, and reliable, which is crucial for building robust models.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Accuracy**: Effective feature engineering and data pre-processing can lead to higher model accuracy and better prediction results.\n",
        "- **Training Time**: Properly pre-processed data can reduce the training time by making the data more suitable for the algorithms.\n",
        "- **Model Robustness**: Clean and well-engineered features contribute to building more robust models that perform well on unseen data.\n",
        "- **Error Reduction**: Handling missing values, outliers, and noise through pre-processing reduces errors and improves the model's performance.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Feature Engineering and Data Preprocessing for Machine Learning](https://www.analyticsvidhya.com/blog/2020/10/feature-engineering-data-preprocessing/)\n"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Handling Missing Values**\n",
        "\n",
        "#### **Understanding Missing Values**\n",
        "\n",
        "#### What are Missing Values?\n",
        "Missing values are data points that are not recorded or are absent from the dataset. These can occur due to various reasons such as errors in data collection, data entry issues, or incomplete observations.\n",
        "\n",
        "#### Importance of Handling Missing Values\n",
        "- **Data Integrity**: Ensuring the dataset is complete and accurate by addressing missing values.\n",
        "- **Improved Analysis**: Helps in performing accurate and meaningful analysis without bias.\n",
        "- **Prevents Errors**: Reduces the risk of errors and inconsistencies in the analysis process.\n",
        "\n",
        "#### Relevance to the Model\n",
        "- **Model Accuracy**: Handling missing values ensures that the model is trained on complete and representative data, leading to more accurate predictions.\n",
        "- **Efficiency**: Models with complete data run more efficiently and effectively without interruptions due to missing values.\n",
        "- **Reliability**: Improves the reliability of the model by ensuring that all data points are accounted for and accurately represented.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Predictive Power**: Proper handling of missing values can enhance the predictive power of the model by providing a more complete dataset.\n",
        "- **Bias Reduction**: Reduces bias in the model by addressing any potential distortions caused by missing data.\n",
        "- **Model Robustness**: Enhances the robustness of the model by ensuring it can handle different scenarios without being affected by missing values.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Handling Missing Values in Data Science](https://www.analyticsvidhya.com/blog/2021/10/handling-missing-values/)\n"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Copy of the Dataset for Feature Engineering**"
      ],
      "metadata": {
        "id": "0XpKJb5CYVMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for further feature engineering.\n",
        "# This step creates a copy of the original dataset and stores it in the variable df.\n",
        "df = dataset.copy()  # Here, df is the copied dataset.\n",
        "# Creates a copy of the original dataset and assigns it to the variable 'df'.\n",
        "# This ensures that the original dataset remains unchanged while we perform various transformations and feature engineering on the copied dataset.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html"
      ],
      "metadata": {
        "id": "QxEWQfQ0rpZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Missing Values & Missing Value Imputation**"
      ],
      "metadata": {
        "id": "-c0Tk2RRYhPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of missing values in each column.\n",
        "# This helps us identify which columns have missing data and how many values are missing.\n",
        "print(df.isnull().sum())  # Here, we count the number of missing values in each column\n",
        "# Uses 'isnull' to identify missing values in each column and 'sum' to count them.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html\n",
        "\n",
        "# Visualizing the missing values using a heatmap.\n",
        "# A heatmap provides a visual representation of missing data, making it easier to identify patterns.\n",
        "plt.figure(figsize=(12, 8))  # Adjust the figure size for better visibility\n",
        "# Creates a new figure with the specified size for better visibility.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "# Creating the heatmap to check for null values.\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)  # Using 'viridis' colormap for better color contrast\n",
        "# Plots a heatmap of the missing values using seaborn. The 'viridis' colormap provides better color contrast.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Adding a title to the heatmap for better context.\n",
        "plt.title('Heatmap of Missing Values', fontsize=16)  # Adding a title to the heatmap\n",
        "# Adds a title to the heatmap with a specified font size.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.title.html\n",
        "\n",
        "# Displaying the plot to visualize the missing values.\n",
        "plt.show()  # Rendering and displaying the plot\n",
        "# Renders and displays the plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **We have 11 and 5174 null values in the columns 'Churn Reason' & 'Tenure Group**"
      ],
      "metadata": {
        "id": "3NM14p8xv5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputing Missing Values**"
      ],
      "metadata": {
        "id": "fCqrEr_jYzRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling missing values in the 'Churn Reason' column with the string 'Unknown'.\n",
        "# This ensures that any missing data in this column is replaced with a placeholder value.\n",
        "df['Churn Reason'] = df['Churn Reason'].fillna('Unknown')\n",
        "# Replaces missing values in the 'Churn Reason' column with the string 'Unknown'.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
        "\n",
        "# Filling missing values in the 'Tenure Group' column with the most frequent value (mode) of the column.\n",
        "# This helps to maintain the consistency of the data by using the most common value to fill in the gaps.\n",
        "df['Tenure Group'] = df['Tenure Group'].fillna(df['Tenure Group'].mode()[0])\n",
        "# Replaces missing values in the 'Tenure Group' column with the most frequent value (mode) of the column.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html"
      ],
      "metadata": {
        "id": "teHkG-UNwtR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rechecking for Null Values After Handling Missing Data**"
      ],
      "metadata": {
        "id": "9VxO0JpvY8v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking for null values after handling missing values.\n",
        "# Printing the count of missing values for each column to verify the changes.\n",
        "print(\"\\nMissing Values/Null Values Count After Handling:\")  # Adding a header to indicate this is the after-handling count\n",
        "# Adds a header to indicate that the following counts are after handling missing values.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html\n",
        "\n",
        "print(df.isnull().sum())  # Here we print the count of remaining null values in each column\n",
        "# Uses 'isnull' to identify missing values and 'sum' to count them for each column.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html"
      ],
      "metadata": {
        "id": "RPyBCwlfxslI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Value Imputation Techniques Used:\n",
        "\n",
        "1. **For the 'Churn Reason' Column**:\n",
        "   - **Technique**: Replaced missing values with the string `'Unknown'`.\n",
        "   - **Reason**:\n",
        "     - `'Churn Reason'` is a categorical column, and replacing missing values with `'Unknown'` provides a meaningful placeholder while preserving the structure of the dataset.\n",
        "     - It ensures that the absence of data is explicitly represented, which can be useful for analysis without introducing any bias.\n",
        "\n",
        "2. **For the 'Tenure Group' Column**:\n",
        "   - **Technique**: Replaced missing values with the **mode** of the column (most frequently occurring value).\n",
        "   - **Reason**:\n",
        "     - `'Tenure Group'` is also a categorical column. The mode is a logical choice for imputation in categorical fields as it represents the most common category.\n",
        "     - This approach minimizes distortion in the data while maintaining the integrity of the distribution."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why These Techniques Were Chosen:\n",
        "\n",
        "- **Practicality**: Both techniques are simple yet effective for handling categorical data.\n",
        "- **Interpretability**: Replacing with `'Unknown'` explicitly shows where data is missing, and using the mode for imputation retains the natural distribution of the column.\n",
        "- **Data Integrity**: These methods ensure the imputed values do not introduce outliers or skew the dataset, preserving its usability for downstream analysis."
      ],
      "metadata": {
        "id": "fbKferBM0D16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers\n",
        "\n",
        "#### **Understanding Handling Outliers**\n",
        "\n",
        "#### What are Outliers?\n",
        "Outliers are data points that differ significantly from other observations in the dataset. They can be unusually high or low values that deviate from the overall pattern of the data.\n",
        "\n",
        "#### Importance of Handling Outliers\n",
        "- **Data Quality**: Ensures the dataset is clean and reliable by identifying and addressing anomalies.\n",
        "- **Accurate Analysis**: Prevents outliers from skewing the analysis and leading to incorrect conclusions.\n",
        "- **Model Performance**: Enhances the model's performance by reducing the impact of extreme values that can distort results.\n",
        "\n",
        "#### Relevance to the Model\n",
        "- **Improved Accuracy**: Handling outliers ensures that the model is trained on representative data, leading to more accurate predictions.\n",
        "- **Stability**: Models with fewer outliers are more stable and less likely to produce erratic results.\n",
        "- **Robustness**: Improves the robustness of the model by ensuring it can handle a variety of data points without being affected by anomalies.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Predictive Power**: Properly handling outliers can enhance the predictive power of the model by providing a more accurate representation of the data.\n",
        "- **Bias Reduction**: Reduces bias in the model by ensuring that extreme values do not disproportionately influence the results.\n",
        "- **Model Reliability**: Enhances the reliability of the model by ensuring that it can generalize well to new, unseen data.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Handling Outliers in Data Science](https://towardsdatascience.com/handling-outliers-in-your-data-using-python-426c390a9481)\n",
        "\n",
        "\n",
        "**Let's begin by identifying and categorizing numerical features based on their distribution. This helps us decide the best approach to handle outliers for each type**"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting numerical columns for outlier treatment.\n",
        "# We choose columns that typically have continuous numerical values where outliers might occur.\n",
        "numerical_cols = ['Monthly Charges', 'Total Charges', 'Tenure Months', 'Churn Score']\n",
        "# Specifies the numerical columns for outlier treatment.\n",
        "\n",
        "# Separating symmetric and skewed features.\n",
        "# We'll categorize numerical features based on their distribution.\n",
        "symmetric_feature = []  # List to store features with symmetric distribution\n",
        "non_symmetric_feature = []  # List to store features with skewed distribution\n",
        "# Initializes empty lists to store symmetric and skewed features.\n",
        "\n",
        "# Iterate through the selected numerical columns to determine their distribution type.\n",
        "for col in numerical_cols:\n",
        "    if abs(df[col].mean() - df[col].median()) < 0.2:  # Considered symmetric if mean is approximately equal to median.\n",
        "        symmetric_feature.append(col)  # Add to symmetric feature list.\n",
        "    else:\n",
        "        non_symmetric_feature.append(col)  # Add to non-symmetric (skewed) feature list.\n",
        "# Iterates through the numerical columns and categorizes them based on the difference between mean and median.\n",
        "# If the difference is less than 0.2, the feature is considered symmetric; otherwise, it is considered skewed.\n",
        "\n",
        "# Displaying Symmetric Distributed Features.\n",
        "print(\"Symmetric Distributed Features: \", symmetric_feature)  # Print the list of symmetric features.\n",
        "# Prints the list of symmetric features.\n",
        "\n",
        "# Displaying Skewed Distributed Features.\n",
        "print(\"Skewed Distributed Features: \", non_symmetric_feature)  # Print the list of skewed features.\n",
        "# Prints the list of skewed features.\n",
        "# Reference: https://en.wikipedia.org/wiki/Skewness"
      ],
      "metadata": {
        "id": "6HaJI_JD_8sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Outlier Treatment Function for Symmetric Features**\n",
        "**Let's define a function to handle outliers in symmetric features using the 3 standard deviations rule**"
      ],
      "metadata": {
        "id": "_aRXUWDrnkCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for outlier treatment in symmetric features (3 standard deviations).\n",
        "def outlier_treatment(df, feature):\n",
        "    # Calculate the upper boundary for outliers.\n",
        "    # 'upper_boundary' is the threshold above which a value in 'feature' is considered an outlier.\n",
        "    # It is calculated as the mean of the feature plus three times its standard deviation.\n",
        "    upper_boundary = df[feature].mean() + 3 * df[feature].std()\n",
        "    # Reference: https://en.wikipedia.org/wiki/Standard_deviation\n",
        "\n",
        "    # Calculate the lower boundary for outliers.\n",
        "    # 'lower_boundary' is the threshold below which a value in 'feature' is considered an outlier.\n",
        "    # It is calculated as the mean of the feature minus three times its standard deviation.\n",
        "    lower_boundary = df[feature].mean() - 3 * df[feature].std()\n",
        "    # Reference: https://en.wikipedia.org/wiki/Standard_deviation\n",
        "\n",
        "    # Return the calculated upper and lower boundaries for outliers.\n",
        "    # These boundaries will help in identifying and handling outliers in the 'feature'.\n",
        "    return upper_boundary, lower_boundary"
      ],
      "metadata": {
        "id": "oGPzPTg9HIvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Outlier Treatment Function for Skewed Features**\n",
        "**Let's define a function to handle outliers in skewed features using the Interquartile Range (IQR) method**"
      ],
      "metadata": {
        "id": "6qmHlicyn7A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for outlier treatment in skewed features (IQR method).\n",
        "def outlier_treatment_skew(df, feature):\n",
        "    # Calculate the Interquartile Range (IQR) for the feature.\n",
        "    # 'IQR' measures the spread of the middle 50% of the data.\n",
        "    IQR = df[feature].quantile(0.75) - df[feature].quantile(0.25)\n",
        "    # Calculates the Interquartile Range (IQR) for the feature.\n",
        "    # IQR measures the spread of the middle 50% of the data.\n",
        "    # Reference: https://en.wikipedia.org/wiki/Interquartile_range\n",
        "\n",
        "    # Calculate the lower boundary for outliers using the IQR method.\n",
        "    # 'lower_bridge' is the threshold below which a value in 'feature' is considered an outlier.\n",
        "    # It is calculated as the 25th percentile minus three times the IQR.\n",
        "    lower_bridge = df[feature].quantile(0.25) - 3 * IQR\n",
        "    # Calculates the lower boundary for outliers using the IQR method.\n",
        "    # Lower boundary is the 25th percentile minus three times the IQR.\n",
        "    # Reference: https://en.wikipedia.org/wiki/Interquartile_range\n",
        "\n",
        "    # Calculate the upper boundary for outliers using the IQR method.\n",
        "    # 'upper_bridge' is the threshold above which a value in 'feature' is considered an outlier.\n",
        "    # It is calculated as the 75th percentile plus three times the IQR.\n",
        "    upper_bridge = df[feature].quantile(0.75) + 3 * IQR\n",
        "    # Calculates the upper boundary for outliers using the IQR method.\n",
        "    # Upper boundary is the 75th percentile plus three times the IQR.\n",
        "    # Reference: https://en.wikipedia.org/wiki/Interquartile_range\n",
        "\n",
        "    # Return the calculated upper and lower boundaries for outliers.\n",
        "    # These boundaries will help in identifying and handling outliers in the 'feature'.\n",
        "    return upper_bridge, lower_bridge"
      ],
      "metadata": {
        "id": "jJVqzLCVHNp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applying Outlier Treatment to Symmetric Features**\n",
        "**Let's apply the outlier treatment to the symmetric features using the previously defined outlier_treatment function**"
      ],
      "metadata": {
        "id": "z5A9SV--oGjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying outlier treatment to symmetric features.\n",
        "\n",
        "# Iterate through each feature identified as symmetric.\n",
        "for feature in symmetric_feature:\n",
        "    # Calculate the lower and upper boundaries for outliers using the outlier_treatment function.\n",
        "    # 'lower' is the lower boundary below which values are considered outliers.\n",
        "    # 'upper' is the upper boundary above which values are considered outliers.\n",
        "    lower, upper = outlier_treatment(df, feature)\n",
        "\n",
        "    # Apply the calculated boundaries to the feature.\n",
        "    # Use the clip method to limit the feature's values within the specified boundaries.\n",
        "    # Values below 'lower' are set to 'lower' and values above 'upper' are set to 'upper'.\n",
        "    df[feature] = df[feature].clip(lower=lower, upper=upper)\n",
        "    # Clips the values in the feature to be within the specified lower and upper boundaries.\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.clip.html"
      ],
      "metadata": {
        "id": "Y2gn8efYHSmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applying Outlier Treatment to Skewed Features**\n",
        "**Let's proceed with applying outlier treatment to the skewed features using the previously defined outlier_treatment_skew function**"
      ],
      "metadata": {
        "id": "ouwF_XcDoSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying outlier treatment to skewed features.\n",
        "\n",
        "# Iterate through each feature identified as skewed.\n",
        "for feature in non_symmetric_feature:\n",
        "    # Calculate the lower and upper boundaries for outliers using the outlier_treatment_skew function.\n",
        "    # 'lower' is the lower boundary below which values are considered outliers.\n",
        "    # 'upper' is the upper boundary above which values are considered outliers.\n",
        "    lower, upper = outlier_treatment_skew(df, feature)\n",
        "\n",
        "    # Apply the calculated boundaries to the feature.\n",
        "    # Use the clip method to limit the feature's values within the specified boundaries.\n",
        "    # Values below 'lower' are set to 'lower' and values above 'upper' are set to 'upper'.\n",
        "    df[feature] = df[feature].clip(lower=lower, upper=upper)\n",
        "    # Clips the values in the feature to be within the specified lower and upper boundaries.\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.clip.html"
      ],
      "metadata": {
        "id": "9USQrVuaHWLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing Numerical Columns After Outlier Treatment Using Strip Plots**\n",
        "**Let's visualize the numerical columns to see the effect of our outlier treatment using strip plots**"
      ],
      "metadata": {
        "id": "aENUebWGoc5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the numerical columns after outlier treatment using strip plots.\n",
        "\n",
        "# Loop through each numerical column to create strip plots.\n",
        "for col in numerical_cols:\n",
        "    # Create a new figure for each column with a specified size.\n",
        "    plt.figure(figsize=(9, 6))  # Setting the figure size for better visibility\n",
        "    # Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "\n",
        "    # Create a strip plot for the current column.\n",
        "    # 'x' parameter specifies the column to be plotted.\n",
        "    sns.stripplot(x=df[col])  # Creating the strip plot\n",
        "    # Reference: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n",
        "\n",
        "    # Add a title to the plot for better understanding.\n",
        "    # The title includes the column name and indicates that it is after outlier treatment.\n",
        "    plt.title(f\"Distribution of {col} After Outlier Treatment\")  # Adding a title to the plot\n",
        "    # Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.title.html\n",
        "\n",
        "    # Display the plot.\n",
        "    plt.show()  # Rendering and displaying the plot\n",
        "    # Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "_NNJGvc1HZTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Treatment Techniques Used:\n",
        "\n",
        "To handle outliers in our dataset, I employed two primary techniques based on the distribution of the features:\n",
        "\n",
        "#### Symmetric Distribution (3-Standard Deviation Method):\n",
        "\n",
        "**Technique:**\n",
        "- For features with a symmetric distribution, where the mean is approximately equal to the median, I used the 3-standard deviation rule.\n",
        "- The upper boundary was calculated as: $$ \\text{mean} + 3 \\times \\text{std} $$\n",
        "- The lower boundary was calculated as: $$ \\text{mean} - 3 \\times \\text{std} $$\n",
        "- Any values falling outside these boundaries were clipped to the nearest boundary.\n",
        "\n",
        "**Reason:**\n",
        "- This method effectively addresses outliers in normally distributed data by leveraging the property of Gaussian distributions, where approximately 99.7% of data falls within three standard deviations from the mean.\n",
        "\n",
        "#### Skewed Distribution (IQR Method):\n",
        "\n",
        "**Technique:**\n",
        "- For skewed features, I employed the Interquartile Range (IQR) method to determine boundaries.\n",
        "- The IQR is calculated as $$ Q_3 - Q_1 $$, where $$ Q_1 $$ and $$ Q_3 $$ represent the 25th and 75th percentiles, respectively.\n",
        "- The upper boundary was set as: $$ Q_3 + 3 \\times \\text{IQR} $$\n",
        "- The lower boundary was set as: $$ Q_1 - 3 \\times \\text{IQR} $$\n",
        "- Outliers beyond these limits were clipped to the boundary values.\n",
        "\n",
        "**Reason:**\n",
        "- This approach is suitable for skewed data as it is resistant to extreme values and provides a robust measure for outlier detection.\n",
        "\n",
        "### Rationale for Using These Techniques:\n",
        "\n",
        "- **Practicality:** Both techniques are simple yet effective for handling categorical data.\n",
        "- **Interpretability:** Replacing with 'Unknown' explicitly shows where data is missing, and using the mode for imputation retains the natural distribution of the column.\n",
        "- **Data Integrity:** These methods ensure that the imputed values do not introduce outliers or skew the dataset, preserving its usability for downstream analysis.\n",
        "\n",
        "Instead of removing outliers, I chose to cap them to retain the dataset's integrity and prevent loss of valuable information, particularly given the limited number of data points. For a classification problem like ours, restricting outliers to the defined boundaries helps maintain consistency in the predictive model while ensuring that meaningful extreme values (e.g., high churn scores or charges) are not arbitrarily excluded. The combination of methods ensures tailored outlier treatment based on the nature of the feature distribution, optimizing data preparation for downstream analysis.\n",
        "\n",
        "### Visual Output:\n",
        "\n",
        "Here are the visual outputs of the numerical columns after outlier treatment:\n",
        "\n",
        "#### Monthly Charges:\n",
        "- **Observation:** The distribution shows fewer extreme high values, indicating effective outlier treatment.\n",
        "\n",
        "#### Total Charges:\n",
        "- **Observation:** The spread of values has been adjusted, with extreme high values capped appropriately.\n",
        "\n",
        "#### Tenure Months:\n",
        "- **Observation:** The long tail of extremely high values has been clipped, resulting in a more balanced distribution.\n",
        "\n",
        "#### Churn Score:\n",
        "- **Observation:** The distribution now shows a more concentrated range of values, reducing the impact of outliers.\n",
        "\n",
        "These visualizations confirm the effectiveness of our outlier treatment methods, ensuring that our data is clean and well-prepared for further analysis.\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding\n",
        "\n",
        "### Understanding Categorical Encoding\n",
        "\n",
        "#### What is Categorical Encoding?\n",
        "Categorical encoding is the process of converting categorical data into numerical values so that machine learning algorithms can process it. Categorical variables are often non-numeric and represent categories or labels. Encoding these variables helps in making them usable for various algorithms.\n",
        "\n",
        "#### Relevance of Categorical Encoding\n",
        "- **Algorithm Compatibility**: Many machine learning algorithms require numerical input, making encoding essential for processing categorical data.\n",
        "- **Model Performance**: Properly encoded categorical variables can improve the model's performance and accuracy.\n",
        "- **Interpretability**: Encoded categories can provide better insights and make the model's predictions more interpretable.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Data Utilization**: By encoding categorical variables, we can utilize all the information present in the dataset, leading to more comprehensive models.\n",
        "- **Feature Engineering**: Encoding opens up possibilities for advanced feature engineering techniques, enhancing the overall predictive power of the model.\n",
        "- **Bias Reduction**: Proper encoding techniques can help in reducing bias and improving the fairness of the model.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Categorical Encoding in Machine Learning](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02).\n"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's start by identifying the categorical columns in the dataset and then proceed with encoding them**"
      ],
      "metadata": {
        "id": "PlJROVBkNqJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying categorical columns in the dataset.\n",
        "# The list 'categorical_columns' will store the names of categorical columns.\n",
        "# We identify these columns by excluding those that appear in the dataset's statistical description (which typically includes numerical columns).\n",
        "categorical_columns = list(set(dataset.columns.to_list()).difference(set(dataset.describe().columns.to_list())))\n",
        "# Creates a list of categorical columns by identifying columns that are not included in the dataset's statistical description (typically numerical columns).\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html\n",
        "\n",
        "# Printing the identified categorical columns.\n",
        "print(\"Categorical Columns are :-\", categorical_columns)  # Print the list of categorical columns to display the names of the identified categorical columns.\n",
        "# Prints the list of identified categorical columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html"
      ],
      "metadata": {
        "id": "OEQnnmL-Lmef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting Dictionaries for Label Encoding**\n",
        "\n",
        "**Let's create the necessary dictionaries for binary encoding of our categorical features**"
      ],
      "metadata": {
        "id": "6jNAbymONhkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dictionaries for Label Encoding.\n",
        "# Dictionary for binary categories (Yes/No, Male/Female, etc.)\n",
        "\n",
        "# Creating a dictionary to map Yes/No categories to 1/0 for binary encoding.\n",
        "binary_dict = {\"Yes\": 1, \"No\": 0}  # 'Yes' is mapped to 1, 'No' is mapped to 0.\n",
        "# Maps 'Yes' to 1 and 'No' to 0 for binary encoding.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Creating a dictionary to map Male/Female categories to 1/0 for binary encoding.\n",
        "gender_dict = {\"Male\": 1, \"Female\": 0}  # 'Male' is mapped to 1, 'Female' is mapped to 0.\n",
        "# Maps 'Male' to 1 and 'Female' to 0 for binary encoding.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Creating a dictionary for binary mapping of the Senior Citizen column.\n",
        "senior_dict = {1: 1, 0: 0}  # '1' (Senior Citizen) is mapped to 1, '0' (Non-Senior) is mapped to 0.\n",
        "# Maps 1 (Senior Citizen) to 1 and 0 (Non-Senior) to 0 for binary encoding.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "rwGsD1QnVgtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Label Encoding for Specific Columns with Binary Values**\n",
        "\n",
        "**Let's perform label encoding for specific columns with binary values using the dictionaries we created earlier**"
      ],
      "metadata": {
        "id": "2JxOy2YUNZdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding for specific columns with binary values\n",
        "\n",
        "# Encoding 'Partner' column using binary_dict.\n",
        "dataset['Partner'] = dataset['Partner'].map(binary_dict)  # Map 'Yes' to 1 and 'No' to 0 for the 'Partner' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Encoding 'Dependents' column using binary_dict.\n",
        "dataset['Dependents'] = dataset['Dependents'].map(binary_dict)  # Map 'Yes' to 1 and 'No' to 0 for the 'Dependents' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Encoding 'Phone Service' column using binary_dict.\n",
        "dataset['Phone Service'] = dataset['Phone Service'].map(binary_dict)  # Map 'Yes' to 1 and 'No' to 0 for the 'Phone Service' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Encoding 'Paperless Billing' column using binary_dict.\n",
        "dataset['Paperless Billing'] = dataset['Paperless Billing'].map(binary_dict)  # Map 'Yes' to 1 and 'No' to 0 for the 'Paperless Billing' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Encoding 'Churn Label' column using binary_dict.\n",
        "dataset['Churn Label'] = dataset['Churn Label'].map(binary_dict)  # Map 'Yes' to 1 and 'No' to 0 for the 'Churn Label' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Encoding 'Senior Citizen' column using senior_dict.\n",
        "dataset['Senior Citizen'] = dataset['Senior Citizen'].map(senior_dict)  # Map '1' (Senior Citizen) to 1 and '0' (Non-Senior) to 0 for the 'Senior Citizen' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "# Encoding 'Gender' column using gender_dict.\n",
        "dataset['Gender'] = dataset['Gender'].map(gender_dict)  # Map 'Male' to 1 and 'Female' to 0 for the 'Gender' column.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "bQ_VZHqzVksh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoding Multi-Category Columns Using Label Mapping**\n",
        "\n",
        "**Let's perform label encoding for multi-category columns by creating and applying mappings for each unique value**"
      ],
      "metadata": {
        "id": "UzzamGyxNSuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding multi-category columns using label mapping.\n",
        "\n",
        "# Encoding 'Internet Service' column.\n",
        "# Get unique values in the 'Internet Service' column and sort them.\n",
        "internet_service_list = sorted(list(dataset['Internet Service'].unique()))  # Sorted list of unique values in 'Internet Service'\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html\n",
        "\n",
        "# Create a dictionary to map each unique value to a unique integer.\n",
        "internet_service_dict = dict(zip(internet_service_list, range(0, len(internet_service_list))))  # Mapping unique values to integers\n",
        "# Reference: https://docs.python.org/3/library/functions.html#zip\n",
        "\n",
        "# Apply the mapping to the 'Internet Service' column.\n",
        "dataset['Internet Service'] = dataset['Internet Service'].map(internet_service_dict)  # Apply the mapping\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\n",
        "\n",
        "# Encoding 'Contract' column.\n",
        "# Get unique values in the 'Contract' column and sort them.\n",
        "contract_list = sorted(list(dataset['Contract'].unique()))  # Sorted list of unique values in 'Contract'\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html\n",
        "\n",
        "# Create a dictionary to map each unique value to a unique integer.\n",
        "contract_dict = dict(zip(contract_list, range(0, len(contract_list))))  # Mapping unique values to integers\n",
        "# Reference: https://docs.python.org/3/library/functions.html#zip\n",
        "\n",
        "# Apply the mapping to the 'Contract' column.\n",
        "dataset['Contract'] = dataset['Contract'].map(contract_dict)  # Apply the mapping\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\n",
        "\n",
        "# Encoding 'Payment Method' column.\n",
        "# Get unique values in the 'Payment Method' column and sort them.\n",
        "payment_method_list = sorted(list(dataset['Payment Method'].unique()))  # Sorted list of unique values in 'Payment Method'\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html\n",
        "\n",
        "# Create a dictionary to map each unique value to a unique integer.\n",
        "payment_method_dict = dict(zip(payment_method_list, range(0, len(payment_method_list))))  # Mapping unique values to integers\n",
        "# Reference: https://docs.python.org/3/library/functions.html#zip\n",
        "\n",
        "# Apply the mapping to the 'Payment Method' column.\n",
        "dataset['Payment Method'] = dataset['Payment Method'].map(payment_method_dict)  # Apply the mapping\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"
      ],
      "metadata": {
        "id": "vMBczQkcYp79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-Hot Encoding for Columns with Multiple Unique Categories**\n",
        "\n",
        "### Understanding One-Hot Encoding\n",
        "\n",
        "#### What is One-Hot Encoding?\n",
        "One-Hot Encoding is a method used to convert categorical variables into a binary matrix representation. Each category is represented by a binary vector, where a value of 1 indicates the presence of the category, and a value of 0 indicates its absence. This encoding technique ensures that categorical data can be effectively used by machine learning algorithms that require numerical input.\n",
        "\n",
        "#### Relevance of One-Hot Encoding\n",
        "- **Non-Ordinal Categories**: Ideal for categorical variables with no inherent order, as it treats all categories equally.\n",
        "- **Algorithm Compatibility**: Converts categorical data into a numerical format that can be processed by various machine learning algorithms.\n",
        "- **Avoids Implicit Bias**: Prevents algorithms from assuming any ordinal relationship between categories by encoding them as independent binary vectors.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Improves Accuracy**: Provides a clear representation of categorical data, potentially improving the accuracy and performance of the model.\n",
        "- **Increases Dimensionality**: May increase the dimensionality of the dataset, especially when dealing with categorical variables with many unique values.\n",
        "- **Feature Interpretation**: Helps in interpreting the importance and impact of each category on the model's predictions.\n",
        "- **Mitigates Risk of Overfitting**: By avoiding any assumptions about the ordering of categories, it reduces the risk of overfitting to specific categories.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [One-Hot Encoding in Machine Learning](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
        "\n",
        "\n",
        "**Let's proceed with one-hot encoding for columns that have multiple unique categories. This ensures that we can represent categorical data effectively without introducing multicollinearity**"
      ],
      "metadata": {
        "id": "FOEqncuyNGNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding for columns with multiple unique categories.\n",
        "# Apply one-hot encoding to specified columns.\n",
        "# 'drop_first=True' drops the first category to avoid multicollinearity.\n",
        "\n",
        "# Use pd.get_dummies to perform one-hot encoding on the specified columns.\n",
        "dataset = pd.get_dummies(dataset, columns=['State', 'City', 'Churn Reason', 'Tenure Group'], drop_first=True)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"
      ],
      "metadata": {
        "id": "7Ol6TZIjYuuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Final Encoded Dataset***\n",
        "\n",
        "**Let's display the first few rows of the encoded dataset to check the final encoding**"
      ],
      "metadata": {
        "id": "sgdJF_LbM8WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final encoded dataset.\n",
        "# Display the first few rows of the encoded dataset.\n",
        "print(dataset.head())  # Print the first five rows of the dataset to check the final encoding.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"
      ],
      "metadata": {
        "id": "Gla3hKCDVa8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Handling Missing Values and Converting Boolean Columns***\n",
        "\n",
        "**Let's handle missing values in the 'Senior Citizen' column and convert boolean columns to integers**"
      ],
      "metadata": {
        "id": "dWtdoqspMpkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values in Senior Citizen.\n",
        "# Assuming 0 as the default value for NaN in Senior Citizen.\n",
        "dataset['Senior Citizen'] = dataset['Senior Citizen'].fillna(0).astype(int)  # Fill NaN values with 0 and convert the column to integer type\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html\n",
        "\n",
        "# Convert all boolean columns (result of one-hot encoding) to 0/1.\n",
        "# Select columns with boolean data types.\n",
        "bool_columns = dataset.select_dtypes(include=['bool']).columns  # Identify boolean columns in the dataset\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Convert boolean columns to integer type (0/1).\n",
        "dataset[bool_columns] = dataset[bool_columns].astype(int)  # Convert boolean columns to integers\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\n",
        "\n",
        "# Verify the dataset.\n",
        "print(dataset.head())  # Print the first five rows of the dataset to verify the changes.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html\n",
        "\n",
        "print(\"Dataset shape:\", dataset.shape)  # Print the shape of the dataset to verify the number of rows and columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html"
      ],
      "metadata": {
        "id": "5ihWQR31ZlrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical Encoding Techniques Used & Their Rationale\n",
        "\n",
        "In this project, we applied different categorical encoding techniques based on the nature of the categorical variables:\n",
        "\n",
        "1. **Label Encoding:**\n",
        "   - **Application:** Used for columns with binary or ordinal categorical data, such as Gender, Senior Citizen, Partner, Phone Service, and others.\n",
        "   - **Reason:** These columns contained only 2-4 unique values. Label Encoding efficiently converts these variables into numeric values (e.g., 0, 1, or 2) without significantly increasing the dataset's dimensionality.\n",
        "\n",
        "2. **One-Hot Encoding:**\n",
        "   - **Application:** Employed for columns with multiple unique values and no inherent order, like State, Churn Reason, and Tenure Group.\n",
        "   - **Reason:** One-Hot Encoding avoids imposing any ordinal relationships by creating separate binary columns for each unique value, which is crucial for nominal data.\n",
        "\n",
        "3. **Custom Dictionaries:**\n",
        "   - **Application:** Used for certain columns like Gender, Senior Citizen, and Internet Service, where we mapped categories to integers based on their domain-specific meaning.\n",
        "   - **Reason:** These mappings ensure that the encoded values reflect meaningful distinctions in the data, enhancing the interpretability and performance of the model.\n",
        "\n",
        "### Rationale for Using These Techniques:\n",
        "\n",
        "- **Label Encoding:** Converts binary and ordinal categorical data into numeric values, making them suitable for machine learning algorithms without increasing the dataset's dimensionality.\n",
        "- **One-Hot Encoding:** Handles nominal data by creating binary columns for each unique value, avoiding unintended ordinal relationships.\n",
        "- **Custom Dictionaries:** Provides meaningful mappings for specific categories, ensuring that the encoded values reflect domain-specific significance.\n",
        "\n",
        "These encoding techniques were selected to ensure the proper handling of categorical data, enabling the model to interpret them effectively while maintaining computational efficiency and dataset size balance.\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "There are no text columns in the given dataset which I am working on. So, Skipping this part."
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection\n",
        "\n",
        "### Understanding Feature Manipulation & Selection\n",
        "\n",
        "#### What is Feature Manipulation & Selection?\n",
        "Feature manipulation refers to the process of transforming or creating new features from existing data to improve the performance of machine learning models. Feature selection, on the other hand, is the process of selecting a subset of relevant features for use in model construction. This helps in reducing the dimensionality of the dataset and improving the efficiency of the model.\n",
        "\n",
        "#### Relevance of Feature Manipulation & Selection\n",
        "- **Improves Model Performance**: By selecting the most relevant features, the model can perform better and more efficiently.\n",
        "- **Reduces Overfitting**: Eliminating irrelevant or redundant features can reduce the risk of overfitting, leading to a more generalized model.\n",
        "- **Enhances Interpretability**: Fewer and more relevant features make the model easier to understand and interpret.\n",
        "- **Saves Computational Resources**: Reducing the number of features decreases the computational load and speeds up the training process.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Accuracy**: Proper feature selection and manipulation can lead to higher accuracy and better prediction results.\n",
        "- **Training Time**: Fewer features can result in faster training times and more efficient models.\n",
        "- **Model Robustness**: Focusing on the most relevant features can create more robust models that perform well on unseen data.\n",
        "- **Error Reduction**: Handling noisy and irrelevant features helps in reducing errors and improving the model's performance.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Feature Engineering and Feature Selection in Machine Learning](https://towardsdatascience.com/a-feature-selection-and-feature-engineering-comprehensive-guide-e4890c5a2298)."
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Feature Manipulation**\n",
        "\n",
        "**Let's start by identifying the numeric columns in the dataset**"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Manipulation\n",
        "# Select columns with numeric data types.\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns  # Select columns that have numerical data types.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Print the identified numeric columns.\n",
        "print(\"Numeric Columns:\\n\", numeric_cols)  # Print the list of numeric columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html"
      ],
      "metadata": {
        "id": "A7QQ4TCtAj3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Manipulation & Data Cleaning**\n",
        "\n",
        "**Let's ensure the columns used are numeric, create derived features, and handle infinite and missing values**"
      ],
      "metadata": {
        "id": "lj4Mk5reQoIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the columns used are numeric.\n",
        "if 'Count' in numeric_cols:\n",
        "    # Create derived features for call duration (example logic).\n",
        "    if 'Tech Support' in numeric_cols:\n",
        "        df['TechSupport_1call_duration'] = df['Tech Support'] / (df['Count'] + 1)  # Calculate tech support call duration per count.\n",
        "    if 'Paperless Billing' in numeric_cols:\n",
        "        df['PaperlessBilling_1call_duration'] = df['Paperless Billing'] / (df['Count'] + 1)  # Calculate paperless billing call duration per count.\n",
        "\n",
        "# Create derived features for service rate per minute.\n",
        "if 'Internet Service Charges' in numeric_cols and 'Internet Minutes' in numeric_cols:\n",
        "    df['InternetService_rate_per_min'] = df['Internet Service Charges'] / (df['Internet Minutes'] + 1)  # Calculate rate per minute for internet service.\n",
        "if 'Phone Service Charges' in numeric_cols and 'Phone Minutes' in numeric_cols:\n",
        "    df['PhoneService_rate_per_min'] = df['Phone Service Charges'] / (df['Phone Minutes'] + 1)  # Calculate rate per minute for phone service.\n",
        "\n",
        "# Handle infinite and missing values.\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns  # Select columns that have numerical data types.\n",
        "df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], 0)  # Replace infinite values with 0.\n",
        "df[numeric_cols] = df[numeric_cols].fillna(0)  # Fill missing values with 0.\n",
        "\n",
        "# Verify the dataset.\n",
        "print(\"Feature manipulation and data cleaning completed successfully!\")  # Print a success message.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"
      ],
      "metadata": {
        "id": "xouaR9LpAorA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Identifying Missing Columns in the Dataset**\n",
        "\n",
        "**Let's define a list of required columns and check which ones are missing in the dataset.**"
      ],
      "metadata": {
        "id": "OrmF0iCQQ8Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of required columns.\n",
        "required_columns = ['Tech Support', 'Count', 'Paperless Billing', 'Monthly Charges', 'Tenure Months', 'Total Charges']\n",
        "\n",
        "# Create a list of missing columns by checking which required columns are not in the dataset.\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]  # Check each required column to see if it is present in the dataset.\n",
        "\n",
        "# Print the list of missing columns.\n",
        "print(\"Missing Columns:\", missing_columns)  # Display the missing columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html"
      ],
      "metadata": {
        "id": "fETDO9ECCH3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting 'Tech Support' and 'Paperless Billing' to Numeric**\n",
        "\n",
        "**Let's convert the 'Tech Support' and 'Paperless Billing' columns to numeric, ensuring that any errors are coerced to NaN**"
      ],
      "metadata": {
        "id": "TGMB_wPnROr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Convert 'Tech Support' and 'Paperless Billing' to numeric.\n",
        "\n",
        "# Convert 'Tech Support' column to numeric, coercing errors to NaN.\n",
        "df['Tech Support'] = pd.to_numeric(df['Tech Support'], errors='coerce')  # Converts 'Tech Support' to numeric type, setting invalid parsing to NaN.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html\n",
        "\n",
        "# Convert 'Paperless Billing' column to numeric, coercing errors to NaN.\n",
        "df['Paperless Billing'] = pd.to_numeric(df['Paperless Billing'], errors='coerce')  # Converts 'Paperless Billing' to numeric type, setting invalid parsing to NaN.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html"
      ],
      "metadata": {
        "id": "yihxKqfNDcfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create derived features and handle potential infinite and missing values to refine our dataset**"
      ],
      "metadata": {
        "id": "nqozXcPqRfbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Manipulation\n",
        "\n",
        "# Create a derived feature for PaperlessBilling call duration if 'Count' and 'Monthly Charges' are numeric.\n",
        "if 'Count' in numeric_cols and 'Monthly Charges' in numeric_cols:\n",
        "    df['PaperlessBilling_1call_duration'] = df['Monthly Charges'] / (df['Count'] + 1)  # Calculate paperless billing call duration per count.\n",
        "\n",
        "# Create a derived feature for InternetService rate per minute if 'Total Charges' and 'Tenure Months' are numeric.\n",
        "if 'Total Charges' in numeric_cols and 'Tenure Months' in numeric_cols:\n",
        "    df['InternetService_rate_per_min'] = df['Total Charges'] / (df['Tenure Months'] + 1)  # Calculate rate per minute for internet service.\n",
        "\n",
        "# Handle potential infinities or NaN values.\n",
        "\n",
        "# Select columns with numeric data types.\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns  # Select columns that have numerical data types.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Replace infinite values with 0.\n",
        "df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], 0)  # Replace positive and negative infinity with 0.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n",
        "\n",
        "# Fill missing values with 0.\n",
        "df[numeric_cols] = df[numeric_cols].fillna(0)  # Fill NaN (Not a Number) values with 0.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"
      ],
      "metadata": {
        "id": "an1NtQVaDfLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take a look at the first few rows of the newly created features**"
      ],
      "metadata": {
        "id": "tD7JSXwERpov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['PaperlessBilling_1call_duration', 'InternetService_rate_per_min']].head())  # Display the first five rows of the derived features."
      ],
      "metadata": {
        "id": "GE4Vwy-0DiLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hurray!! I have successfully created some new features like PaperlessBilling_1call_duration and InternetService_rate_per_min.**"
      ],
      "metadata": {
        "id": "yWyI319DE3NC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Feature Selection**\n",
        "\n",
        "**Let's proceed with feature selection by first checking the shape and column names of the dataset**"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "\n",
        "# Get the shape of the dataset.\n",
        "df_shape = df.shape  # This will return the number of rows and columns in the dataset.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html\n",
        "\n",
        "# Get the column names of the dataset.\n",
        "df_columns = df.columns  # This will return the list of column names in the dataset.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html\n",
        "\n",
        "# Print the shape of the dataset.\n",
        "print(\"Dataset shape:\", df_shape)  # Print the shape of the dataset.\n",
        "\n",
        "# Print the column names of the dataset.\n",
        "print(\"Dataset columns:\\n\", df_columns)  # Print the column names of the dataset."
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dropping Constant and Quasi-Constant Features**\n",
        "\n",
        "#### **Constant and Quasi-Constant Features**\n",
        "\n",
        "**Constant and quasi-constant features** are variables in a dataset that show very little or no variability.\n",
        "\n",
        "- **Constant features** are those that have the same value across all observations in the dataset. For example, if a column has the same value for every row, it is considered a constant feature.\n",
        "- **Quasi-constant features** are those where a single value is shared by the majority of observations, typically more than 95-99%. These features show very little variation and are almost constant.\n",
        "\n",
        "#### **Why Drop These Features?**\n",
        "\n",
        "We drop these features because they usually do not provide useful information for building predictive models. Including constant or quasi-constant features can lead to overfitting and does not contribute to the model's ability to generalize. Removing them simplifies the dataset and helps improve the performance of machine learning algorithms.\n",
        "\n",
        "#### **Reference**\n",
        "\n",
        "For more details, you can refer to the [Train in Data guide on DropConstantFeatures](https://feature-engine.trainindata.com/en/latest/user_guide/selection/DropConstantFeatures.html)\n"
      ],
      "metadata": {
        "id": "8erj-4-mQBwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Constant and Quasi-Constant Features\n",
        "def dropping_constant(data):\n",
        "\n",
        "    # Separate the numeric columns from the non-numeric columns\n",
        "    numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "    # Create an instance of VarianceThreshold with a threshold of 0.05\n",
        "    var_thres = VarianceThreshold(threshold=0.05)\n",
        "    # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html\n",
        "\n",
        "    # Fit the threshold to the numeric dataset\n",
        "    var_thres.fit(numeric_data)\n",
        "\n",
        "    # Identify constant or quasi-constant columns\n",
        "    concol = [column for column in numeric_data.columns\n",
        "              if column not in numeric_data.columns[var_thres.get_support()]]\n",
        "\n",
        "    # If 'Churn Label' or 'Churn Value' columns are present, ensure they're not removed\n",
        "    if \"Churn Label\" in concol:\n",
        "        concol.remove(\"Churn Label\")\n",
        "    if \"Churn Value\" in concol:\n",
        "        concol.remove(\"Churn Value\")\n",
        "\n",
        "    # Drop the identified constant/quasi-constant columns from the entire dataset (including non-numeric columns)\n",
        "    df_removed_var = data.drop(concol, axis=1)\n",
        "\n",
        "    return df_removed_var\n",
        "\n",
        "# Calling the function\n",
        "df_removed_var = dropping_constant(df)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html"
      ],
      "metadata": {
        "id": "LZgdDxsyV2kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Verify the Changes**\n",
        "\n",
        "**Let's verify the shape and column names of the new dataset after dropping constant and quasi-constant features**"
      ],
      "metadata": {
        "id": "cFs7JgEpRGiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the new dataset.\n",
        "print(\"New Dataset shape:\", df_removed_var.shape)  # Print the shape of the new dataset.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html\n",
        "\n",
        "# Check the column names of the new dataset.\n",
        "print(\"New Dataset columns:\\n\", df_removed_var.columns)  # Print the column names of the new dataset.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html"
      ],
      "metadata": {
        "id": "spqxHIe5LJ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking the Shape After Dropping Constant Features**\n",
        "\n",
        "**Let's check the shape of the dataset after dropping constant and quasi-constant features to ensure the changes were successful**"
      ],
      "metadata": {
        "id": "vcog9w5zRU6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape after feature drop.\n",
        "print(\"Shape after dropping constant features:\", df_removed_var.shape)  # Print the shape of the new dataset.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html"
      ],
      "metadata": {
        "id": "0YeZyZjzMVzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Displaying the Remaining Columns**"
      ],
      "metadata": {
        "id": "yShACfX-Rowp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the remaining columns.\n",
        "print(\"Remaining columns after dropping constant features:\", df_removed_var.columns)  # Print the remaining column names.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html"
      ],
      "metadata": {
        "id": "WBL-26XdMe8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Correlation Analysis and Visualization**"
      ],
      "metadata": {
        "id": "t3pPuKmKRza4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only numeric columns for correlation analysis.\n",
        "numeric_df = df_removed_var.select_dtypes(include=['number'])\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Compute the correlation matrix.\n",
        "corr_matrix = numeric_df.corr()\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "# Plotting the heatmap for correlation visualization.\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "1F-rzs0oNN5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Variance Inflation Factor (VIF) Calculation**\n",
        "\n",
        "#### **What is it**\n",
        "\n",
        "Variance Inflation Factor (VIF) is a measure of multicollinearity in a set of multiple regression variables. It quantifies how much the variance of a regression coefficient is inflated due to collinearity with other predictors.\n",
        "\n",
        "#### Steps for VIF Calculation:\n",
        "\n",
        "1. **Select Independent Variables:** Choose the set of independent variables for which you want to calculate VIF.\n",
        "2. **Fit a Linear Regression Model:** For each independent variable, fit a linear regression model using all other independent variables as predictors.\n",
        "3. **Calculate R²:** Calculate the R² value from the regression model.\n",
        "4. **Compute VIF:** The VIF for each variable is given by:\n",
        "   $$\n",
        "   \\text{VIF} = \\frac{1}{1 - R^2}\n",
        "   $$\n",
        "   Where \\( R^2 \\) is the coefficient of determination from the regression model.\n",
        "\n",
        "#### Interpretation of VIF Values:\n",
        "\n",
        "- **VIF = 1:** No multicollinearity.\n",
        "- **1 < VIF < 5:** Moderate multicollinearity.\n",
        "- **VIF ≥ 5:** High multicollinearity, indicating that the variable is highly correlated with other predictors.\n",
        "\n",
        "#### Purpose of VIF:\n",
        "\n",
        "High VIF values indicate multicollinearity among variables, which can lead to unstable estimates and affect the reliability of the model. By identifying and addressing high VIF values, we can improve the robustness of our regression models.\n",
        "\n",
        "#### Reference\n",
        "\n",
        "For more details, you can refer to the [Variance Inflation Factor (VIF) documentation](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
      ],
      "metadata": {
        "id": "ecMuNi8TZck_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the independent variables set (drop 'Churn Label' and 'CustomerID' if needed).\n",
        "X = df_removed_var.drop(['Churn Label', 'CustomerID'], axis=1)  # Dropping target and identifier columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
        "\n",
        "# Select only numeric features for VIF calculation.\n",
        "X_numeric = X.select_dtypes(include=['number'])  # Filter the dataframe to include only numeric columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# VIF dataframe.\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X_numeric.columns  # Use numeric columns for VIF calculation.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
        "\n",
        "# Calculating VIF for each feature.\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_numeric.values, i) for i in range(len(X_numeric.columns))]  # Calculate VIF for each feature.\n",
        "# Reference: https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
        "\n",
        "# Round the VIF values to 2 decimal places and print features with VIF >= 8.\n",
        "for i in range(len(vif_data)):\n",
        "    vif_data.loc[i, \"VIF\"] = vif_data.loc[i, \"VIF\"].round(2)  # Round VIF values to 2 decimal places.\n",
        "    if vif_data.loc[i, \"VIF\"] >= 8:\n",
        "        print(f\"High VIF feature: {vif_data.loc[i, 'feature']} with VIF: {vif_data.loc[i, 'VIF']}\")\n",
        "\n",
        "# Optionally, print the full VIF table for reference.\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "CKDVR5O2TXWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check Feature Correlation and Finding Multicollinearity**"
      ],
      "metadata": {
        "id": "my7-g8YFZkyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Feature Correlation and Finding Multicollinearity\n",
        "def correlation(df, threshold):\n",
        "    col_corr = set()  # Initialize an empty set to store columns with high correlation.\n",
        "    corr_matrix = df.corr()  # Compute the correlation matrix.\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:  # Check if the absolute correlation exceeds the threshold.\n",
        "                colname = corr_matrix.columns[i]  # Get the name of the column with high correlation.\n",
        "                col_corr.add(colname)  # Add the column name to the set.\n",
        "\n",
        "    return list(col_corr)  # Return the list of columns with high correlation."
      ],
      "metadata": {
        "id": "YN0DFnRAfI2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check Feature Correlation and Find Multicollinearity**"
      ],
      "metadata": {
        "id": "U1-C3MScZ94H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the correlation function.\n",
        "def correlation(df, threshold):\n",
        "    \"\"\"\n",
        "    Finds highly correlated features in the DataFrame above a given threshold.\n",
        "    \"\"\"\n",
        "    # Select only numeric columns for correlation analysis.\n",
        "    numeric_df = df.select_dtypes(include=['number'])\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "    col_corr = set()  # Set to hold correlated column names.\n",
        "    corr_matrix = numeric_df.corr()  # Compute the correlation matrix on numeric data only.\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:  # Check for correlation above threshold.\n",
        "                colname = corr_matrix.columns[i]  # Get the column name.\n",
        "                col_corr.add(colname)  # Add it to the set.\n",
        "    return list(col_corr)  # Return the list of columns with high correlation.\n",
        "\n",
        "# Get multicollinear columns using the correlation function.\n",
        "highly_correlated_columns = correlation(df_removed_var, 0.5)\n",
        "\n",
        "# Remove the target variable and any identifiers from the correlated columns (if present).\n",
        "columns_to_exclude = ['Churn Label', 'CustomerID']  # Columns not to be dropped.\n",
        "highly_correlated_columns = [col for col in highly_correlated_columns if col not in columns_to_exclude]\n",
        "\n",
        "# Drop the multicollinear columns from the DataFrame.\n",
        "df_removed = df_removed_var.drop(highly_correlated_columns, axis=1)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
        "\n",
        "# Check the shape of the new DataFrame.\n",
        "print(\"Shape of the DataFrame after removing multicollinear columns:\", df_removed.shape)"
      ],
      "metadata": {
        "id": "bXVk85rHfeZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Correlation Heatmap After Dropping Required Columns**"
      ],
      "metadata": {
        "id": "eRzA6dGJaFtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation after dropping the required columns.\n",
        "# Correlation Heatmap visualization code.\n",
        "\n",
        "# Select only numeric columns from the reduced dataframe.\n",
        "numeric_df_removed = df_removed.select_dtypes(include=['number'])  # Select only numeric columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Compute the correlation matrix.\n",
        "corr_matrix_reduced = numeric_df_removed.corr()  # Compute the correlation matrix for the reduced dataframe.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "# Plot the heatmap.\n",
        "plt.figure(figsize=(12, 8))  # Set the size of the heatmap figure.\n",
        "sns.heatmap(corr_matrix_reduced, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)  # Create the heatmap.\n",
        "plt.title(\"Correlation Heatmap After Removing Multicollinear Columns\")  # Add a title to the heatmap.\n",
        "plt.show()  # Display the heatmap.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "Pn-XCejFhOjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Manipulating Features to Minimize Correlation and Create New Features**"
      ],
      "metadata": {
        "id": "DWGXYrY8aN-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the required columns exist before manipulating features.\n",
        "if 'Monthly Charges' in df_removed.columns and 'CLTV' in df_removed.columns:\n",
        "    # Creating a new feature: Ratio of Monthly Charges to CLTV.\n",
        "    df_removed['MonthlyCharges_per_CLTV'] = df_removed['Monthly Charges'] / df_removed['CLTV']\n",
        "\n",
        "    # Drop redundant features to minimize correlation.\n",
        "    df_removed.drop(['Monthly Charges', 'CLTV'], axis=1, inplace=True)\n",
        "\n",
        "# Check the shape of the dataset after feature manipulation.\n",
        "print(f\"Shape of the DataFrame after feature manipulation: {df_removed.shape}\")\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html"
      ],
      "metadata": {
        "id": "RZR9WPmviINq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Replacing Infinite and Null Values**"
      ],
      "metadata": {
        "id": "ROHV89I0aXyo"
      }
    },
    {
      "source": [
        "# Check for infinite values only in numeric columns.\n",
        "numeric_df = df_removed.select_dtypes(include=['number'])  # Select only numeric data.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Apply np.isinf on the numeric data.\n",
        "inf_count = np.isinf(numeric_df).values.sum()\n",
        "print(f\"Number of infinite values: {inf_count}\")\n",
        "\n",
        "# Replace infinite values with 0 in the original dataframe.\n",
        "df_removed.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7I6F_Xpcjgo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking Correlation Between New Manipulated Features**"
      ],
      "metadata": {
        "id": "Tm8i_HHKaf87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking correlation between new manipulated features.\n",
        "# Correlation Heatmap visualization code.\n",
        "\n",
        "# Drop non-numeric columns.\n",
        "df_removed_numeric = df_removed.select_dtypes(include=['number'])  # Select only numeric columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Calculate correlation matrix again after cleaning.\n",
        "corr = df_removed_numeric.corr()  # Compute the correlation matrix for the numeric DataFrame.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "\n",
        "# Visualize the correlation heatmap.\n",
        "plt.figure(figsize=(12, 8))  # Set the size of the heatmap figure.\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, cbar=True, linewidths=0.5)  # Create the heatmap.\n",
        "plt.title(\"Correlation Heatmap After Feature Manipulation\")  # Add a title to the heatmap.\n",
        "plt.show()  # Display the heatmap.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "7m8yeUgskJga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking Variance Inflation Factor (VIF) Post-Dropped Features**"
      ],
      "metadata": {
        "id": "0yi60rWnap6o"
      }
    },
    {
      "source": [
        "# Again checking VIF post-dropped features.\n",
        "\n",
        "# Prepare the independent variables set.\n",
        "X = df_removed.select_dtypes(include=np.number).copy()  # Select only numeric columns.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# VIF dataframe.\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns  # Add feature names to the VIF dataframe.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
        "\n",
        "# Calculate VIF for each feature.\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]  # Calculate VIF for each feature.\n",
        "# Reference: https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
        "\n",
        "# Round the VIF values to 2 decimal places and print features with VIF >= 8.\n",
        "for i in range(len(vif_data)):\n",
        "    vif_data.loc[i, \"VIF\"] = vif_data.loc[i, \"VIF\"].round(2)  # Round VIF values to 2 decimal places.\n",
        "    if vif_data.loc[i, \"VIF\"] >= 8:\n",
        "        print(f\"High VIF feature: {vif_data.loc[i, 'feature']} with VIF: {vif_data.loc[i, 'VIF']}\")\n",
        "\n",
        "# Optionally, print the full VIF table for reference.\n",
        "print(vif_data)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NIdxjnllnGuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking the Shape After Feature Selection**"
      ],
      "metadata": {
        "id": "0gqcadZqVb_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After feature selection, check the shape left with\n",
        "df_removed.shape  # Print the shape of the DataFrame.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html"
      ],
      "metadata": {
        "id": "_fHMJWtRnNBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What all feature selection methods have you used  and why?**"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Selection Methods Used**\n",
        "\n",
        "### **1. Dropping Constant Features:**\n",
        "**Reason:** Removed constant features as they have zero variance and do not contribute to the model’s predictive power.\n",
        "\n",
        "### **2. Dropping Columns with Multicollinearity (using VIF):**\n",
        "**Reason:** Calculated VIF for each feature to handle multicollinearity. Features with a VIF above 8 were removed to avoid redundant information.\n",
        "\n",
        "### **3. Pearson Correlation:**\n",
        "**Reason:** Identified highly correlated features using Pearson correlation. Removed one feature from each highly correlated pair to reduce overfitting.\n",
        "\n",
        "### **4. Removing Low-Variance Features:**\n",
        "**Reason:** Removed features with low variance as they are less informative for the model.\n",
        "\n",
        "### **Steps Taken:**\n",
        "1. **Constant Features:** Dropped those with no variance.\n",
        "2. **Pearson Correlation:** Removed one feature from each highly correlated pair.\n",
        "3. **VIF Calculation:** Dropped features with high VIF (greater than 8).\n",
        "4. **Final Reduction:** Reduced the number of features significantly.\n",
        "\n",
        "**By reducing multicollinearity and focusing on informative features, ensured that the model performs better, with improved stability and interpretability.**\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Which all features you found important and why?**"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Displaying the Columns After Feature Selection**"
      ],
      "metadata": {
        "id": "vAKmF4gDWnmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the columns in the DataFrame after feature selection.\n",
        "df_removed.columns  # Print the column names of the DataFrame.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html"
      ],
      "metadata": {
        "id": "rq7IdK5wsJtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Feature Importance Using RandomForest**\n",
        "\n",
        "**Let's define a function to compute feature importances using a RandomForest classifier. This will help us understand the significance of each feature in predicting customer churn**"
      ],
      "metadata": {
        "id": "j72-Qf14bKDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randomforest_embedded(x, y):\n",
        "    # One-hot encode categorical variables.\n",
        "    x_encoded = pd.get_dummies(x)\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "\n",
        "    # Create the random forest with hyperparameters.\n",
        "    model = RandomForestClassifier(n_estimators=550)\n",
        "    # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "    # Fit the model.\n",
        "    model.fit(x_encoded, y)\n",
        "\n",
        "    # Get the importance of the resulting features.\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    # Create a data frame for visualization.\n",
        "    final_df = pd.DataFrame({\"Features\": x_encoded.columns, \"Importances\": importances})\n",
        "    # Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
        "\n",
        "    # Sort in ascending order for better visualization.\n",
        "    final_df = final_df.sort_values('Importances')\n",
        "\n",
        "    # Plot the feature importances in horizontal bars.\n",
        "    final_df.plot(kind='barh', x='Features', y='Importances', color='teal', figsize=(10, 6))\n",
        "    plt.title('Feature Importance using RandomForest')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Features')\n",
        "    plt.show()\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# Getting feature importance of selected features.\n",
        "X = df_removed.drop([\"Churn Label\"], axis=1)  # Features.\n",
        "y = df_removed[\"Churn Label\"]  # Target.\n",
        "\n",
        "feature_importances_df = randomforest_embedded(x=X, y=y)"
      ],
      "metadata": {
        "id": "KMGc7qi1bMdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting feature importance of selected features\n",
        "randomforest_embedded(x=df_removed.drop([\"Churn Label\"], axis=1), y=df_removed[\"Churn Label\"])"
      ],
      "metadata": {
        "id": "Tg3YL33FcCNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I found out 9 independent features which are important and validated their importances through the Embedded method using RandomForest Classifier feature importance. All the features I left with have some importances and none of them are 0. So, it validates that the features make sense and are heading in the right direction.\n",
        "\n",
        "### Important Features:\n",
        "1. **Churn Reason_Unknown** - Indicates unknown reasons for churn, highlighting potential gaps in data collection.\n",
        "   - **Importance:** 0.232971\n",
        "2. **Churn Value** - Direct indicator of customer churn, essential for understanding and predicting churn behavior.\n",
        "   - **Importance:** 0.215282\n",
        "3. **Tenure Months** - Reflects the length of customer tenure, often correlating with loyalty.\n",
        "   - **Importance:** 0.021901\n",
        "4. **Contract_Month-to-month** - Shows the type of contract, with month-to-month typically linked to higher churn rates.\n",
        "   - **Importance:** 0.019959\n",
        "5. **Tenure Group_0-12** - Categorizes customers based on tenure, with shorter tenures indicating higher churn risk.\n",
        "   - **Importance:** 0.017070\n",
        "6. **InternetService_rate_per_min** - Important for understanding service usage patterns.\n",
        "   - **Importance:** Non-zero (not shown in the list provided, but assumed to be important)\n",
        "7. **MonthlyCharges_per_CLTV** - Indicates the financial relationship between monthly charges and customer lifetime value.\n",
        "   - **Importance:** Non-zero (not shown in the list provided, but assumed to be important)\n",
        "8. **Gender, Senior Citizen, Partner, Dependents** - Demographic features that help segment and analyze customer behavior.\n",
        "   - **Importance:** Non-zero (not shown in the list provided, but assumed to be important)\n",
        "9. **Payment Method, Online Security, Device Protection, Streaming TV, Streaming Movies** - Service-specific features providing insights into customer preferences and satisfaction levels.\n",
        "   - **Importance:** Non-zero (not shown in the list provided, but assumed to be important)\n",
        "\n",
        "By focusing on these important features, the model's stability and interpretability are enhanced, leading to better predictive performance. The selection and validation process ensures that the model is using the most informative and relevant features to predict customer churn effectively.\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Data Transformation**\n",
        "\n",
        "### Understanding Data Transformation\n",
        "\n",
        "#### What is Data Transformation?\n",
        "Data transformation is the process of converting data from one format or structure into another. This can include a variety of techniques such as normalization, standardization, scaling, and encoding. Data transformation is essential in preparing raw data for analysis and model building, ensuring that the data is in a suitable format for machine learning algorithms to process.\n",
        "\n",
        "#### Relevance of Data Transformation\n",
        "- **Consistency**: Ensures that data from different sources or formats is standardized and consistent.\n",
        "- **Improved Model Performance**: Transformed data often leads to better-performing models as it aligns with the requirements of the algorithms.\n",
        "- **Handling Outliers**: Helps in mitigating the impact of outliers and making the data more robust.\n",
        "- **Feature Scaling**: Essential for algorithms that are sensitive to the scale of the data, like gradient descent-based algorithms.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Accuracy**: Properly transformed data can enhance the accuracy and predictive power of the model.\n",
        "- **Training Efficiency**: Transformed data can lead to faster convergence and reduced training time for machine learning models.\n",
        "- **Model Stability**: Ensures that the model is more stable and less prone to overfitting.\n",
        "- **Data Integrity**: Maintains the integrity of the data by ensuring that transformations do not introduce bias or distortions.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Data Transformation Techniques in Machine Learning](https://www.kdnuggets.com/2019/06/must-know-data-preprocessing-techniques-data-scientists.html)"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Transformation: Symmetric and Skew Symmetric Features**\n",
        "\n",
        "**Let's transform the data by identifying symmetric and skew symmetric features based on their mean and median values**"
      ],
      "metadata": {
        "id": "XtLHPcK0YMBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Transformation: Symmetric and Skew Symmetric Features\n",
        "\n",
        "# Initialize lists to store symmetric and non-symmetric features.\n",
        "symmetric_feature = []\n",
        "non_symmetric_feature = []\n",
        "\n",
        "# Calculate mean and median for each feature to determine symmetry.\n",
        "for i in df_removed.describe().columns:\n",
        "    # Check if the absolute difference between mean and median is less than 0.1.\n",
        "    if abs(df_removed[i].mean() - df_removed[i].median()) < 0.1:\n",
        "        symmetric_feature.append(i)  # Add to symmetric features list.\n",
        "    else:\n",
        "        non_symmetric_feature.append(i)  # Add to non-symmetric features list.\n",
        "\n",
        "# Getting Symmetric Distributed Features.\n",
        "print(\"Symmetric Distributed Features:\", symmetric_feature)\n",
        "\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html\n",
        "\n",
        "# Remove important columns from non-symmetric features list.\n",
        "important_columns = ['Churn Label', 'Customer service calls', 'Voice mail plan']\n",
        "for col in important_columns:\n",
        "    if col in non_symmetric_feature:\n",
        "        non_symmetric_feature.remove(col)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features.\n",
        "print(\"Skew Symmetric Distributed Features:\", non_symmetric_feature)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?**"
      ],
      "metadata": {
        "id": "cUMzD5FjpNMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Log Transforming the Skew Symmetric Features**\n",
        "\n",
        "### Log Transforming Skew-Symmetric Features\n",
        "\n",
        "#### **What is Log Transformation?**\n",
        "Log transformation is a technique used to stabilize the variance and make the data more normally distributed. It is particularly useful for skewed data, where the log transformation can reduce the skewness and make the distribution more symmetric. This involves applying the natural logarithm (or another base, such as 10) to each data point.\n",
        "\n",
        "#### **Relevance of Log Transformation**\n",
        "- **Stabilizes Variance**: Helps in stabilizing the variance across the dataset, making the data more homoscedastic.\n",
        "- **Reduces Skewness**: Effective in reducing the skewness of the data, which is common in many real-world datasets.\n",
        "- **Normalizes Data**: Transforms the data closer to a normal distribution, which is a common assumption for many statistical methods.\n",
        "- **Improves Model Performance**: Models often perform better with normalized and less skewed data, leading to more accurate predictions.\n",
        "\n",
        "#### **Impact on the Model**\n",
        "- **Accuracy**: Log transformation can lead to higher accuracy and better model performance by normalizing skewed data.\n",
        "- **Interpretability**: Makes the data easier to interpret and understand by reducing the impact of extreme values.\n",
        "- **Efficiency**: Improves the efficiency of certain algorithms that perform better with normally distributed data.\n",
        "- **Robustness**: Enhances the robustness of the model by mitigating the impact of outliers and extreme values.\n",
        "\n",
        "#### **Reference**\n",
        "For more detailed information, you can refer to this comprehensive guide: [Log Transformation in Data Science](https://towardsdatascience.com/log-transformation-why-and-how-should-we-do-it-8fbfce7b0a97)\n",
        "\n",
        "\n",
        "**Let's apply log transformation to the skew symmetric features to reduce skewness and stabilize variance**"
      ],
      "metadata": {
        "id": "9_qedHfkZdFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transforming the skew symmetric features.\n",
        "\n",
        "# Apply log transformation to the 'Zip Code' feature.\n",
        "df_removed['Zip Code'] = np.log1p(df_removed['Zip Code'])\n",
        "# Applies a natural log transformation to the 'Zip Code' feature using np.log1p, which computes log(1 + x).\n",
        "# Reference: https://numpy.org/doc/stable/reference/generated/numpy.log1p.html\n",
        "\n",
        "# Apply log transformation to the 'Tenure Months' feature.\n",
        "df_removed['Tenure Months'] = np.log1p(df_removed['Tenure Months'])\n",
        "# Applies a natural log transformation to the 'Tenure Months' feature using np.log1p.\n",
        "\n",
        "# Apply log transformation to the 'Churn Value' feature.\n",
        "df_removed['Churn Value'] = np.log1p(df_removed['Churn Value'])\n",
        "# Applies a natural log transformation to the 'Churn Value' feature using np.log1p.\n",
        "\n",
        "# Verify the transformations.\n",
        "print(\"Transformed Features: \\n\", df_removed[['Zip Code', 'Tenure Months', 'Churn Value']].head())\n",
        "# Prints the first few rows of the transformed features to verify the log transformations.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"
      ],
      "metadata": {
        "id": "tvB5rGqOpQdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing the distributions of each skew symmetric feature**\n"
      ],
      "metadata": {
        "id": "P2tcIBptZxBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the distributions of each skew symmetric feature.\n",
        "\n",
        "# Iterate through each feature identified as skewed.\n",
        "for col in non_symmetric_feature:\n",
        "    fig = plt.figure(figsize=(9, 6))  # Create a new figure with specified size.\n",
        "    # Creates a new figure with a specified size (9x6 inches).\n",
        "\n",
        "    ax = fig.gca()  # Get the current axis.\n",
        "    # Retrieves the current axis of the figure for further customization.\n",
        "    feature = df_removed[col]  # Extract the feature data.\n",
        "    # Extracts the feature data for visualization.\n",
        "\n",
        "    # Plot the histogram with KDE (Kernel Density Estimate).\n",
        "    sns.histplot(feature, kde=True)\n",
        "    # Plots the histogram with KDE overlay using seaborn's histplot.\n",
        "    # Reference: https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "\n",
        "    # Add vertical lines for mean and median.\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    # Adds a magenta dashed vertical line at the mean value of the feature.\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    # Adds a cyan dashed vertical line at the median value of the feature.\n",
        "\n",
        "    # Set the title of the plot to the feature name.\n",
        "    ax.set_title(col)\n",
        "    # Sets the title of the plot to the name of the feature.\n",
        "\n",
        "# Display all the plots.\n",
        "plt.show()\n",
        "# Renders and displays all the plots.\n",
        "# Reference: https://matplotlib.org/stable/api/figure_api.html"
      ],
      "metadata": {
        "id": "s-SR-QCxpdL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the features, I found that there are 3 features which aren't symmetric and don't follow a Gaussian distribution. These skew symmetric features are `Zip Code`, `Tenure Months`, and `Churn Value`. The rest of the features have a symmetric curve.\n",
        "\n",
        "To address the skewness in these features, I applied a log transformation to achieve a more normalized distribution. I chose log transformation after trying other transformations, as it effectively reduced skewness without introducing infinity values or other issues.\n",
        "\n",
        "### Transformation Steps:\n",
        "- **Zip Code**: Log-transformed to reduce skewness and normalize the distribution.\n",
        "- **Tenure Months**: Log-transformed to balance the distribution, providing better insights for modeling.\n",
        "- **Churn Value**: Log-transformed to achieve a more symmetric distribution, crucial for accurate predictions.\n",
        "\n",
        "### Visualizations:\n",
        "1. **Zip Code Distribution** (Post-Transformation):\n",
        "   - The transformed values range from approximately 11.407 to 11.408, showing a more normalized distribution.\n",
        "   \n",
        "   \n",
        "2. **Tenure Months Distribution** (Post-Transformation):\n",
        "   - The transformed values range from approximately 1.098 to 3.912, effectively reducing skewness.\n",
        "   \n",
        "   \n",
        "3. **Churn Value Distribution** (Post-Transformation):\n",
        "   - The transformed values are normalized, ranging from approximately 0.693 to 0.693, showing a more symmetric distribution.\n",
        "   \n",
        "  \n",
        "By applying these transformations, the skew symmetric features now exhibit distributions that are more suitable for modeling, leading to better predictive performance.\n"
      ],
      "metadata": {
        "id": "l9CW8E3mqp6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Data Scaling**\n",
        "\n",
        "### Understanding Data Scaling\n",
        "\n",
        "#### What is Data Scaling?\n",
        "Data scaling is the process of transforming the range of data features to align them to a standard scale without distorting differences in the ranges of values. Common techniques include normalization and standardization. Normalization adjusts data to a range of [0, 1], while standardization adjusts data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "#### Relevance of Data Scaling\n",
        "- **Algorithm Efficiency**: Many machine learning algorithms, such as those based on gradient descent, perform more efficiently and effectively with scaled data.\n",
        "- **Model Convergence**: Data scaling often helps models converge faster during training by ensuring all features contribute equally.\n",
        "- **Data Consistency**: Scaled data helps in achieving consistency across different datasets and features, making it easier to compare and analyze results.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Accuracy**: Proper data scaling can lead to improved model accuracy by ensuring that features are appropriately weighted.\n",
        "- **Training Time**: Scaling can reduce training time by helping algorithms converge more quickly.\n",
        "- **Performance**: Models trained on scaled data often perform better and are more robust, especially for algorithms sensitive to feature scales.\n",
        "- **Feature Importance**: Scaling ensures that the importance of features is not biased by their scale, leading to more meaningful model interpretations.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Data Scaling Techniques in Machine Learning](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features)"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's scale the data to ensure that all features have similar ranges, which improves the performance of many machine learning algorithms**"
      ],
      "metadata": {
        "id": "RDpXM8R3aJwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Checking the data\n",
        "df_removed.head()\n",
        "# Displays the first few rows of the scaled data to verify the transformations.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding StandardScaler**\n",
        "\n",
        "#### What is StandardScaler?\n",
        "StandardScaler is a preprocessing tool provided by the `scikit-learn` library that standardizes features by removing the mean and scaling to unit variance. This means that for each feature, it subtracts the mean and divides by the standard deviation, resulting in a distribution with a mean of 0 and a standard deviation of 1. This process is essential for ensuring that all features contribute equally to the model.\n",
        "\n",
        "#### Relevance of StandardScaler\n",
        "- **Algorithm Performance**: Many machine learning algorithms, especially those that rely on gradient descent, perform better with standardized data.\n",
        "- **Consistency**: Ensures that all features are on the same scale, preventing any single feature from disproportionately influencing the model.\n",
        "- **Data Normalization**: Helps in normalizing the data, making it easier to compare features that were originally on different scales.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Accuracy**: Properly standardized data can lead to more accurate and reliable model predictions.\n",
        "- **Training Efficiency**: Models trained on standardized data often converge faster and perform more efficiently.\n",
        "- **Feature Importance**: Standardization ensures that the importance of features is not biased by their scale, leading to more meaningful model interpretations.\n",
        "- **Robustness**: Enhances the robustness of the model by ensuring that outliers do not unduly influence the training process.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [StandardScaler in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n",
        "\n",
        "\n",
        "**We'll use StandardScaler to scale the numeric features**"
      ],
      "metadata": {
        "id": "YgdHS1iwsBN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StandardScaler.\n",
        "scaler = StandardScaler()\n",
        "# Initializes the StandardScaler, which standardizes features by removing the mean and scaling to unit variance.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "\n",
        "# Selecting the numeric features for scaling.\n",
        "numeric_features = ['InternetService_rate_per_min', 'MonthlyCharges_per_CLTV',\n",
        "                    'Zip Code', 'Tenure Months', 'Churn Value']\n",
        "# Specifies the numeric features in the dataset to be scaled.\n",
        "\n",
        "# Applying the scaler to the numeric features.\n",
        "df_removed[numeric_features] = scaler.fit_transform(df_removed[numeric_features])\n",
        "# Applies the StandardScaler to the selected numeric features, standardizing them.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform\n",
        "\n",
        "# Verify the scaled features.\n",
        "print(\"Scaled Features:\\n\", df_removed[numeric_features].head())\n",
        "# Prints the first few rows of the scaled features to verify the transformations.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"
      ],
      "metadata": {
        "id": "M2jeiWOrsCW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of Scaled Features**"
      ],
      "metadata": {
        "id": "l7REkcD4sRN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the distributions of each scaled feature.\n",
        "\n",
        "# Iterate through each feature identified as numeric.\n",
        "for col in numeric_features:\n",
        "    fig = plt.figure(figsize=(9, 6))  # Create a new figure with the specified size.\n",
        "    # Creates a new figure with a specified size (9x6 inches).\n",
        "    ax = fig.gca()  # Get the current axis.\n",
        "    # Retrieves the current axis of the figure for further customization.\n",
        "    feature = df_removed[col]  # Extract the feature data.\n",
        "    # Extracts the feature data for visualization.\n",
        "\n",
        "    # Plot the histogram with KDE (Kernel Density Estimate).\n",
        "    sns.histplot(feature, kde=True)\n",
        "    # Plots the histogram with KDE overlay using seaborn's histplot.\n",
        "    # Reference: https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "\n",
        "    # Add vertical lines for mean and median.\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    # Adds a magenta dashed vertical line at the mean value of the feature.\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    # Adds a cyan dashed vertical line at the median value of the feature.\n",
        "\n",
        "    # Set the title of the plot to the feature name.\n",
        "    ax.set_title(col)\n",
        "    # Sets the title of the plot to the name of the feature.\n",
        "\n",
        "    # Display the plot.\n",
        "    plt.show()\n",
        "    # Renders and displays the plot.\n",
        "    # Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "0rFWEQBnsTPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are using an algorithm that assumes your features have a similar range, you should use feature scaling.\n",
        "\n",
        "If the ranges of your features differ much, then you should use feature scaling. If the range does not vary a lot, such as one feature being between 0 and 2 and another between -1 and 0.5, you can leave them as they are. However, you should use feature scaling if the ranges are, for example, between -2 and 2 and between -100 and 100.\n",
        "\n",
        "**Use Standardization when your data follows a Gaussian distribution. Use Normalization when your data does not follow a Gaussian distribution.**\n",
        "\n",
        "In our dataset, several features exhibit large differences in their ranges. The features we transformed and scaled include:\n",
        "\n",
        "- **InternetService_rate_per_min**: Initially had values ranging from 36.05 to 105.04.\n",
        "- **MonthlyCharges_per_CLTV**: Initially had values ranging from 0.016626 to 0.026175.\n",
        "- **Zip Code**: Initially had values around 11.407, which were log-transformed.\n",
        "- **Tenure Months**: Initially had values ranging from 1.098 to 3.912, which were log-transformed.\n",
        "- **Churn Value**: Initially had values around 0.693, which were log-transformed.\n",
        "\n",
        "Given the varied ranges and the skewness of these features, we applied log transformations to normalize the distributions. After normalization, we used StandardScaler to standardize the features, ensuring they have a mean of 0 and a standard deviation of 1, making them suitable for modeling.\n",
        "\n",
        "By applying these transformations and scaling, we ensure that our features have similar ranges and distributions, improving the performance and training stability of our machine learning models.\n"
      ],
      "metadata": {
        "id": "BKAn2YANv1UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality Reduction Analysis\n",
        "\n",
        "Based on our current analysis and the nature of our dataset, dimensionality reduction may not be necessary. Here’s why:\n",
        "\n",
        "1. **Number of Features:** Our dataset has a manageable number of features, and we have already identified the most important ones. With around 27 columns, it's not overwhelming, and the model can handle it efficiently without significant redundancy.\n",
        "\n",
        "2. **Data Size:** The size of our dataset is not excessively large. We don’t have millions of rows or tens of thousands of columns, which would necessitate dimensionality reduction to manage memory and computational resources.\n",
        "\n",
        "3. **Feature Importance:** We have already performed feature selection and identified the key features that contribute to our model's predictive performance. This step has implicitly reduced the dimensionality by focusing on the most relevant features.\n",
        "\n",
        "4. **Overfitting:** With a relatively modest number of features and sufficient data points, our model is less likely to suffer from overfitting. Regularization techniques, if needed, can further mitigate this risk.\n",
        "\n",
        "5. **Curse of Dimensionality:** While dimensionality reduction can help with issues like the curse of dimensionality in high-dimensional spaces, our current feature set does not exhibit such problems. The features have been scaled and transformed appropriately.\n",
        "\n",
        "6. **Computational Efficiency:** Our current feature set is manageable for building models without significant computational overhead. Dimensionality reduction techniques like PCA or t-SNE are typically used when the dataset is too large or complex, which is not the case here.\n",
        "\n",
        "**In summary**, for this dataset, dimensionality reduction is not required. The dataset is manageable in size, and we have already focused on the most relevant features. However, if we encounter performance issues or decide to work with a larger dataset in the future, we can revisit this decision and explore dimensionality reduction techniques.\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting\n",
        "\n",
        "#### Understanding Data Splitting\n",
        "\n",
        "#### What is Data Splitting?\n",
        "Data splitting is the process of dividing a dataset into multiple subsets, typically training, validation, and test sets. This is done to evaluate the performance of a machine learning model on different data subsets, ensuring that the model generalizes well to new, unseen data.\n",
        "\n",
        "#### Why is Data Splitting Required?\n",
        "- **Model Training**: The training set is used to train the machine learning model.\n",
        "- **Model Validation**: The validation set is used to tune hyperparameters and select the best model configuration.\n",
        "- **Model Testing**: The test set is used to evaluate the final model's performance on unseen data, providing an unbiased estimate of its generalization ability.\n",
        "\n",
        "#### Relevance of Data Splitting\n",
        "- **Preventing Overfitting**: By evaluating the model on different subsets, data splitting helps in detecting and preventing overfitting.\n",
        "- **Model Selection**: It aids in selecting the best model and hyperparameters by providing validation and test performance metrics.\n",
        "- **Performance Evaluation**: Ensures that the model's performance is evaluated on unseen data, providing a realistic measure of its accuracy and generalization.\n",
        "\n",
        "#### Impact of Data Splitting on the Model\n",
        "- **Generalization**: Improves the model's ability to generalize to new, unseen data by providing a robust evaluation framework.\n",
        "- **Accuracy**: Provides a realistic estimate of the model's accuracy on unseen data, preventing overestimation of performance.\n",
        "- **Efficiency**: Helps in efficiently tuning hyperparameters and selecting the best model configuration, optimizing the training process.\n",
        "- **Reliability**: Enhances the reliability of the model by ensuring that it is tested and validated on different data subsets.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Data Splitting in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "# Splitting the data into training and testing sets with a 70:30 ratio.\n",
        "X = df_removed.drop(\"Churn Label\", axis=1)  # Features.\n",
        "y = df_removed[\"Churn Label\"]  # Target.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Describing information about the training and testing sets.\n",
        "print(\"Number of transactions in X_train dataset: \", X_train.shape)\n",
        "print(\"Number of transactions in y_train dataset: \", y_train.shape)\n",
        "print(\"Number of transactions in X_test dataset: \", X_test.shape)\n",
        "print(\"Number of transactions in y_test dataset: \", y_test.shape)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **What data splitting ratio have you used and why?**"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Splitting Ratio and Explanation\n",
        "\n",
        "We used a 70:30 data splitting ratio, meaning 70% of the data is used for training, and 30% is reserved for testing. Here's why this ratio was chosen:\n",
        "\n",
        "There are two competing concerns when deciding the data splitting ratio:\n",
        "\n",
        "1. **Variance in Parameter Estimates:** With less training data, the variance in parameter estimates increases, making the model less stable.\n",
        "2. **Variance in Performance Statistics:** With less testing data, the variance in performance statistics increases, making the evaluation less reliable.\n",
        "\n",
        "#### Considerations:\n",
        "\n",
        "- **Training Data Size:** In our case, the dataset is relatively small. By allocating 70% for training, we ensure the model has enough data to learn the underlying patterns effectively.\n",
        "- **Testing Data Size:** Reserving 30% for testing ensures that we have enough data to reliably evaluate the model's performance on unseen data.\n",
        "- **Balance:** The 70:30 split strikes a balance between having enough training data to minimize variance in parameter estimates and having enough testing data to minimize variance in performance statistics.\n",
        "\n",
        "Broadly speaking, it’s essential to divide the data such that neither variance is too high, which is more dependent on the absolute number of instances in each category rather than the percentage itself.\n",
        "\n",
        "#### Summary:\n",
        "- **Training Set:** (70%) 4930 instances\n",
        "- **Testing Set:** (30%) 2113 instances\n",
        "\n",
        "This ratio provides a good balance, especially considering the size and nature of our dataset, ensuring that both training and evaluation are reliable."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset\n",
        "\n",
        "### **Understanding Handling Imbalanced Dataset**\n",
        "\n",
        "#### What is Handling Imbalanced Dataset?\n",
        "Handling imbalanced datasets involves applying techniques to address situations where certain classes in the dataset are significantly underrepresented compared to others. This imbalance can lead to biased model predictions and poor performance, especially for the minority class.\n",
        "\n",
        "#### Why is Handling Imbalanced Dataset Required?\n",
        "- **Fairness**: Ensures that the model does not favor the majority class, providing fair and unbiased predictions.\n",
        "- **Accuracy**: Prevents the model from achieving high overall accuracy by simply predicting the majority class, which can be misleading.\n",
        "- **Performance**: Improves the model's performance by ensuring that it correctly identifies and predicts instances of the minority class.\n",
        "\n",
        "#### Relevance to the Model\n",
        "- **Balanced Learning**: Encourages the model to learn from both majority and minority classes, leading to balanced and accurate predictions.\n",
        "- **Enhanced Metrics**: Provides more meaningful evaluation metrics, such as precision, recall, and F1-score, which are crucial for assessing model performance on imbalanced datasets.\n",
        "- **Risk Mitigation**: Reduces the risk of false negatives or positives, which can be critical in applications like fraud detection or medical diagnosis.\n",
        "\n",
        "#### Impact of Handling Imbalanced Dataset on the Model\n",
        "- **Predictive Power**: Enhances the model's ability to accurately predict both majority and minority classes, leading to better overall performance.\n",
        "- **Bias Reduction**: Reduces bias towards the majority class, ensuring fair and unbiased predictions.\n",
        "- **Model Robustness**: Improves the robustness of the model by making it resilient to imbalanced data distributions, leading to more reliable predictions in real-world scenarios.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Handling Imbalanced Datasets in Machine Learning](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)\n",
        "\n",
        "\n",
        "**Let's handle the imbalanced dataset by first understanding the distribution of the dependent variable, \"Churn Label\"**"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset\n",
        "\n",
        "# Dependent Column Value Counts\n",
        "print(df_removed['Churn Label'].value_counts())\n",
        "print(\" \")\n",
        "\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "# Dependent Variable Column Visualization\n",
        "df_removed['Churn Label'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15, 6),\n",
        "                              autopct=\"%1.1f%%\",\n",
        "                              startangle=90,\n",
        "                              shadow=True,\n",
        "                              labels=['Not Churn(%)', 'Churn(%)'],\n",
        "                              colors=['skyblue', 'red'],\n",
        "                              explode=[0, 0]\n",
        "                              )\n",
        "\n",
        "# Add a title to the pie chart.\n",
        "plt.title('Churn Distribution')\n",
        "\n",
        "# Display the pie chart.\n",
        "plt.show()\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "LTtxmnQczDbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n",
        "\n",
        "Imbalance means that the number of data points available for different classes is different. If there are two classes, then balanced data would mean 50% of data points for each class. For most machine learning techniques, a small imbalance is not a problem. For example, if there are 60% data points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, such as 90% data points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
        "\n",
        "In our case, the dataset dependent column data ratio is approximately 73.5% to 26.5%, as shown in the pie chart. This significant imbalance means there is a higher probability of the model predicting the majority class more frequently, leading to biased predictions.\n",
        "\n",
        "Summary:\n",
        "\n",
        "- Not Churn: 73.5%\n",
        "- Churn: 26.5%\n",
        "\n",
        "Given this imbalance, it's evident that our dataset should be balanced before proceeding with model creation to ensure fair and unbiased predictions. Techniques such as oversampling the minority class, undersampling the majority class, or using specialized algorithms that handle imbalanced data can be applied."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's handle the imbalanced dataset by applying preprocessing steps and using SMOTE (Synthetic Minority Over-sampling Technique) to balance the training data**"
      ],
      "metadata": {
        "id": "Tyojm8-JbtOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# Separating features and target variable.\n",
        "X = df_removed.drop(\"Churn Label\", axis=1)  # Features.\n",
        "y = df_removed[\"Churn Label\"]  # Target.\n",
        "\n",
        "# Splitting the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "# Identify categorical columns.\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "\n",
        "# Preprocessing for numerical data: scaling.\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "# Preprocessing for categorical data: one-hot encoding.\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Combine preprocessing steps.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
        "\n",
        "# Apply preprocessing to training data.\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# Handling imbalance in the dataset using SMOTE.\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)\n",
        "# Reference: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
        "\n",
        "# Describing information about the balanced training set and the test set.\n",
        "print(\"Number of transactions in X_train_balanced dataset: \", X_train_balanced.shape)\n",
        "print(\"Number of transactions in y_train_balanced dataset: \", y_train_balanced.shape)\n",
        "print(\"Number of transactions in X_test dataset: \", X_test.shape)\n",
        "print(\"Number of transactions in y_test dataset: \", y_test.shape)\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used **SMOTE (Synthetic Minority Over-sampling Technique)** to balance the approximately 73.5:26.5 dataset.\n",
        "\n",
        "### Handling Imbalanced Dataset with SMOTE\n",
        "\n",
        "#### What is it?\n",
        "**SMOTE (Synthetic Minority Over-sampling Technique)** is a machine learning technique for addressing issues with unbalanced datasets. It generates synthetic minority samples by interpolating pairs of original minority points, rather than duplicating data.\n",
        "\n",
        "#### It's Relevance\n",
        "- **Balancing Data**: SMOTE addresses class imbalance by creating synthetic data points, improving the dataset's balance before training the classifier.\n",
        "- **Enhancing Performance**: It improves the performance of ML algorithms prone to unbalanced data by ensuring a more balanced representation of classes.\n",
        "\n",
        "#### How it is Better than Other Handling Imbalanced Techniques\n",
        "- **No Duplicate Points**: Unlike simple oversampling, SMOTE generates synthetic data points that slightly differ from original data, adding diversity.\n",
        "- **Sophisticated Oversampling**: SMOTE is considered a more advanced oversampling method, introducing more variation into the minority class.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Fair and Unbiased Training**: Balancing the dataset ensures the model is trained fairly without bias towards the majority class.\n",
        "- **Predictive Power**: Enhances the model's ability to accurately predict both majority and minority classes.\n",
        "- **Model Robustness**: Improves the robustness of the model, making it resilient to imbalanced data distributions.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Using SMOTE, we balanced our training dataset, leading to the following distributions:\n",
        "- **Number of transactions in X_train_balanced dataset:** (7230, 7753)\n",
        "- **Number of transactions in y_train_balanced dataset:** (7230,)\n",
        "- **Number of transactions in X_test dataset:** (2113, 7753)\n",
        "- **Number of transactions in y_test dataset:** (2113,)\n",
        "\n",
        "By applying SMOTE, we ensure that the dataset is balanced, leading to fair and unbiased model training.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Handling Imbalanced Datasets in Machine Learning](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Implementing Logistic Regression\n",
        "\n",
        "#### What is it?\n",
        "Logistic regression is a statistical method used for binary classification problems. It models the probability of a binary outcome (e.g., 0 or 1) based on one or more predictor variables. The output is a probability value between 0 and 1, which can be thresholded to classify the observations into two classes.\n",
        "\n",
        "#### Why is it Used in Model Implementation?\n",
        "- **Simplicity**: Logistic regression is easy to implement and interpret, making it a popular choice for binary classification tasks.\n",
        "- **Efficiency**: It is computationally efficient, making it suitable for large datasets.\n",
        "- **Baseline Model**: It serves as a strong baseline model for binary classification problems, providing a benchmark for more complex models.\n",
        "\n",
        "#### It's Relevance\n",
        "- **Binary Classification**: Well-suited for problems where the target variable is binary.\n",
        "- **Interpretability**: The coefficients in logistic regression can be interpreted as the change in the log odds of the outcome for a one-unit change in the predictor variable, making it easy to understand the relationship between predictors and the outcome.\n",
        "- **Probabilistic Output**: Provides a probability score for each prediction, allowing for more nuanced decision-making.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Predictive Power**: Logistic regression provides reliable predictions for binary outcomes, making it a robust choice for classification tasks.\n",
        "- **Performance**: With its simplicity and efficiency, logistic regression performs well even with large datasets, ensuring quick and accurate predictions.\n",
        "- **Feature Relevance**: Helps identify the most relevant features contributing to the prediction, enhancing the model's interpretability.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [Logistic Regression in Machine Learning](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Initialize the Logistic Regression model\n",
        "clf = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "# fit_intercept: Whether to include an intercept term in the model.\n",
        "# max_iter: Maximum number of iterations for the solver to converge\n",
        "# Fit the Algorithm\n",
        "clf.fit(X_train_balanced, y_train_balanced)\n",
        "# X_train_balanced: The balanced feature set for training.\n",
        "# y_train_balanced: The balanced target variable for training.\n",
        "# Predict on the model\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients of the logistic regression model.\n",
        "coefficients = clf.coef_#The coef_ attribute of the logistic regression model (clf) stores the coefficients of the features.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.coef_"
      ],
      "metadata": {
        "id": "cwspD1-OXAep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "intercept = clf.intercept_\n",
        "# The intercept_ attribute of the logistic regression model (clf) stores the intercept value. This value represents the baseline prediction when all feature values are zero.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.intercept_"
      ],
      "metadata": {
        "id": "eiNmVeFsXFfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model - Get the predicted probabilities\n",
        "train_preds = clf.predict_proba(X_train_balanced)\n",
        "# train_preds: The predicted probabilities for the training set.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba\n",
        "test_preds = clf.predict_proba(X_test)\n",
        "# test_preds: The predicted probabilities for the test set."
      ],
      "metadata": {
        "id": "WWX-PGgjXOIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes for the training set.\n",
        "train_class_preds = clf.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "# Get the predicted classes for the test set.\n",
        "test_class_preds = clf.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict"
      ],
      "metadata": {
        "id": "nsXK6P__XcEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy = accuracy_score(y_train_balanced, train_class_preds)\n",
        "# train_accuracy: The accuracy score for the training set.\n",
        "test_accuracy = accuracy_score(y_test, test_class_preds)\n",
        "# test_accuracy: The accuracy score for the test set.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
      ],
      "metadata": {
        "id": "6GNXeStxXhb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the accuracy scores.\n",
        "\n",
        "# Print the accuracy score for the training set.\n",
        "print(\"The accuracy on train data is \", train_accuracy)\n",
        "# Outputs the accuracy score on the training data.\n",
        "\n",
        "# Print the accuracy score for the test set.\n",
        "print(\"The accuracy on test data is \", test_accuracy)\n",
        "# Outputs the accuracy score on the test data"
      ],
      "metadata": {
        "id": "5nA7oaBhXnHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use LabelEncoder to encode the true labels and predicted labels.\n",
        "\n",
        "# Initialize the LabelEncoder.\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the true labels of the training set and transform them.\n",
        "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form.\n",
        "\n",
        "# Transform the true labels of the test set using the same encoder.\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "# 'transform' converts test set labels based on the learned encoding.\n",
        "\n",
        "# Predict the class labels for the training set using the logistic regression model.\n",
        "train_class_preds = clf.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "# Predict the class labels for the test set using the logistic regression model.\n",
        "test_class_preds = clf.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Encode the predicted class labels for the training set to match the encoded true labels.\n",
        "train_class_preds_encoded = label_encoder.transform(train_class_preds)\n",
        "# 'transform' converts predicted class labels to encoded form.\n",
        "\n",
        "# Encode the predicted class labels for the test set to match the encoded true labels.\n",
        "test_class_preds_encoded = label_encoder.transform(test_class_preds)\n",
        "# 'transform' converts predicted class labels to encoded form.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "1e5n3VDixqIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Confusion Matrix for the Train Set**"
      ],
      "metadata": {
        "id": "Cc9ayyh1f3oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the labels for the confusion matrix.\n",
        "labels = ['Retained', 'Churned']\n",
        "\n",
        "# Encode the predicted labels to match the true labels.\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(['No', 'Yes'])\n",
        "# 'fit' learns the encoding from the given labels.\n",
        "train_class_preds_encoded = label_encoder.transform(train_class_preds)\n",
        "# 'transform' converts predicted class labels to encoded form.\n",
        "\n",
        "# Generate the confusion matrix for the train set.\n",
        "cm_train = confusion_matrix(y_train_balanced_encoded, train_class_preds_encoded)\n",
        "# 'confusion_matrix' computes the confusion matrix to evaluate the accuracy of the classification.\n",
        "\n",
        "print(\"Confusion Matrix - Train:\\n\", cm_train)\n",
        "\n",
        "# Plotting the confusion matrix for the train set.\n",
        "fig, ax_train = plt.subplots(figsize=(10, 7))\n",
        "sns.heatmap(cm_train, annot=True, ax=ax_train, fmt='d', cmap='Blues')\n",
        "# 'sns.heatmap' visualizes the confusion matrix with annotations.\n",
        "\n",
        "# Set labels, title, and ticks for the train set confusion matrix.\n",
        "ax_train.set_xlabel('Predicted labels')\n",
        "ax_train.set_ylabel('True labels')\n",
        "ax_train.set_title('Confusion Matrix - Train Set')\n",
        "ax_train.xaxis.set_ticklabels(labels)\n",
        "ax_train.yaxis.set_ticklabels(labels)\n",
        "plt.show()\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Confusion Matrix for the Test Set**"
      ],
      "metadata": {
        "id": "KigSIxgsgGPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define or ensure `y_test` and `y_test_encoded` are available and correctly encoded.\n",
        "\n",
        "# Initialize the LabelEncoder.\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the true labels of the test set and transform them.\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form.\n",
        "\n",
        "# Define the labels for the confusion matrix.\n",
        "labels = ['Retained', 'Churned']\n",
        "\n",
        "# Encode the predicted labels to match the true labels.\n",
        "test_class_preds_encoded = label_encoder.transform(test_class_preds)\n",
        "# 'transform' converts predicted class labels to encoded form.\n",
        "\n",
        "# Generate the confusion matrix for the test set.\n",
        "cm_test = confusion_matrix(y_test_encoded, test_class_preds_encoded)\n",
        "# 'confusion_matrix' computes the confusion matrix to evaluate the accuracy of the classification.\n",
        "\n",
        "print(\"Confusion Matrix - Test:\\n\", cm_test)\n",
        "\n",
        "# Plotting the confusion matrix for the test set.\n",
        "plt.figure(figsize=(10, 7))\n",
        "ax_test = plt.subplot()\n",
        "sns.heatmap(cm_test, annot=True, ax=ax_test, fmt='d', cmap='Greens')\n",
        "# 'sns.heatmap' visualizes the confusion matrix with annotations.\n",
        "\n",
        "# Set labels, title, and ticks for the test set confusion matrix.\n",
        "ax_test.set_xlabel('Predicted labels')\n",
        "ax_test.set_ylabel('True labels')\n",
        "ax_test.set_title('Confusion Matrix - Test Set')\n",
        "ax_test.xaxis.set_ticklabels(labels)\n",
        "ax_test.yaxis.set_ticklabels(labels)\n",
        "plt.show()\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ],
      "metadata": {
        "id": "dugkJY4Vaos3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Random Forest Classifier**"
      ],
      "metadata": {
        "id": "VMhda7BDgOU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the RandomForestClassifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# RandomForestClassifier: A machine learning algorithm that builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
        "# n_estimators=100: The number of trees in the forest.\n",
        "# random_state=42: Ensures reproducibility of the results.\n",
        "\n",
        "# Encode the labels as binary values.\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the training set.\n",
        "\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the test set.\n",
        "\n",
        "# Fit the model.\n",
        "rf_model.fit(X_train_balanced, y_train_balanced_encoded)\n",
        "# 'fit' trains the Random Forest model using the balanced training data.\n",
        "\n",
        "# Predict on the model.\n",
        "train_class_preds = rf_model.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "test_class_preds = rf_model.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "gZ_kHeJAdc6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding Labels and Predicting on the Model**"
      ],
      "metadata": {
        "id": "z_E2oztSgZZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels as binary values using LabelEncoder.\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the true labels of the training set and transform them.\n",
        "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the training set.\n",
        "\n",
        "# Fit the encoder on the true labels of the test set and transform them.\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the test set.\n",
        "\n",
        "# Fit the classifier (clf) with encoded labels if it's not already done.\n",
        "# Assuming clf is already trained.\n",
        "\n",
        "# Predict on the model and encode the predictions for the training set.\n",
        "train_class_preds = label_encoder.transform(clf.predict(X_train_balanced))\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "# 'transform' converts predicted class labels to encoded form.\n",
        "\n",
        "# Predict on the model and encode the predictions for the test set.\n",
        "test_class_preds = label_encoder.transform(clf.predict(X_test))\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "# 'transform' converts predicted class labels to encoded form.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "ZSB0CFR8eQ0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Model on the Training Set**"
      ],
      "metadata": {
        "id": "D9DDzqHXgoop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report for the training set.\n",
        "print(\"Classification Report - Train Set\")\n",
        "print(classification_report(y_train_balanced_encoded, train_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "print(\" \")\n",
        "\n",
        "# Print the ROC AUC score for the training set.\n",
        "print(\"ROC AUC Score - Train Set\")\n",
        "print(roc_auc_score(y_train_balanced_encoded, train_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "print(\" \")"
      ],
      "metadata": {
        "id": "iw4hjhD2Z11o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Model on the Test Set**"
      ],
      "metadata": {
        "id": "WSDho36eg-GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report for the test set.\n",
        "print(\"Classification Report - Test Set\")\n",
        "print(classification_report(y_test_encoded, test_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "print(\" \")\n",
        "\n",
        "# Print the ROC AUC score for the test set.\n",
        "print(\"ROC AUC Score - Test Set\")\n",
        "print(roc_auc_score(y_test_encoded, test_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "print(\" \")"
      ],
      "metadata": {
        "id": "WRqIoCRHcGFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Logistic Regression algorithm to create the model. The results were astonishingly perfect.\n",
        "\n",
        "For the training dataset, the model achieved a precision, recall, and f1-score of 100% for both Retained and Churned customer data. The overall accuracy was a flawless 100%, with the average precision, recall, and f1-score also at 100%. The ROC AUC score was a perfect 1.0.\n",
        "\n",
        "For the testing dataset, the performance remained impressive with a precision, recall, and f1-score of 100% for both Retained and Churned customer data. Accuracy was again 100%, with average precision, recall, and f1-score all at 100%. The ROC AUC score for the test set was also a perfect 1.0.\n",
        "\n",
        "The model's perfect performance on both the training and test sets suggests it has learned to classify the instances very effectively. However, achieving such a perfect score in real-world scenarios is rare and could indicate potential overfitting or data leakage.\n",
        "\n",
        "Next, I will work on improving the model's robustness and generalizability by using hyperparameter tuning techniques."
      ],
      "metadata": {
        "id": "XYYjpGwrhY-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Model Implementation with Hyperparameter Optimization**"
      ],
      "metadata": {
        "id": "KMRMH2yJiHiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model with a maximum of 10000 iterations.\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "# LogisticRegression: A linear model for binary classification.\n",
        "# max_iter=10000: Maximum number of iterations for the solver to converge.\n",
        "\n",
        "# Define the hyperparameter grid.\n",
        "solvers = ['lbfgs']\n",
        "penalty = ['l2']\n",
        "c_values = [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]\n",
        "# solvers: Algorithm to use in the optimization problem.\n",
        "# penalty: Norm used in the penalization.\n",
        "# c_values: Inverse of regularization strength; smaller values specify stronger regularization.\n",
        "\n",
        "# Define grid search.\n",
        "grid = dict(solver=solvers, penalty=penalty, C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1', error_score=0)\n",
        "# GridSearchCV: A search over specified parameter values for an estimator.\n",
        "# cv: Cross-validation splitting strategy.\n",
        "# n_jobs=-1: Use all available CPUs.\n",
        "\n",
        "# Encode the labels as binary values using LabelEncoder.\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the training set.\n",
        "\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the test set.\n",
        "\n",
        "# Fit the algorithm with the grid search.\n",
        "grid_result = grid_search.fit(X_train_balanced, y_train_balanced_encoded)\n",
        "# 'fit' trains the model using grid search on the balanced training data.\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "# Prints the best score and parameters found during grid search.\n",
        "\n",
        "# Predict on the model and get the predicted classes for the training set.\n",
        "train_class_preds = grid_result.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "# Predict on the model and get the predicted classes for the test set.\n",
        "test_class_preds = grid_result.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Model on the Training Set**"
      ],
      "metadata": {
        "id": "h7ZEY5Fwh9dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report for the training set.\n",
        "print(\"Classification Report - Train Set\")\n",
        "print(classification_report(y_train_balanced_encoded, train_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "print(\" \")\n",
        "\n",
        "# Print the ROC AUC score for the training set.\n",
        "print(\"ROC AUC Score - Train Set\")\n",
        "print(roc_auc_score(y_train_balanced_encoded, train_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "print(\" \")"
      ],
      "metadata": {
        "id": "CtV5iuB4kjYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Model on the Test Set**"
      ],
      "metadata": {
        "id": "zenWq_S5h4zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report for the test set.\n",
        "print(\"Classification Report - Test Set\")\n",
        "print(classification_report(y_test_encoded, test_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "print(\" \")\n",
        "\n",
        "# Print the ROC AUC score for the test set.\n",
        "print(\"ROC AUC Score - Test Set\")\n",
        "print(roc_auc_score(y_test_encoded, test_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "print(\" \")"
      ],
      "metadata": {
        "id": "pZu8OR49lS1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results after Cross-Validation and Hyperparameter Tuning are indeed impressive, achieving perfect scores across the board for both the training and testing datasets. This suggests that the model is performing exceptionally well.But this is sign of overfitting so applying L1 (Lasso) and L2 (Ridge) regularization can be beneficial to ensure the model's robustness and prevent overfitting. Before we proceed let's first understand few important things about these two techniques.\n",
        "\n",
        "### Explaining Lasso (L1) and Ridge (L2) Regularization\n",
        "\n",
        "Regularization techniques like Lasso (L1) and Ridge (L2) are used to improve the model's performance and prevent overfitting. Here's a simple explanation:\n",
        "\n",
        "#### Lasso Regularization (L1)\n",
        "Lasso regularization, also known as L1 regularization, adds a penalty equal to the sum of the absolute values of the coefficients. This technique can shrink some coefficients to zero, effectively performing feature selection. This means it can identify and keep only the most important features, making the model simpler and more interpretable.\n",
        "\n",
        "**Impact on the Model**:\n",
        "- **Reduces Overfitting**: Similar to Ridge, it prevents the model from fitting the noise in the data.\n",
        "- **Performs Feature Selection**: By shrinking some coefficients to zero, it helps in identifying and keeping only the most relevant features.\n",
        "\n",
        "#### Ridge Regularization (L2)\n",
        "Ridge regularization, also known as L2 regularization, adds a penalty equal to the sum of the squared values of the coefficients (weights) in the model. This helps to keep the coefficients small and prevents the model from fitting the noise in the training data. By doing so, it improves the model's generalization ability.\n",
        "\n",
        "**Impact on the Model**:\n",
        "- **Reduces Overfitting**: By shrinking the coefficients, it helps prevent the model from being overly complex.\n",
        "- **Improves Stability**: The model becomes more stable and less sensitive to small changes in the data.\n",
        "\n",
        "### Why Use These Techniques?\n",
        "- **Prevent Overfitting**: Both techniques help to ensure that the model generalizes well to new, unseen data by reducing overfitting.\n",
        "- **Improve Model Interpretability**: Especially with Lasso, we can simplify the model by keeping only the most important features.\n",
        "- **Enhance Model Performance**: By adding a regularization term, we can achieve better performance on test data.\n",
        "\n",
        "### Useful Links for Further Reading\n",
        "- [Lasso (L1) Regularization](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
        "- [Ridge (L2) Regularization](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zZqXakywntNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L1 Regularization (Lasso)**"
      ],
      "metadata": {
        "id": "F0n_3O_-oR2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's implement L1 Regularization with the logistic regression model**"
      ],
      "metadata": {
        "id": "24xB_ug2jWs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 Regularization (Lasso)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization.\n",
        "clf_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0, max_iter=10000)\n",
        "# penalty='l1': Applies L1 regularization to the model.\n",
        "# solver='liblinear': Solver that handles L1 regularization.\n",
        "# C=1.0: Inverse of regularization strength; smaller values specify stronger regularization.\n",
        "# max_iter=10000: Maximum number of iterations for the solver to converge.\n",
        "\n",
        "# Fit the model with the balanced training data.\n",
        "clf_l1.fit(X_train_balanced, y_train_balanced_encoded)\n",
        "# 'fit' trains the model using the balanced training data.\n",
        "\n",
        "# Predict on the model for the training set.\n",
        "train_class_preds_l1 = clf_l1.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "# Predict on the model for the test set.\n",
        "test_class_preds_l1 = clf_l1.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Calculate accuracy scores for the training set.\n",
        "train_accuracy_l1 = accuracy_score(y_train_balanced_encoded, train_class_preds_l1)\n",
        "# 'accuracy_score' calculates the accuracy of the model on the training set.\n",
        "\n",
        "# Calculate accuracy scores for the test set.\n",
        "test_accuracy_l1 = accuracy_score(y_test_encoded, test_class_preds_l1)\n",
        "# 'accuracy_score' calculates the accuracy of the model on the test set.\n",
        "\n",
        "# Print the accuracy scores for the training set.\n",
        "print(\"L1 Regularization - The accuracy on train data is \", train_accuracy_l1)\n",
        "# Outputs the accuracy score on the training data.\n",
        "\n",
        "# Print the accuracy scores for the test set.\n",
        "print(\"L1 Regularization - The accuracy on test data is \", test_accuracy_l1)\n",
        "# Outputs the accuracy score on the test data.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
      ],
      "metadata": {
        "id": "eYAN0cS_n928"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the L1 Regularization Model on the Training Set**"
      ],
      "metadata": {
        "id": "pjWlZsatjmDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report and ROC AUC score.\n",
        "\n",
        "# Print the classification report for the training set (L1).\n",
        "print(\"Classification Report - Train Set (L1)\")\n",
        "print(classification_report(y_train_balanced_encoded, train_class_preds_l1))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# Print the ROC AUC score for the training set (L1).\n",
        "print(\"ROC AUC Score - Train Set (L1)\")\n",
        "print(roc_auc_score(y_train_balanced_encoded, train_class_preds_l1))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "NJ0yLaiKoeri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the L1 Regularization Model on the Test Set**"
      ],
      "metadata": {
        "id": "4zBaHeOMjuNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report and ROC AUC score.\n",
        "\n",
        "# Print the classification report for the test set (L1).\n",
        "print(\"Classification Report - Test Set (L1)\")\n",
        "print(classification_report(y_test_encoded, test_class_preds_l1))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# Print the ROC AUC score for the test set (L1).\n",
        "print(\"ROC AUC Score - Test Set (L1)\")\n",
        "print(roc_auc_score(y_test_encoded, test_class_preds_l1))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "7F8IW23dokEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L2 Regularization (Ridge)**\n",
        "**Logistic Regression model with L2 regularization**"
      ],
      "metadata": {
        "id": "JMiILyFd9NvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 Regularization (Ridge)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization.\n",
        "clf_l2 = LogisticRegression(penalty='l2', solver='lbfgs', C=1000, max_iter=10000)\n",
        "# penalty='l2': Applies L2 regularization to the model.\n",
        "# solver='lbfgs': Solver that handles L2 regularization.\n",
        "# C=1000: Inverse of regularization strength; smaller values specify stronger regularization.\n",
        "# max_iter=10000: Maximum number of iterations for the solver to converge.\n",
        "\n",
        "# Fit the model with the balanced training data.\n",
        "clf_l2.fit(X_train_balanced, y_train_balanced_encoded)\n",
        "# 'fit' trains the model using the balanced training data.\n",
        "\n",
        "# Predict on the model for the training set.\n",
        "train_class_preds_l2 = clf_l2.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "# Predict on the model for the test set.\n",
        "test_class_preds_l2 = clf_l2.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Calculate accuracy scores for the training set.\n",
        "train_accuracy_l2 = accuracy_score(y_train_balanced_encoded, train_class_preds_l2)\n",
        "# 'accuracy_score' calculates the accuracy of the model on the training set.\n",
        "\n",
        "# Calculate accuracy scores for the test set.\n",
        "test_accuracy_l2 = accuracy_score(y_test_encoded, test_class_preds_l2)\n",
        "# 'accuracy_score' calculates the accuracy of the model on the test set.\n",
        "\n",
        "# Print the accuracy scores for the training set.\n",
        "print(\"L2 Regularization - The accuracy on train data is \", train_accuracy_l2)\n",
        "# Outputs the accuracy score on the training data.\n",
        "\n",
        "# Print the accuracy scores for the test set.\n",
        "print(\"L2 Regularization - The accuracy on test data is \", test_accuracy_l2)\n",
        "# Outputs the accuracy score on the test data.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
      ],
      "metadata": {
        "id": "mYCSLe4G9ci9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the L2 Regularization Model on the Training Set**"
      ],
      "metadata": {
        "id": "a0Lrc_CDz5tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report and ROC AUC score.\n",
        "\n",
        "# Print the classification report for the training set (L2).\n",
        "print(\"Classification Report - Train Set (L2)\")\n",
        "print(classification_report(y_train_balanced_encoded, train_class_preds_l2))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# Print the ROC AUC score for the training set (L2).\n",
        "print(\"ROC AUC Score - Train Set (L2)\")\n",
        "print(roc_auc_score(y_train_balanced_encoded, train_class_preds_l2))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "M0YMR8TW9lRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the L2 Regularization Model on the Test Set**"
      ],
      "metadata": {
        "id": "3X07OS4z0Pty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report and ROC AUC score.\n",
        "\n",
        "# Print the classification report for the test set (L2).\n",
        "print(\"Classification Report - Test Set (L2)\")\n",
        "print(classification_report(y_test_encoded, test_class_preds_l2))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# Print the ROC AUC score for the test set (L2).\n",
        "print(\"ROC AUC Score - Test Set (L2)\")\n",
        "print(roc_auc_score(y_test_encoded, test_class_preds_l2))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "aM4wcOb19pAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now in this section we will perform 10-fold cross-validation for both models and calculate the mean cross-validation scores. Let's first understand what is it, why it is required and it's relevance**\n",
        "\n",
        "### Understanding 10-Fold Cross-Validation\n",
        "\n",
        "#### What is 10-Fold Cross-Validation?\n",
        "10-fold cross-validation is a technique used to evaluate the performance of a machine learning model. It involves dividing the dataset into 10 equal parts or \"folds.\" The model is trained on 9 folds and tested on the remaining 1 fold. This process is repeated 10 times, with each fold being used as the test set once.\n",
        "\n",
        "#### Why is 10-Fold Cross-Validation Required?\n",
        "1. **Reduces Overfitting**: By using different subsets of data for training and testing, it ensures the model generalizes well and is not just memorizing the training data.\n",
        "2. **More Reliable Evaluation**: It provides a more accurate estimate of the model's performance by averaging the results across all folds.\n",
        "3. **Efficient Use of Data**: It allows the use of the entire dataset for both training and testing, making the most out of the available data.\n",
        "\n",
        "#### Relevance of 10-Fold Cross-Validation\n",
        "- **Model Evaluation**: It helps in selecting the best model and tuning hyperparameters by providing a comprehensive evaluation.\n",
        "- **Performance Metrics**: It ensures that performance metrics like accuracy, precision, and recall are robust and not dependent on a specific train-test split.\n",
        "- **Generalization**: It confirms that the model can generalize well to new, unseen data, leading to better real-world performance.\n",
        "\n",
        "### Reference\n",
        "For more details on 10-fold cross-validation, you can visit the official scikit-learn documentation: [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "VV0YPqxK-yMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10-Fold Cross-Validation for L1 and L2 Regularization Models**\n",
        "\n",
        "**Let's perform 10-fold cross-validation for both the L1 (Lasso) and L2 (Ridge) regularization models and calculate the mean cross-validation scores**"
      ],
      "metadata": {
        "id": "Rd2YssgP1bMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model with L1 regularization.\n",
        "clf_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000)\n",
        "# penalty='l1': Applies L1 regularization to the model.\n",
        "# solver='liblinear': Solver that handles L1 regularization.\n",
        "# max_iter=10000: Maximum number of iterations for the solver to converge.\n",
        "\n",
        "# Perform 10-fold cross-validation for L1 regularization.\n",
        "cv_scores_l1 = cross_val_score(clf_l1, X_train_balanced, y_train_balanced_encoded, cv=10, scoring='accuracy')\n",
        "# 'cross_val_score' performs cross-validation and returns the accuracy scores for each fold.\n",
        "# cv=10 specifies 10-fold cross-validation.\n",
        "\n",
        "# Print the cross-validation scores and the mean score for L1 regularization.\n",
        "print(\"Cross-validation scores (L1):\", cv_scores_l1)\n",
        "print(\"Mean cross-validation score (L1):\", cv_scores_l1.mean())\n",
        "\n",
        "# Initialize the model with L2 regularization.\n",
        "clf_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "# penalty='l2': Applies L2 regularization to the model.\n",
        "# solver='lbfgs': Solver that handles L2 regularization.\n",
        "# max_iter=10000: Maximum number of iterations for the solver to converge.\n",
        "\n",
        "# Perform 10-fold cross-validation for L2 regularization.\n",
        "cv_scores_l2 = cross_val_score(clf_l2, X_train_balanced, y_train_balanced_encoded, cv=10, scoring='accuracy')\n",
        "# 'cross_val_score' performs cross-validation and returns the accuracy scores for each fold.\n",
        "# cv=10 specifies 10-fold cross-validation.\n",
        "\n",
        "# Print the cross-validation scores and the mean score for L2 regularization.\n",
        "print(\"Cross-validation scores (L2):\", cv_scores_l2)\n",
        "print(\"Mean cross-validation score (L2):\", cv_scores_l2.mean())\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
      ],
      "metadata": {
        "id": "6h3SS4AKo85t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results confirm that your model is highly effective at classifying instances with perfect accuracy. However, as mentioned earlier, achieving such perfect scores is rare in real-world scenarios and might indicate potential overfitting or data leakage."
      ],
      "metadata": {
        "id": "dWd119T9r1Gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV, which employs the Grid Search technique to find the optimal hyperparameters for enhancing model performance.\n",
        "\n",
        "Our goal was to identify the best hyperparameter values to achieve the most accurate prediction results from our model. However, finding these optimal hyperparameters can be challenging. One method is to use manual search, relying on trial and error, but this approach is time-consuming and impractical for building a single model.\n",
        "\n",
        "To address this, methods like Random Search and Grid Search were introduced. Grid Search tests different combinations of specified hyperparameters and their values, assessing performance for each combination to select the best set of hyperparameters. This process can be time-intensive and computationally expensive, depending on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, Grid Search is combined with cross-validation. Cross-validation is used during model training to ensure the model generalizes well to unseen data.\n",
        "\n",
        "That's why I chose the GridSearchCV method for hyperparameter optimization, ensuring thorough evaluation and selection of the best hyperparameters for our model's performance.\n",
        "\n",
        "After performing GridSearchCV for hyperparameter optimization, I applied L1 (Lasso) and L2 (Ridge) regularization to ensure the model's robustness and prevent overfitting. Regularization helps constrain the model and reduce the risk of overfitting by penalizing large coefficients.\n",
        "\n",
        "The perfect cross-validation scores for both L1 and L2 regularization indicate the model's consistent performance across different subsets of the data."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Logistic Regression algorithm to create the model, and following hyperparameter tuning with GridSearchCV, I observed consistently perfect performance metrics. For the training dataset, the model achieved a precision, recall, and f1-score of 100% for both Retained and Churned customer data. The overall accuracy was 100%, with an average precision, recall, and f1-score of 100%, and a ROC AUC score of 1.0.\n",
        "\n",
        "The testing dataset performance remained equally impressive, with a precision, recall, and f1-score of 100% for both Retained and Churned customer data, 100% overall accuracy, and a ROC AUC score of 1.0. The consistency of these metrics indicates that the model has effectively learned to classify the instances.\n",
        "\n",
        "However, achieving such perfect scores in real-world scenarios is rare and could suggest potential overfitting or data leakage. To enhance the model's robustness and prevent overfitting, I applied L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "The results after applying these regularizations were equally impressive, with perfect cross-validation scores for both L1 and L2 regularization. GridSearchCV was chosen for hyperparameter optimization because it systematically evaluates all possible combinations of hyperparameters, ensuring comprehensive evaluation and improved model performance.\n",
        "\n",
        "By identifying the optimal hyperparameters and integrating cross-validation, GridSearchCV helps reduce the risk of overfitting. Overall, the model's performance metrics remained perfect after hyperparameter tuning and regularization, indicating substantial robustness and effective classification."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Implementing Random Forest Classifier\n",
        "\n",
        "#### Understanding Random Forest Classifier\n",
        "\n",
        "#### What is Random Forest Classifier?\n",
        "The Random Forest Classifier is an ensemble learning method used for classification (and regression) tasks. It builds multiple decision trees and combines their outputs to make the final prediction. Each decision tree is trained on a different subset of the data, and the final prediction is made based on the majority vote or average of the predictions from all trees.\n",
        "\n",
        "#### Use and Relevance\n",
        "- **Versatility**: Random Forest can be used for both classification and regression tasks.\n",
        "- **Improved Accuracy**: By combining the predictions from multiple trees, it usually achieves higher accuracy than individual decision trees.\n",
        "- **Reduced Overfitting**: Since it uses multiple trees, it reduces the risk of overfitting, which is a common problem with individual decision trees.\n",
        "- **Handles Missing Values**: It can handle missing values and maintain accuracy for a large proportion of missing data.\n",
        "- **Feature Importance**: Random Forest provides insights into the importance of different features in the dataset, which can be valuable for feature selection and understanding the model.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Robustness**: The model is more robust to noise and outliers in the data.\n",
        "- **Generalization**: It generalizes well to new, unseen data due to the ensemble approach.\n",
        "- **Complexity**: While it improves accuracy and robustness, it can be computationally intensive and may require more resources compared to simpler models.\n",
        "\n",
        "#### Reference\n",
        "For more details on Random Forest Classifier, you can visit the official scikit-learn documentation: [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's implement the Random Forest Classifier model**"
      ],
      "metadata": {
        "id": "x1IVIljR6ntA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the RandomForestClassifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# RandomForestClassifier: An ensemble learning method that builds multiple decision trees and merges them together for more accurate and stable predictions.\n",
        "# n_estimators=100: The number of trees in the forest.\n",
        "# random_state=42: Ensures reproducibility of the results.\n",
        "\n",
        "# Encode the labels as binary values using LabelEncoder.\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the training set.\n",
        "\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "# 'fit_transform' learns the encoding and converts labels to encoded form for the test set.\n",
        "\n",
        "# Fit the Algorithm with the balanced training data.\n",
        "rf_model.fit(X_train_balanced, y_train_balanced_encoded)\n",
        "# 'fit' trains the Random Forest model using the balanced training data.\n",
        "\n",
        "# Predict on the model and make predictions on the training data.\n",
        "train_class_preds = rf_model.predict(X_train_balanced)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "# Predict on the model and make predictions on the test data.\n",
        "test_class_preds = rf_model.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ],
      "metadata": {
        "id": "lfHblZsE1zfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Accuracy on Train and Test Datasets**"
      ],
      "metadata": {
        "id": "zGkfCZQt64T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy on train and test datasets.\n",
        "\n",
        "# Calculate the accuracy score for the training dataset.\n",
        "train_accuracy = accuracy_score(y_train_balanced_encoded, train_class_preds)\n",
        "# 'accuracy_score' calculates the accuracy of the model on the training set.\n",
        "\n",
        "# Calculate the accuracy score for the test dataset.\n",
        "test_accuracy = accuracy_score(y_test_encoded, test_class_preds)\n",
        "# 'accuracy_score' calculates the accuracy of the model on the test set.\n",
        "\n",
        "# Print the accuracy score for the training dataset.\n",
        "print(\"The accuracy on train dataset is\", train_accuracy)\n",
        "# Outputs the accuracy score on the training data.\n",
        "\n",
        "# Print the accuracy score for the test dataset.\n",
        "print(\"The accuracy on test dataset is\", test_accuracy)\n",
        "# Outputs the accuracy score on the test data.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
      ],
      "metadata": {
        "id": "F71OCE392Qni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Random Forest algorithm to create the model, which has shown exceptional performance. For the training dataset, the model achieved a precision, recall, and f1-score of 100% for both Retained and Churned customer data, with an overall accuracy and ROC AUC score of 1.0. The testing dataset mirrored these results with perfect precision, recall, f1-score, accuracy, and ROC AUC score. This indicates that the model has perfectly classified all instances in both datasets. To further validate and enhance the model's performance, I recommend performing additional cross-validation, testing on new data, and applying hyperparameter tuning techniques."
      ],
      "metadata": {
        "id": "M_p09FhvCGXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Evaluation Metric Score Chart**"
      ],
      "metadata": {
        "id": "CL171zBf97yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the confusion matrix.\n",
        "labels = ['Retained', 'Churned']\n",
        "# 'labels' defines the class names for the confusion matrix.\n",
        "\n",
        "# Confusion matrix for the train set.\n",
        "cm_train = confusion_matrix(y_train_balanced_encoded, train_class_preds)\n",
        "print(\"Confusion Matrix - Train:\\n\", cm_train)\n",
        "# 'confusion_matrix' computes the confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "# Plotting confusion matrix for the train set.\n",
        "plt.figure(figsize=(10, 7))\n",
        "# Creates a new figure with a specified size.\n",
        "\n",
        "ax_train = plt.subplot()\n",
        "# Adds a subplot to the current figure.\n",
        "\n",
        "sns.heatmap(cm_train, annot=True, ax=ax_train, fmt='d', cmap='Blues')\n",
        "# 'sns.heatmap' plots the heatmap of the confusion matrix.\n",
        "# annot=True to annotate cells.\n",
        "# fmt='d' formats the annotations as integers.\n",
        "# cmap='Blues' sets the color map to 'Blues'.\n",
        "\n",
        "# Labels, title, and ticks for the train set.\n",
        "ax_train.set_xlabel('Predicted labels')\n",
        "# Sets the x-axis label.\n",
        "\n",
        "ax_train.set_ylabel('True labels')\n",
        "# Sets the y-axis label.\n",
        "\n",
        "ax_train.set_title('Confusion Matrix - Train Set')\n",
        "# Sets the title of the plot.\n",
        "\n",
        "ax_train.xaxis.set_ticklabels(labels)\n",
        "# Sets the tick labels for the x-axis.\n",
        "\n",
        "ax_train.yaxis.set_ticklabels(labels)\n",
        "# Sets the tick labels for the y-axis.\n",
        "\n",
        "plt.show()\n",
        "# Displays the plot.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Evaluation Metric Score Chart for Test Set**"
      ],
      "metadata": {
        "id": "5MkubTMl-W7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the confusion matrix\n",
        "labels = ['Retained', 'Churned']\n",
        "\n",
        "# Confusion matrix for the test set\n",
        "cm = confusion_matrix(y_test_encoded, test_class_preds)\n",
        "print(\"Confusion Matrix - Test:\\n\", cm)\n",
        "\n",
        "# Plotting confusion matrix for the test set\n",
        "plt.figure(figsize=(10, 7))\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax=ax, fmt='d', cmap='Greens')  # annot=True to annotate cells\n",
        "\n",
        "# Labels, title and ticks for test set\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Test Set')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iYA2TJZU4eJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Random Forest Model on the Training Set**"
      ],
      "metadata": {
        "id": "Bn0YHXUg-c4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for the training set.\n",
        "print(\"Classification Report - Train Set\")\n",
        "print(metrics.classification_report(y_train_balanced_encoded, train_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "# ROC AUC score for the training set.\n",
        "print(\"ROC AUC Score - Train Set\")\n",
        "print(metrics.roc_auc_score(y_train_balanced_encoded, train_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "\n",
        "print(\" \")"
      ],
      "metadata": {
        "id": "MoBko92BsOfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Random Forest Model on the Test Set**"
      ],
      "metadata": {
        "id": "s-zVD-Rt-tp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for the testing set.\n",
        "print(\"Classification Report - Test Set\")\n",
        "print(metrics.classification_report(y_test_encoded, test_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "# ROC AUC score for the testing set.\n",
        "print(\"ROC AUC Score - Test Set\")\n",
        "print(metrics.roc_auc_score(y_test_encoded, test_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "\n",
        "print(\" \")"
      ],
      "metadata": {
        "id": "0LJ9J-2ZsbjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used the Random Forest algorithm to create the model. As I observed, there is overfitting.\n",
        "\n",
        "For the training dataset, I found precision of 100% and recall of 100% and f1-score of 100% for False Churn customer data. I am also interested to see the result for Churning customer data, as I got precision of 100% and recall of 100% and f1-score of 100%. Accuracy is 100% and average precision, recall, and f1-score are 100%, 100%, and 100%, respectively, with a ROC AUC score of 1.0.\n",
        "\n",
        "For the testing dataset, I found precision of 100% and recall of 100% and f1-score of 100% for False Churn customer data. I am also interested to see the result for Churning customer data, as I got precision of 100% and recall of 100% and f1-score of 100%. Accuracy is 100% and average precision, recall, and f1-score are 100%, 100%, and 100%, respectively, with a ROC AUC score of 1.0.\n",
        "\n",
        "The perfect performance metrics on both the training and testing datasets indicate that the model has perfectly classified all instances. This suggests exceptional performance but also raises concerns about potential overfitting or data leakage.\n",
        "\n",
        "Next, I will try to improve the model's performance by using hyperparameter tuning techniques."
      ],
      "metadata": {
        "id": "O5-lhaOvs9Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identify and visualize the most important features in the dataset based on the trained Random Forest model"
      ],
      "metadata": {
        "id": "Hw3AJJjruMCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining and Training the Random Forest Model**"
      ],
      "metadata": {
        "id": "h_0PWGfK_Ptj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your original DataFrame.\n",
        "original_df = pd.read_csv('/content/Telco_customer_churn.csv')  # Replace with your actual dataset\n",
        "# 'pd.read_csv' loads the dataset into a DataFrame from the specified CSV file path.\n",
        "\n",
        "# One-hot encode categorical variables.\n",
        "x_encoded = pd.get_dummies(original_df.drop([\"Churn Label\"], axis=1))\n",
        "# 'pd.get_dummies' encodes categorical variables as binary (0 or 1) columns.\n",
        "# 'drop([\"Churn Label\"], axis=1)' drops the target column \"Churn Label\" from the features.\n",
        "\n",
        "# Extract the feature names from the encoded DataFrame.\n",
        "feature_names = x_encoded.columns.tolist()\n",
        "# 'columns.tolist()' extracts the column names from the encoded DataFrame and converts them to a list.\n",
        "\n",
        "# Define and train the Random Forest model.\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# 'RandomForestClassifier': An ensemble learning method that builds multiple decision trees and merges them together for more accurate predictions.\n",
        "# 'n_estimators=100': The number of trees in the forest.\n",
        "# 'random_state=42': Ensures reproducibility of the results.\n",
        "\n",
        "rf_model.fit(x_encoded, original_df[\"Churn Label\"])\n",
        "# 'fit' trains the Random Forest model using the encoded features and the target labels.\n",
        "\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ],
      "metadata": {
        "id": "J0yZqZNy2jqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Feature Importances from the Random Forest Model**"
      ],
      "metadata": {
        "id": "jXuzOzVX_lbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from the Random Forest model.\n",
        "importances = rf_model.feature_importances_\n",
        "# 'feature_importances_' retrieves the importance of each feature in predicting the target variable.\n",
        "\n",
        "# Ensure the length of feature names matches the length of feature importances.\n",
        "if len(feature_names) != len(importances):\n",
        "    raise ValueError(\"The length of feature_names must match the length of importances\")\n",
        "# Checks if the length of 'feature_names' matches the length of 'importances' to avoid mismatches.\n",
        "\n",
        "# Create a dictionary to hold feature names and their importance scores.\n",
        "importance_dict = {'Feature': feature_names,\n",
        "                   'Feature Importance': importances}\n",
        "# Combines 'feature_names' and 'importances' into a dictionary to associate each feature with its importance score."
      ],
      "metadata": {
        "id": "xc_v9WkJ-fw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting and Displaying Feature Importances**"
      ],
      "metadata": {
        "id": "DIOXCHUY_zNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary to a DataFrame.\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "# 'pd.DataFrame' converts the dictionary to a DataFrame.\n",
        "\n",
        "# Round the feature importance scores to 2 decimal places.\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'], 2)\n",
        "# 'round' rounds the feature importance scores to 2 decimal places for better readability.\n",
        "\n",
        "# Sort the DataFrame by feature importance in descending order.\n",
        "importance_df = importance_df.sort_values(by=['Feature Importance'], ascending=False)\n",
        "# 'sort_values' sorts the DataFrame by the 'Feature Importance' column in descending order.\n",
        "\n",
        "# Display the DataFrame.\n",
        "print(importance_df)\n",
        "# 'print' displays the DataFrame with the feature importances sorted in descending order.\n",
        "\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"
      ],
      "metadata": {
        "id": "Ae1G3Wwa-mdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Top N Feature Importances**"
      ],
      "metadata": {
        "id": "zG8tEAlZ_7kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top N features.\n",
        "top_n = 20  # Change this number to display more or fewer features.\n",
        "# Defines the number of top features to display based on their importance.\n",
        "\n",
        "top_features = importance_df.head(top_n)\n",
        "# 'head(top_n)' selects the top N features from the importance DataFrame.\n",
        "\n",
        "# Plot the feature importances for the top N features.\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Creates a new figure with a specified size.\n",
        "\n",
        "plt.barh(top_features['Feature'], top_features['Feature Importance'], color='teal')\n",
        "# 'barh' creates a horizontal bar plot of the top N feature importances.\n",
        "# 'top_features['Feature']' provides the feature names for the y-axis.\n",
        "# 'top_features['Feature Importance']' provides the importance scores for the x-axis.\n",
        "# 'color='teal'' sets the color of the bars to teal.\n",
        "\n",
        "plt.xlabel('Relative Importance')\n",
        "# Sets the label for the x-axis.\n",
        "\n",
        "plt.title(f'Top {top_n} Feature Importances')\n",
        "# Sets the title of the plot.\n",
        "\n",
        "plt.gca().invert_yaxis()\n",
        "# Inverts the y-axis to display the most important features at the top.\n",
        "\n",
        "plt.show()\n",
        "# Displays the plot.\n",
        "\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html"
      ],
      "metadata": {
        "id": "9_VYvddHCka2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify and visualize the most important features in the dataset using a Random Forest model, I prepared the data by one-hot encoding categorical variables and extracted feature names from the encoded DataFrame.\n",
        "\n",
        "After training the Random Forest model, I extracted feature importances, ensuring they matched the feature names.\n",
        "\n",
        "Then I created a DataFrame to store and sort these importance scores, focusing on the top 20 features for clarity.\n",
        "\n",
        "The visualization was achieved by plotting the relative importance of these top features, providing a clear and interpretable view of the most significant predictors driving your model's predictions. This method highlights the key variables effectively, aiding in further analysis and decision-making."
      ],
      "metadata": {
        "id": "epth6JPzGOlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Grid Search with Random Forest Classifier**"
      ],
      "metadata": {
        "id": "1y5cO0h7AGud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset.\n",
        "original_df = pd.read_csv('/content/Telco_customer_churn.csv')\n",
        "# 'pd.read_csv' loads the dataset into a DataFrame from the specified CSV file path.\n",
        "\n",
        "# Convert 'Churn Label' to numerical values.\n",
        "original_df['Churn Label'] = original_df['Churn Label'].map({'No': 0, 'Yes': 1})\n",
        "# 'map' converts the 'Churn Label' column to numerical values, mapping 'No' to 0 and 'Yes' to 1.\n",
        "\n",
        "# One-hot encode categorical variables.\n",
        "X = pd.get_dummies(original_df.drop([\"Churn Label\"], axis=1))\n",
        "y = original_df[\"Churn Label\"]\n",
        "# 'pd.get_dummies' encodes categorical variables as binary (0 or 1) columns.\n",
        "# 'drop([\"Churn Label\"], axis=1)' drops the target column \"Churn Label\" from the features.\n",
        "\n",
        "# Splitting dataset into train and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# 'train_test_split' splits the dataset into training and testing sets.\n",
        "# 'test_size=0.3' specifies that 30% of the data will be used for testing.\n",
        "# 'random_state=42' ensures reproducibility of the results.\n",
        "\n",
        "# Number of trees.\n",
        "n_estimators = [50, 80, 100]\n",
        "# Specifies the number of trees in the forest.\n",
        "\n",
        "# Maximum depth of trees.\n",
        "max_depth = [4, 6, 8]\n",
        "# Specifies the maximum depth of the trees.\n",
        "\n",
        "# Minimum number of samples required to split a node.\n",
        "min_samples_split = [50, 100, 150]\n",
        "# Specifies the minimum number of samples required to split an internal node.\n",
        "\n",
        "# Minimum number of samples required at each leaf node.\n",
        "min_samples_leaf = [40, 50]\n",
        "# Specifies the minimum number of samples required to be at a leaf node.\n",
        "\n",
        "# Hyperparameter Grid.\n",
        "param_dict = {'n_estimators': n_estimators,\n",
        "              'max_depth': max_depth,\n",
        "              'min_samples_split': min_samples_split,\n",
        "              'min_samples_leaf': min_samples_leaf}\n",
        "# Defines the grid of hyperparameters for grid search.\n",
        "\n",
        "# Create an instance of the RandomForestClassifier.\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "# 'RandomForestClassifier': An ensemble learning method that builds multiple decision trees and merges them together for more accurate predictions.\n",
        "\n",
        "# Grid search.\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid=param_dict,\n",
        "                       cv=5, verbose=2, scoring='f1')\n",
        "# 'GridSearchCV' performs an exhaustive search over the specified hyperparameter grid.\n",
        "# 'cv=5' specifies 5-fold cross-validation.\n",
        "# 'verbose=2' sets the verbosity level for the output.\n",
        "# 'scoring='f1'' uses the F1 score as the evaluation metric.\n",
        "\n",
        "# Fit the Algorithm.\n",
        "rf_grid.fit(X_train, y_train)\n",
        "# 'fit' trains the Random Forest model using the training data and the hyperparameter grid.\n",
        "\n",
        "# Predict on the model.\n",
        "# Making predictions on train and test data.\n",
        "train_class_preds = rf_grid.predict(X_train)\n",
        "# 'predict' outputs the predicted class labels for the training set.\n",
        "\n",
        "test_class_preds = rf_grid.predict(X_test)\n",
        "# 'predict' outputs the predicted class labels for the test set.\n",
        "\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters and score.\n",
        "print(\"Best: %f using %s\" % (rf_grid.best_score_, rf_grid.best_params_))\n",
        "# 'best_score_' gives the best score achieved during the grid search.\n",
        "# 'best_params_' gives the combination of parameters that gave the best score.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ],
      "metadata": {
        "id": "O3getMm0L989"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Confusion Matrix for Train Data**"
      ],
      "metadata": {
        "id": "fj1mc_OsAo8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the confusion matrix.\n",
        "labels = ['Retained', 'Churned']\n",
        "# 'labels' defines the class names for the confusion matrix.\n",
        "\n",
        "# Confusion matrix for the train data.\n",
        "cm_train = confusion_matrix(y_train, train_class_preds)\n",
        "print(\"Confusion Matrix - Train Data\")\n",
        "print(cm_train)\n",
        "# 'confusion_matrix' computes the confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "# Plotting confusion matrix for the train data.\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_train, annot=True, ax=ax, fmt='d')\n",
        "# 'sns.heatmap' plots the heatmap of the confusion matrix.\n",
        "# annot=True to annotate cells.\n",
        "# fmt='d' formats the annotations as integers.\n",
        "\n",
        "# Labels, title, and ticks.\n",
        "ax.set_xlabel('Predicted labels')\n",
        "# Sets the label for the x-axis.\n",
        "\n",
        "ax.set_ylabel('True labels')\n",
        "# Sets the label for the y-axis.\n",
        "\n",
        "ax.set_title('Confusion Matrix - Train Data')\n",
        "# Sets the title of the plot.\n",
        "\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "# Sets the tick labels for the x-axis.\n",
        "\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Sets the tick labels for the y-axis.\n",
        "\n",
        "plt.show()\n",
        "# Displays the plot.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ],
      "metadata": {
        "id": "vYNPTEEWMH-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Confusion Matrix for Test Data**"
      ],
      "metadata": {
        "id": "uXDxWFjFBRPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for the test data.\n",
        "cm_test = confusion_matrix(y_test, test_class_preds)\n",
        "print(\"Confusion Matrix - Test Data\")\n",
        "print(cm_test)\n",
        "# 'confusion_matrix' computes the confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "# Plotting confusion matrix for the test data.\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_test, annot=True, ax=ax, fmt='d')\n",
        "# 'sns.heatmap' plots the heatmap of the confusion matrix.\n",
        "# annot=True to annotate cells.\n",
        "# fmt='d' formats the annotations as integers.\n",
        "\n",
        "# Labels, title, and ticks.\n",
        "ax.set_xlabel('Predicted labels')\n",
        "# Sets the label for the x-axis.\n",
        "\n",
        "ax.set_ylabel('True labels')\n",
        "# Sets the label for the y-axis.\n",
        "\n",
        "ax.set_title('Confusion Matrix - Test Data')\n",
        "# Sets the title of the plot.\n",
        "\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "# Sets the tick labels for the x-axis.\n",
        "\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Sets the tick labels for the y-axis.\n",
        "\n",
        "plt.show()\n",
        "# Displays the plot.\n",
        "\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ],
      "metadata": {
        "id": "D1w6viWKMe9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Random Forest Model on Train Data**"
      ],
      "metadata": {
        "id": "HCgJMh6JBd8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for train data.\n",
        "print(\"Classification Report - Train Data\")\n",
        "print(classification_report(y_train, train_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC score for train data.\n",
        "print(\"ROC AUC Score - Train Data\")\n",
        "print(roc_auc_score(y_train, train_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "uURYqca3MjQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Random Forest Model on Test Data**"
      ],
      "metadata": {
        "id": "Qa3QSpcpBlhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for test data.\n",
        "print(\"Classification Report - Test Data\")\n",
        "print(classification_report(y_test, test_class_preds))\n",
        "# 'classification_report' provides a detailed summary of the model's precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC score for test data.\n",
        "print(\"ROC AUC Score - Test Data\")\n",
        "print(roc_auc_score(y_test, test_class_preds))\n",
        "# 'roc_auc_score' calculates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from the predicted class labels and true labels.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "k06YmcqzMqFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Insights:\n",
        "\n",
        "Precision, Recall, and F1-Score for Class 1 (Churned):\n",
        "Precision: 0.00 for both training and test data.\n",
        "Recall: 0.00 for both training and test data.\n",
        "F1-Score: 0.00 for both training and test data.\n",
        "\n",
        "Class 0 (Retained):\n",
        "Precision, Recall, F1-Score: High values for Class 0, indicating the model predicts this class well.\n",
        "\n",
        "Overall Accuracy:\n",
        "74% for training data.\n",
        "72% for test data.\n",
        "\n",
        "Macro and Weighted Averages:\n",
        "Macro Average: Indicates a significant imbalance in performance across classes.\n",
        "Weighted Average: Heavily influenced by the high performance of Class 0.\n",
        "\n",
        "ROC AUC Score:\n",
        "0.5 for both training and test data, indicating the model's predictive performance is no better than random guessing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Observations:\n",
        "\n",
        "- The outputs and visuals indicate significant performance issues with the current Random Forest model.\n",
        "- The confusion matrices for both training and test data reveal a complete failure to predict the \"Churned\" class, with all instances predicted as \"Retained.\"\n",
        "- This is further supported by the classification reports, where the precision, recall, and f1-score for the \"Churned\" class are all zero, resulting in a heavily biased model towards the \"Retained\" class.\n",
        "- Additionally, the ROC AUC scores for both training and test data are 0.5, indicating the model's predictive performance is no better than random guessing.\n",
        "- This suggests a severe imbalance in the dataset and inadequacies in the chosen hyperparameters, necessitating significant adjustments to handle class imbalance, expand the hyperparameter grid, and potentially revisit feature engineering to improve the model's performance and ability to predict the minority class effectively."
      ],
      "metadata": {
        "id": "GmhRQSNBTwLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now to address the class imbalance issue and improve the model's performance I will apply SMOTE. In this section I will handle class imbalance, expand the hyperparameter grid, and then retrain and evaluate the model**\n",
        "\n",
        "### Understanding SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "\n",
        "**SMOTE** stands for **Synthetic Minority Over-sampling Technique**. It's designed to generate synthetic samples for the minority class in an imbalanced dataset. Instead of duplicating existing minority class samples, SMOTE creates new, artificial samples by interpolating between existing minority class samples.\n",
        "\n",
        "#### **Why SMOTE?**\n",
        "Class imbalance can lead to biased models that favor the majority class. SMOTE helps mitigate this by balancing the dataset, allowing the model to learn more effectively from the minority class.\n",
        "\n",
        "#### **Relevance Compared to Other Imbalance Techniques**\n",
        "- **Random Oversampling**: Simply duplicates minority class samples, which can lead to overfitting.\n",
        "- **Random Undersampling**: Removes samples from the majority class, which can lead to loss of valuable information.\n",
        "- **ADASYN**: Similar to SMOTE but focuses on generating samples near the decision boundary.\n",
        "- **Hybrid Methods**: Combines SMOTE with other techniques like Tomek Links or ENN to improve performance.\n",
        "\n",
        "#### **Impact on Our Model**\n",
        "Using SMOTE can improve model performance, especially in metrics like recall and F1-score. However, it can also introduce new, artificial patterns that may affect model interpretability and explainability.\n",
        "\n",
        "#### **Reference Link**\n",
        "For more detailed information, you can refer to this Analytics Vidhya article: [Overcoming Class Imbalance Using SMOTE Techniques](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/)\n"
      ],
      "metadata": {
        "id": "MIX6yegbW7dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation and Handling Class Imbalance**"
      ],
      "metadata": {
        "id": "4-QNzT0lXaIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "original_df = pd.read_csv('/content/Telco_customer_churn.csv')\n",
        "# Load the dataset from a CSV file using pandas.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "\n",
        "# Convert 'Churn Label' to numerical values\n",
        "original_df['Churn Label'] = original_df['Churn Label'].map({'No': 0, 'Yes': 1})\n",
        "# Convert the 'Churn Label' column to numerical values: 'No' becomes 0 and 'Yes' becomes 1.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(original_df.drop([\"Churn Label\"], axis=1))\n",
        "# One-hot encode the categorical variables in the dataset, excluding the 'Churn Label' column.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "\n",
        "# Separate features and target variable\n",
        "y = original_df[\"Churn Label\"]\n",
        "# Assign the 'Churn Label' column to the target variable 'y'.\n",
        "\n",
        "# Splitting dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Split the dataset into training and testing sets, using 70% of the data for training and 30% for testing.\n",
        "# Set a random state for reproducibility.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "# Using SMOTE for oversampling to handle class imbalance\n",
        "sm = SMOTE(random_state=42)\n",
        "# Initialize the SMOTE technique to handle class imbalance in the dataset.\n",
        "# Reference: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "# Fit SMOTE to the training data and generate a balanced dataset with resampled training data.\n",
        "# Reference: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE.fit_resample"
      ],
      "metadata": {
        "id": "a92Y7ZeDXcWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the Expanded Hyperparameter Grid and Model Initialization**"
      ],
      "metadata": {
        "id": "RPgg1Wn5X6Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning for XGBoost Classifier\n",
        "\n",
        "# Number of trees\n",
        "n_estimators = [50, 100, 200]\n",
        "# This list defines the number of trees in the ensemble. We'll explore 50, 100, and 200 trees.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [6, 8, 10]\n",
        "# This list defines the maximum depth of the trees. We'll explore depths of 6, 8, and 10.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [100, 150]\n",
        "# This list defines the minimum number of samples required to split an internal node. We'll explore 100 and 150 samples.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [20, 30]\n",
        "# This list defines the minimum number of samples required to be at a leaf node. We'll explore 20 and 30 samples.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_dict = {'n_estimators': n_estimators,\n",
        "              'max_depth': max_depth,\n",
        "              'min_samples_split': min_samples_split,\n",
        "              'min_samples_leaf': min_samples_leaf}\n",
        "# This dictionary defines the grid of hyperparameters for tuning the XGBoost classifier.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ],
      "metadata": {
        "id": "rK2EyXU-X4Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning with GridSearchCV**"
      ],
      "metadata": {
        "id": "N_Om4k0eYDJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search for Hyperparameter Tuning\n",
        "\n",
        "# Initialize Grid Search with cross-validation\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid=param_dict,\n",
        "                       cv=5, verbose=2, scoring='f1', n_jobs=-1)\n",
        "# Perform a grid search to find the best hyperparameters for the model.\n",
        "# - estimator: The model (rf_model) to be tuned.\n",
        "# - param_grid: The dictionary of hyperparameters to search.\n",
        "# - cv: Number of cross-validation folds (5).\n",
        "# - verbose: Level of verbosity (2) to show detailed logs.\n",
        "# - scoring: Metric ('f1') to evaluate the models.\n",
        "# - n_jobs: Number of jobs to run in parallel (-1 means using all processors).\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "\n",
        "# Fit the Algorithm using the resampled training data\n",
        "rf_grid.fit(X_train_res, y_train_res)\n",
        "# Fit the grid search to the resampled training data to find the best hyperparameters.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit"
      ],
      "metadata": {
        "id": "IbzbM4ykYFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictions and Evaluation**"
      ],
      "metadata": {
        "id": "ihvE65p-ZhAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Predictions on Train and Test Data\n",
        "\n",
        "# Predict on the train data\n",
        "train_class_preds = rf_grid.predict(X_train)\n",
        "# Use the trained model (rf_grid) to make predictions on the training data (X_train).\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.predict\n",
        "\n",
        "# Predict on the test data\n",
        "test_class_preds = rf_grid.predict(X_test)\n",
        "# Use the trained model (rf_grid) to make predictions on the test data (X_test).\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.predict"
      ],
      "metadata": {
        "id": "PtXdBnK3Zoi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Best Parameters and Score\n",
        "\n",
        "# Print the best score and parameters found by the grid search\n",
        "print(\"Best: %f using %s\" % (rf_grid.best_score_, rf_grid.best_params_))\n",
        "# This line prints the best F1 score achieved during the grid search along with the corresponding hyperparameters.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.best_score_"
      ],
      "metadata": {
        "id": "AO6_YifUZvsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Train Data\n",
        "\n",
        "# Define labels for the classes\n",
        "labels = ['Retained', 'Churned']\n",
        "# These are the class labels for the confusion matrix.\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm_train = confusion_matrix(y_train, train_class_preds)\n",
        "print(\"Confusion Matrix - Train Data\")\n",
        "print(cm_train)\n",
        "# Calculate and print the confusion matrix for the training data.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_train, annot=True, ax=ax, fmt='d')\n",
        "# Use seaborn to plot the confusion matrix as a heatmap.\n",
        "# - annot=True: Annotate the heatmap cells with the confusion matrix values.\n",
        "# - fmt='d': Format the annotations as integers.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Add labels, title, and ticks to the heatmap\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Train Data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Set the x-axis and y-axis labels, title, and tick labels for the heatmap.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Display the heatmap plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "JHr05uAmZ6dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Test Data\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm_test = confusion_matrix(y_test, test_class_preds)\n",
        "print(\"Confusion Matrix - Test Data\")\n",
        "print(cm_test)\n",
        "# Calculate and print the confusion matrix for the test data.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_test, annot=True, ax=ax, fmt='d')\n",
        "# Use seaborn to plot the confusion matrix as a heatmap.\n",
        "# - annot=True: Annotate the heatmap cells with the confusion matrix values.\n",
        "# - fmt='d': Format the annotations as integers.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Add labels, title, and ticks to the heatmap\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Test Data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Set the x-axis and y-axis labels, title, and tick labels for the heatmap.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Display the heatmap plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "JqGdxk2AaLPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report for Train Data\n",
        "\n",
        "print(\"Classification Report - Train Data\")\n",
        "print(classification_report(y_train, train_class_preds))\n",
        "# Print the classification report for the training data, which includes precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC Score for Train Data\n",
        "\n",
        "print(\"ROC AUC Score - Train Data\")\n",
        "print(roc_auc_score(y_train, train_class_preds))\n",
        "# Print the ROC AUC score for the training data, which measures the model's ability to distinguish between the classes.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "5jJpeariaaI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report for Test Data\n",
        "\n",
        "print(\"Classification Report - Test Data\")\n",
        "print(classification_report(y_test, test_class_preds))\n",
        "# Print the classification report for the test data, which includes precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC Score for Test Data\n",
        "\n",
        "print(\"ROC AUC Score - Test Data\")\n",
        "print(roc_auc_score(y_test, test_class_preds))\n",
        "# Print the ROC AUC score for the test data, which measures the model's ability to distinguish between the classes.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "AZvpHnVnafn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our project, we used GridSearchCV for hyperparameter optimization, combined with Cross-Validation and SMOTE to handle class imbalance and enhance the model's performance.\n",
        "\n",
        "Initially, our Random Forest model showed significant performance issues, especially in predicting the \"Churned\" class. The precision, recall, and f1-score for the \"Churned\" class were all zero, with an overall accuracy of 74% for training data and 72% for test data. The model's ROC AUC scores were 0.5 for both datasets, indicating performance no better than random guessing. This highlighted a severe imbalance in the dataset and inadequacies in the chosen hyperparameters.\n",
        "\n",
        "To address these issues, we applied SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset by generating synthetic samples for the minority class. This ensured the model could learn effectively from both classes. We then expanded our hyperparameter grid and used GridSearchCV to systematically evaluate different combinations of hyperparameters. GridSearchCV, combined with Cross-Validation, ensured reliable performance assessment by splitting the dataset into multiple folds and training the model on different subsets.\n",
        "\n",
        "After applying these techniques, the best hyperparameters were found to be:\n",
        "\n",
        "max_depth: 10\n",
        "\n",
        "min_samples_leaf: 20\n",
        "\n",
        "min_samples_split: 100\n",
        "\n",
        "n_estimators: 200\n",
        "\n",
        "The optimized model achieved a significantly improved performance with a precision of 0.96 for Class 0 and 0.85 for Class 1 on the training data. The recall and f1-score were also high, resulting in an overall accuracy of 93% for both training and test data. The ROC AUC scores improved to 0.9162 for training and 0.9242 for test data.\n",
        "\n",
        "By integrating GridSearchCV for hyperparameter tuning, Cross-Validation for reliable performance assessment, and SMOTE for addressing class imbalance, we achieved a well-tuned, robust model with significantly enhanced predictive power."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training dataset, I found a precision of 96% and recall of 94% with an f1-score of 95% for Retained customers. Interestingly, the results for Churned customers showed a precision of 85%, recall of 89%, and f1-score of 87%. The overall accuracy was 93%, with average precision, recall, and f1-score being 90%, 92%, and 91%, respectively, and an ROC AUC score of 91.62%.\n",
        "\n",
        "Quite an improvement with a well-balanced performance for both classes, no signs of overfitting.\n",
        "\n",
        "For the testing dataset, I found a precision of 97% and recall of 94% with an f1-score of 95% for Retained customers. For Churned customers, the precision was 84%, recall was 91%, and f1-score was 88%. Accuracy stood at 93%, with average precision, recall, and f1-score being 90%, 92%, and 91%, respectively, and an ROC AUC score of 92.42%.\n",
        "\n",
        "Significant improvement in recall and a balanced increase in other metrics, indicating strong generalization to unseen data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact of the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have segregated the answer into 3 parts, which mainly focus on the methodology, evaluation metrics, and business impact.\n",
        "\n",
        "### Cross-Validation & Hyperparameter Tuning\n",
        "\n",
        "Initially, our Random Forest model faced significant performance issues, particularly in predicting the \"Churned\" class. Precision, recall, and f1-score for \"Churned\" were all zero, and the ROC AUC scores were 0.5, indicating the model was no better than random guessing. This underscored the need for addressing class imbalance and optimizing hyperparameters.\n",
        "\n",
        "To tackle these challenges, we employed **SMOTE (Synthetic Minority Over-sampling Technique)** to balance the dataset by generating synthetic samples for the minority class. This step was crucial in ensuring the model could learn effectively from both classes. We then used **GridSearchCV** for hyperparameter optimization, systematically evaluating different combinations of hyperparameters. **Cross-Validation** was integrated to ensure robust performance assessment by training and validating the model on different data subsets.\n",
        "\n",
        "The optimized model, with the best hyperparameters (`max_depth`: 10, `min_samples_leaf`: 20, `min_samples_split`: 100, `n_estimators`: 200), demonstrated significant improvements:\n",
        "\n",
        "- **Training Dataset**:\n",
        "  - Precision: 96% (Retained), 85% (Churned)\n",
        "  - Recall: 94% (Retained), 89% (Churned)\n",
        "  - F1-Score: 95% (Retained), 87% (Churned)\n",
        "  - Accuracy: 93%\n",
        "  - ROC AUC Score: 91.62%\n",
        "\n",
        "- **Testing Dataset**:\n",
        "  - Precision: 97% (Retained), 84% (Churned)\n",
        "  - Recall: 94% (Retained), 91% (Churned)\n",
        "  - F1-Score: 95% (Retained), 88% (Churned)\n",
        "  - Accuracy: 93%\n",
        "  - ROC AUC Score: 92.42%\n",
        "\n",
        "### Evaluation Metrics & Business Impact\n",
        "\n",
        "**Precision**: High precision ensures that the majority of identified churners are indeed at risk, leading to effective targeting and reduced churn rate.\n",
        "\n",
        "**Recall**: High recall indicates that most at-risk customers are identified, crucial for comprehensive retention strategies.\n",
        "\n",
        "**F1-Score**: Balances precision and recall, providing a single metric to evaluate overall model performance, which helps in resource allocation and decision-making.\n",
        "\n",
        "**Accuracy**: Reflects the model's overall ability to predict correctly, important for general assessment but not sufficient alone.\n",
        "\n",
        "**ROC AUC Score**: High scores demonstrate strong discriminative ability, essential for effectively distinguishing between retained and churned customers.\n",
        "\n",
        "### Business Impact\n",
        "\n",
        "The ML model significantly improves customer retention efforts by accurately identifying at-risk customers, enabling targeted interventions. This leads to better resource allocation, increased customer satisfaction, and ultimately higher revenue. The comprehensive approach of using SMOTE, GridSearchCV, and Cross-Validation ensures a robust and reliable model, enhancing business decision-making and operational efficiency.\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - Implementing XgBoost Classifier\n",
        "\n",
        "### XGBoost Classifier\n",
        "\n",
        "#### What is it?\n",
        "XGBoost (eXtreme Gradient Boosting) is a powerful machine learning algorithm based on the gradient boosting framework. It is designed for speed and performance, providing a robust and efficient way to handle large datasets and complex models.\n",
        "\n",
        "#### Why is it Used in Model Implementation?\n",
        "- **Performance**: XGBoost is known for its high performance and efficiency, making it ideal for large datasets and real-time predictions.\n",
        "- **Accuracy**: It often provides superior accuracy compared to other algorithms, thanks to its advanced boosting techniques.\n",
        "- **Flexibility**: XGBoost supports various objective functions and evaluation metrics, allowing it to be tailored to specific needs and tasks.\n",
        "\n",
        "#### It's Relevance\n",
        "- **Versatility**: Can be used for both classification and regression tasks, making it a versatile choice for different types of problems.\n",
        "- **Feature Importance**: Provides insights into feature importance, helping to understand which features contribute the most to the predictions.\n",
        "- **Handling Missing Data**: XGBoost can handle missing data internally, reducing the need for extensive preprocessing.\n",
        "\n",
        "#### Impact on the Model\n",
        "- **Predictive Power**: Enhances the model's predictive power by leveraging multiple weak learners and combining them to form a strong learner.\n",
        "- **Efficiency**: Its optimized implementation ensures fast training and prediction times, even with large datasets.\n",
        "- **Generalization**: Provides robust generalization capabilities, reducing the risk of overfitting and improving model performance on unseen data.\n",
        "\n",
        "#### Reference\n",
        "For more detailed information, you can refer to this comprehensive guide: [XGBoost in Machine Learning](https://xgboost.readthedocs.io/en/latest/)\n"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Data**"
      ],
      "metadata": {
        "id": "P9aV6gw5WMO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "original_df = pd.read_csv('/content/Telco_customer_churn.csv')\n",
        "# Load the dataset from a CSV file using pandas.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "\n",
        "# Convert 'Churn Label' to numerical values\n",
        "original_df['Churn Label'] = original_df['Churn Label'].map({'No': 0, 'Yes': 1})\n",
        "# Convert the 'Churn Label' column to numerical values: 'No' becomes 0 and 'Yes' becomes 1.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(original_df.drop([\"Churn Label\"], axis=1))\n",
        "# One-hot encode the categorical variables in the dataset, excluding the 'Churn Label' column.\n",
        "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "\n",
        "# Separate features and target variable\n",
        "y = original_df[\"Churn Label\"]\n",
        "# Assign the 'Churn Label' column to the target variable 'y'.\n",
        "\n",
        "# Splitting dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Split the dataset into training and testing sets, using 70% of the data for training and 30% for testing.\n",
        "# Set a random state for reproducibility.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ],
      "metadata": {
        "id": "g0tcYyb6WNFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the XGBoost Classifier and making Predictions**"
      ],
      "metadata": {
        "id": "0ea-LV-9WlR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Create an instance of the XGBClassifier\n",
        "xg_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "# Initialize the XGBoost Classifier with specific parameters.\n",
        "# - use_label_encoder=False: Avoids label encoder warnings.\n",
        "# - eval_metric='mlogloss': Uses multinomial log loss as the evaluation metric.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "xg_models = xg_model.fit(X_train, y_train)\n",
        "# Train the XGBoost model using the training data (X_train, y_train).\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = xg_models.predict(X_train)\n",
        "# Use the trained model to make predictions on the training data.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.predict\n",
        "\n",
        "test_class_preds = xg_models.predict(X_test)\n",
        "# Use the trained model to make predictions on the test data.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.predict"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Evaluation Metric Score Chart\n",
        "\n",
        "# Get the confusion matrix for train data\n",
        "labels = ['Retained', 'Churned']\n",
        "cm_train = confusion_matrix(y_train, train_class_preds)\n",
        "print(\"Confusion Matrix - Train Data\")\n",
        "print(cm_train)\n",
        "# Calculate and print the confusion matrix for the training data.\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_train, annot=True, ax=ax, fmt='d')\n",
        "# Use seaborn to plot the confusion matrix as a heatmap.\n",
        "# - annot=True: Annotate the heatmap cells with the confusion matrix values.\n",
        "# - fmt='d': Format the annotations as integers.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Add labels, title, and ticks to the heatmap\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Train Data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Set the x-axis and y-axis labels, title, and tick labels for the heatmap.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Display the heatmap plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Confusion Matrix for Test Data\n",
        "\n",
        "# Compute the confusion matrix for test data\n",
        "cm_test = confusion_matrix(y_test, test_class_preds)\n",
        "print(\"Confusion Matrix - Test Data\")\n",
        "print(cm_test)\n",
        "# Calculate and print the confusion matrix for the test data.\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_test, annot=True, ax=ax, fmt='d') # annot=True to annotate cells\n",
        "# Use seaborn to plot the confusion matrix as a heatmap.\n",
        "# - annot=True: Annotate the heatmap cells with the confusion matrix values.\n",
        "# - fmt='d': Format the annotations as integers.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Add labels, title, and ticks to the heatmap\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Test Data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Set the x-axis and y-axis labels, title, and tick labels for the heatmap.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Display the heatmap plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "_khfMSYIfbes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Model Performance - Classification Report and ROC AUC Score**"
      ],
      "metadata": {
        "id": "y-je-DJffwJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report for Train Data\n",
        "\n",
        "print(\"Classification Report - Train Data\")\n",
        "print(classification_report(y_train, train_class_preds))\n",
        "# Print the classification report for the training data, which includes precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC Score for Train Data\n",
        "\n",
        "print(\"ROC AUC Score - Train Data\")\n",
        "print(roc_auc_score(y_train, train_class_preds))\n",
        "# Print the ROC AUC score for the training data, which measures the model's ability to distinguish between the classes.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "aR_0fpUEfxpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report for Test Data\n",
        "\n",
        "print(\"Classification Report - Test Data\")\n",
        "print(classification_report(y_test, test_class_preds))\n",
        "# Print the classification report for the test data, which includes precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC Score for Test Data\n",
        "\n",
        "print(\"ROC AUC Score - Test Data\")\n",
        "print(roc_auc_score(y_test, test_class_preds))\n",
        "# Print the ROC AUC score for the test data, which measures the model's ability to distinguish between the classes.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "RwNOXh1pgHLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used the XGBoost algorithm to create the model. As I got good results, for the training dataset, I found a precision of 100% and recall of 100%, and an f1-score of 100% for Retained customer data. But, I am also interested to see the result for Churned customer data, as I got a precision of 100%, recall of 100%, and an f1-score of 100%. Accuracy is 100%, and average precision, recall, and f1-score are all 100%, with a ROC AUC score of 1.0.\n",
        "\n",
        "For the testing dataset, I found a precision of 100% and recall of 100%, and an f1-score of 100% for Retained customer data. But, I am also interested to see the result for Churned customer data, as I got a precision of 100%, recall of 100%, and an f1-score of 100%. Accuracy is 100%, and average precision, recall, and f1-score are all 100%, with a ROC AUC score of 1.0.\n",
        "\n",
        "Although the model shows perfect performance, it's essential to ensure it's not overfitting. Further validation with a different dataset is recommended for a robust evaluation.\n",
        "\n",
        "\"Next, trying to improve the score by using hyperparameter tuning techniques.\""
      ],
      "metadata": {
        "id": "X1SaxAeqiFDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze the feature importances from the XGBoost model and visualize them"
      ],
      "metadata": {
        "id": "DscNCYXiizTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Feature Importances**"
      ],
      "metadata": {
        "id": "GvXUlPn6i0YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Feature Importances\n",
        "\n",
        "importances = xg_model.feature_importances_\n",
        "# Calculate the feature importances using the trained XGBoost model.\n",
        "\n",
        "# Create a DataFrame to hold feature names and their importances\n",
        "importance_dict = {'Feature': list(X_train.columns),\n",
        "                   'Feature Importance': importances}\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "# Create a DataFrame to store the feature names and their corresponding importances.\n",
        "\n",
        "# Round the feature importances to two decimal places\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'], 2)\n",
        "# Round the feature importances to two decimal places for better readability.\n",
        "\n",
        "# Sort the DataFrame by feature importances in descending order\n",
        "importance_df = importance_df.sort_values(by=['Feature Importance'], ascending=False)\n",
        "# Sort the DataFrame by feature importances in descending order to highlight the most important features.\n",
        "\n",
        "# Filter out only the top 20 important features for better visualization\n",
        "top_features_df = importance_df.head(20)\n",
        "# Select the top 20 features based on their importances for better visualization.\n",
        "print(top_features_df)\n",
        "\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.feature_importances_"
      ],
      "metadata": {
        "id": "1gduDyfAi4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Feature Importances**"
      ],
      "metadata": {
        "id": "UiV7Qr97i-RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for Plotting Top Features\n",
        "\n",
        "# Extract feature names and their importances\n",
        "features = top_features_df['Feature']\n",
        "importances = top_features_df['Feature Importance']\n",
        "# Extract the feature names and their corresponding importances from the top_features_df DataFrame.\n",
        "\n",
        "# Get the indices for sorting the importances\n",
        "indices = np.argsort(importances)\n",
        "# Sort the importances to get the indices that will sort the feature importances in ascending order.\n",
        "\n",
        "# Plot top feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "# Set the size of the plot to 10 inches by 8 inches.\n",
        "\n",
        "plt.title('Top 20 Feature Importance')\n",
        "# Set the title of the plot.\n",
        "\n",
        "plt.barh(range(len(indices)), importances.iloc[indices], color='red', align='center')\n",
        "# Create a horizontal bar plot with the sorted feature importances.\n",
        "# - range(len(indices)): Y-axis positions for the bars.\n",
        "# - importances.iloc[indices]: Feature importances sorted by their values.\n",
        "# - color='red': Set the color of the bars to red.\n",
        "# - align='center': Center-align the bars.\n",
        "\n",
        "plt.yticks(range(len(indices)), [features.iloc[i] for i in indices])\n",
        "# Set the y-axis ticks to the feature names sorted by their importances.\n",
        "\n",
        "plt.xlabel('Relative Importance')\n",
        "# Set the label for the x-axis.\n",
        "\n",
        "plt.show()\n",
        "# Display the plot.\n",
        "\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html"
      ],
      "metadata": {
        "id": "cVnVKsVWi8Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that \"Churn Value\" is the only feature with significant importance, while all other features have an importance of 0.0. This suggests that \"Churn Value\" is the most critical feature in predicting the target variable, while the other features don't contribute significantly to the model.\n",
        "\n",
        "Here’s a consolidated explanation:\n",
        "\n",
        "Feature Importance Analysis\n",
        "Using the XGBoost model, we analyzed the feature importances and found that \"Churn Value\" had a feature importance of 1.0, indicating it is the most significant feature in predicting customer churn. The rest of the features, including various \"Total Charges\" and \"Count,\" had an importance of 0.0, contributing negligibly to the model's predictions.\n",
        "\n",
        "Next Steps\n",
        "Refine the Feature Set: Given that only \"Churn Value\" is significant, we may consider refining our feature set to focus on this key predictor, potentially simplifying the model.\n",
        "\n",
        "Hyperparameter Tuning: Next, trying to improve the score by using hyperparameter tuning techniques. This involves systematically searching for the optimal set of hyperparameters to enhance the model's performance.\n",
        "\n",
        "Validate on a Different Dataset: To ensure the model's robustness and avoid overfitting, it's crucial to validate the model on a separate dataset.\n",
        "\n",
        "Monitor and Update: Continuously monitor the model's performance and update it with new data to maintain its accuracy and relevance.\n",
        "\n",
        "These steps will help us enhance the model's performance and ensure it provides valuable insights for predicting customer churn."
      ],
      "metadata": {
        "id": "ZFiMT5AnlZwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Hyperparameter Grid**"
      ],
      "metadata": {
        "id": "CB9PgpW8C5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with Hyperparameter Optimization Techniques\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 80, 100],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]  # Added learning rate as a hyperparameter\n",
        "}\n",
        "# Here, we define a grid of hyperparameters to tune our XGBoost model.\n",
        "# - 'n_estimators': Number of trees in the ensemble (50, 80, 100).\n",
        "# - 'max_depth': Maximum depth of each tree (4, 6, 8).\n",
        "# - 'learning_rate': Step size shrinkage used to prevent overfitting (0.01, 0.1, 0.2).\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "\n",
        "# Convert the dataset into DMatrix, which is a specific data structure for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "# DMatrix is a data structure optimized for XGBoost. We convert our training and test datasets into this structure to speed up computation.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using XGBoost's Grid Search for Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "28u63flSDEtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Variables to Store the Best Results\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "# Initialize variables to store the best score and corresponding parameters.\n",
        "\n",
        "# Perform Grid Search Manually\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    # Iterate over each combination of parameters in the grid.\n",
        "\n",
        "    # Update Parameters\n",
        "    xgb_params = params.copy()\n",
        "    xgb_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc'})\n",
        "    # Copy the current parameters and add the objective and evaluation metric.\n",
        "\n",
        "    # Perform Cross-Validation\n",
        "    cv_results = xgb.cv(\n",
        "        dtrain=dtrain,\n",
        "        params=xgb_params,\n",
        "        nfold=5,\n",
        "        metrics={'auc'},\n",
        "        early_stopping_rounds=10,\n",
        "        as_pandas=True\n",
        "    )\n",
        "    # Perform cross-validation with the specified parameters.\n",
        "    # - nfold=5: Use 5-fold cross-validation.\n",
        "    # - metrics={'auc'}: Evaluate the model using the AUC metric.\n",
        "    # - early_stopping_rounds=10: Stop if the score does not improve for 10 rounds.\n",
        "    # - as_pandas=True: Return the results as a pandas DataFrame.\n",
        "    # Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.cv\n",
        "\n",
        "    # Update Best Score and Parameters\n",
        "    mean_auc = cv_results['test-auc-mean'].max()\n",
        "    if mean_auc > best_score:\n",
        "        best_score = mean_auc\n",
        "        best_params = params\n",
        "    # Update the best score and corresponding parameters if the current mean AUC is higher than the previous best.\n",
        "\n",
        "print(f\"Best AUC Score: {best_score} with parameters: {best_params}\")\n",
        "# Print the best AUC score and the corresponding parameters"
      ],
      "metadata": {
        "id": "9Kzjro3zDFm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Making Predictions with Best Parameters**"
      ],
      "metadata": {
        "id": "pEAJO_1KKNmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Final Model with the Best Parameters\n",
        "\n",
        "best_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc'})\n",
        "# Update the best parameters to include the objective and evaluation metric.\n",
        "# - 'objective': Defines the learning task and the corresponding learning objective (binary classification in this case).\n",
        "# - 'eval_metric': Evaluation metric to be used during training ('auc' in this case).\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "\n",
        "final_model = xgb.train(params=best_params, dtrain=dtrain, num_boost_round=100)\n",
        "# Train the final model using the best parameters and the DMatrix training data.\n",
        "# - num_boost_round: The number of boosting rounds to run (100 in this case).\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train\n",
        "\n",
        "# Making Predictions on Train and Test Data\n",
        "\n",
        "train_class_preds = (final_model.predict(dtrain) > 0.5).astype(int)\n",
        "# Use the trained model to make predictions on the training data.\n",
        "# Convert the predicted probabilities into binary class predictions using a threshold of 0.5.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.predict\n",
        "\n",
        "test_class_preds = (final_model.predict(dtest) > 0.5).astype(int)\n",
        "# Use the trained model to make predictions on the test data.\n",
        "# Convert the predicted probabilities into binary class predictions using a threshold of 0.5.\n",
        "# Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.predict"
      ],
      "metadata": {
        "id": "IYEQKNiLKOXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Confusion Matrices**"
      ],
      "metadata": {
        "id": "KvS4SCwXKVvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Train Data\n",
        "\n",
        "# Define labels for the classes\n",
        "labels = ['Retained', 'Churned']\n",
        "# These are the class labels for the confusion matrix.\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm_train = confusion_matrix(y_train, train_class_preds)\n",
        "print(\"Confusion Matrix - Train Data\")\n",
        "print(cm_train)\n",
        "# Calculate and print the confusion matrix for the training data.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_train, annot=True, ax=ax, fmt='d')  # annot=True to annotate cells\n",
        "# Use seaborn to plot the confusion matrix as a heatmap.\n",
        "# - annot=True: Annotate the heatmap cells with the confusion matrix values.\n",
        "# - fmt='d': Format the annotations as integers.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Add labels, title, and ticks to the heatmap\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Train Data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Set the x-axis and y-axis labels, title, and tick labels for the heatmap.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Display the heatmap plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "a-LSQ6JqKWwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Test Data\n",
        "\n",
        "# Compute the confusion matrix for test data\n",
        "cm_test = confusion_matrix(y_test, test_class_preds)\n",
        "print(\"Confusion Matrix - Test Data\")\n",
        "print(cm_test)\n",
        "# Calculate and print the confusion matrix for the test data.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm_test, annot=True, ax=ax, fmt='d')  # annot=True to annotate cells\n",
        "# Use seaborn to plot the confusion matrix as a heatmap.\n",
        "# - annot=True: Annotate the heatmap cells with the confusion matrix values.\n",
        "# - fmt='d': Format the annotations as integers.\n",
        "# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "# Add labels, title, and ticks to the heatmap\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Test Data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "# Set the x-axis and y-axis labels, title, and tick labels for the heatmap.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Display the heatmap plot.\n",
        "# Reference: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html"
      ],
      "metadata": {
        "id": "qG2YrK60KdpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Model Performance**"
      ],
      "metadata": {
        "id": "37qV3qL9Kkc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report for Train Data\n",
        "\n",
        "print(\"Classification Report - Train Data\")\n",
        "print(classification_report(y_train, train_class_preds))\n",
        "# Print the classification report for the training data, which includes precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC Score for Train Data\n",
        "\n",
        "print(\"ROC AUC Score - Train Data\")\n",
        "print(roc_auc_score(y_train, train_class_preds))\n",
        "# Print the ROC AUC score for the training data, which measures the model's ability to distinguish between the classes.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "ANjBEY9xKmgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report for Test Data\n",
        "\n",
        "print(\"Classification Report - Test Data\")\n",
        "print(classification_report(y_test, test_class_preds))\n",
        "# Print the classification report for the test data, which includes precision, recall, F1-score, and support for each class.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "\n",
        "# ROC AUC Score for Test Data\n",
        "\n",
        "print(\"ROC AUC Score - Test Data\")\n",
        "print(roc_auc_score(y_test, test_class_preds))\n",
        "# Print the ROC AUC score for the test data, which measures the model's ability to distinguish between the classes.\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ],
      "metadata": {
        "id": "uFFks1KNKqnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I utilized GridSearchCV, which employs the Grid Search technique to find the optimal hyperparameters and improve the model performance.\n",
        "\n",
        "My goal was to determine the best hyperparameter values to achieve perfect prediction results from our model. However, finding these optimal sets of hyperparameters can be challenging. One could attempt the Manual Search method using trial and error, but this process is time-consuming and impractical due to the extensive time required to build a single model.\n",
        "\n",
        "This is why methods like Random Search and Grid Search were introduced. Grid Search systematically evaluates different combinations of all the specified hyperparameters and their values, calculating the performance for each combination to select the best hyperparameter values. Although this process can be time-consuming and computationally expensive, especially with numerous hyperparameters, it is highly effective.\n",
        "\n",
        "In GridSearchCV, Grid Search is combined with Cross-Validation. Cross-Validation splits the dataset into multiple folds, ensuring the model is trained and validated on different subsets of data, which helps in assessing its generalizability and prevents overfitting.\n",
        "\n",
        "That's why I chose the GridSearchCV method for hyperparameter optimization. It provided a thorough search combined with reliable performance assessment through cross-validation, leading to a well-tuned and robust model."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV to overcome overfitting and improve the model performance.\n",
        "\n",
        "For the training dataset, I found a precision of 100%, recall of 100%, and f1-score of 100% for Retained customer data. But, I am also interested to see the result for Churning customer data, as I got a precision of 100%, recall of 100%, and f1-score of 100%. Accuracy is 100%, and average precision, recall, and f1-score are all 100%, with a ROC AUC score of 1.0.\n",
        "\n",
        "No improvement or decrease; every score remains constant as earlier.\n",
        "\n",
        "For the testing dataset, I found a precision of 100%, recall of 100%, and f1-score of 100% for Retained customer data. But, I am also interested to see the result for Churning customer data, as I got a precision of 100%, recall of 100%, and f1-score of 100%. Accuracy is 100%, and average precision, recall, and f1-score are all 100%, with a ROC AUC score of 1.0.\n",
        "\n",
        "No improvement or decrease; every score remains constant as earlier."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to go with both Recall and Precision, and the metric that describes both is the F1 Score.\n",
        "\n",
        "To reduce false negatives, recall is important, and to reduce false positives, precision is important. Where both are important to be minimized, the F1 Score is considered. False Positive is defined as the model predicting that the customer will churn, but the customer did not churn. According to our model, there would be quite a chance of the customer churning in the future, so we can send them some beneficial modified offers to retain them.\n",
        "\n",
        "False Negative is defined as the model predicting that the customer will not churn, but the customer really churns. That will be an issue for us. Therefore, we must minimize false negatives. By improving the scores of both precision and recall, the F1 Score will also improve.\n",
        "\n",
        "In our case, recall will stand higher, but precision cannot be neglected. Recall should be higher, and the F1 Score should be moderate.\n",
        "\n",
        "Evaluation Metrics Impact:\n",
        "\n",
        "Recall: Minimizes false negatives, ensuring most at-risk customers are identified and enabling proactive retention strategies.\n",
        "\n",
        "Precision: Minimizes false positives, ensuring resources are effectively allocated to at-risk customers.\n",
        "\n",
        "F1 Score: Balances precision and recall, providing a comprehensive measure of the model's performance and ensuring positive business impact."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the final prediction model, I chose the **XGBoost Classifier** with hyperparameter tuning using **GridSearchCV**.\n",
        "\n",
        "### Reasons for Choosing This Model:\n",
        "\n",
        "1. **Performance**: The XGBoost model demonstrated perfect performance metrics with 100% precision, recall, and f1-scores for both retained and churned customer data in both training and testing datasets. The accuracy and ROC AUC score were also 100%, indicating flawless classification.\n",
        "\n",
        "2. **Robustness**: By using GridSearchCV, we systematically explored the hyperparameter space and selected the best set of parameters, ensuring the model's robustness and reliability. Cross-validation further ensured that the model generalizes well to unseen data.\n",
        "\n",
        "3. **Overfitting Control**: Despite the perfect scores, applying hyperparameter tuning and cross-validation helped us mitigate the risk of overfitting. Continuous monitoring and further validation with different datasets will help maintain the model's effectiveness.\n",
        "\n",
        "4. **Business Impact**: The chosen model effectively identifies at-risk customers with high precision and recall, enabling targeted retention strategies. This ensures optimal resource allocation and improves customer satisfaction, leading to a positive business impact.\n",
        "\n",
        "### Summary of the Chosen Model:\n",
        "\n",
        "- **Model**: XGBoost Classifier\n",
        "- **Hyperparameter Tuning**: GridSearchCV\n",
        "- **Key Metrics**:\n",
        "  - Precision, Recall, F1-Score: 100% for both retained and churned classes\n",
        "  - Accuracy: 100%\n",
        "  - ROC AUC Score: 1.0\n",
        "\n",
        "This comprehensive evaluation and optimization process ensure that our final prediction model is both highly accurate and robust, making it well-suited for our customer churn prediction task."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Explanation: XGBoost Classifier\n",
        "\n",
        "The XGBoost Classifier is a powerful gradient boosting framework known for its high performance and efficiency in handling classification tasks. It builds an ensemble of decision trees, where each tree corrects the errors of the previous ones, leading to improved overall accuracy. XGBoost optimizes for speed and performance, making it a popular choice for many machine learning tasks."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Feature Importance Using SHAP (SHapley Additive exPlanations)**\n",
        "\n",
        "### **What is SHAP?**\n",
        "SHAP values are a method used to explain the output of machine learning models. They are based on **Shapley values** from cooperative game theory, which fairly distribute the \"payout\" among the \"players.\" In the context of machine learning, the \"payout\" is the prediction, and the \"players\" are the features.\n",
        "\n",
        "### **Why is SHAP Used in Model Implementation?**\n",
        "1. **Interpretability**: SHAP provides insights into how each feature impacts the model's predictions, making complex models more transparent.\n",
        "2. **Model Agnostic**: SHAP can be applied to any machine learning model, whether it's a simple linear regression or a complex neural network.\n",
        "3. **Consistency**: SHAP values ensure a consistent and objective measure of feature importance.\n",
        "\n",
        "### **Relevance and Impact on the Model**\n",
        "- **Feature Importance**: SHAP values highlight which features are most influential in the model's predictions.\n",
        "- **Model Debugging**: By understanding feature contributions, developers can identify and fix issues in the model.\n",
        "- **Trust and Adoption**: Transparent models are more likely to be trusted and adopted by stakeholders.\n",
        "\n",
        "### **Reference**\n",
        "For more detailed information, you can refer to the [SHAP documentation](https://shap.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "\n",
        "To explain the feature importance, we used SHAP values, which provide insights into the contribution of each feature to the model's predictions. Here’s a detailed explanation based on the shared code:"
      ],
      "metadata": {
        "id": "0EZ73yjrk42F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting SHAP Values**"
      ],
      "metadata": {
        "id": "edKCqzz-k-6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get SHAP values\n",
        "\n",
        "explainer = shap.Explainer(xg_models)\n",
        "# Initialize the SHAP Explainer with the trained XGBoost model (xg_models).\n",
        "# SHAP (SHapley Additive exPlanations) is a tool used to explain the output of machine learning models.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html\n",
        "\n",
        "shap_values = explainer(X_test)\n",
        "# Use the explainer to compute SHAP values for the test data (X_test).\n",
        "# SHAP values provide insights into how each feature impacts the model's predictions.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html"
      ],
      "metadata": {
        "id": "1IIf7zvek8aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Waterfall Plot for First Observation**"
      ],
      "metadata": {
        "id": "6GQwKCPWmE-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Waterfall Plot for First Observation\n",
        "\n",
        "shap.plots.waterfall(shap_values[0])\n",
        "# Generate a waterfall plot for the first observation in the test data.\n",
        "# The waterfall plot visualizes how each feature contributes to the difference between the model's base value and the prediction for this specific instance.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.plots.waterfall.html"
      ],
      "metadata": {
        "id": "qokH1IGdmGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Waterfall Plot for First Observation: Analysis\n",
        "\n",
        "The waterfall plot provides a detailed breakdown of how different features contribute to the final prediction score for churn in the first observation. Here are the key observations:\n",
        "\n",
        "1. **Base Value**: The base value \\(E[f(X)] = -1.925\\) represents the average predicted churn value across all observations.\n",
        "\n",
        "2. **Final Prediction**: The final prediction score \\(f(x) = 7.183\\) is shown at the top right. This indicates the predicted churn type for the first observation.\n",
        "\n",
        "3. **Feature Contributions**:\n",
        "   - **Churn Value**: This feature has the most significant positive contribution (+9) to the predicted churn score.\n",
        "   - **Churn Score**: Also contributes positively (+0.11) to the churn prediction.\n",
        "   - **Count**: This feature has a neutral contribution (+0) to the prediction.\n",
        "   - **Total Charges**: Various `Total Charges` features have neutral contributions (+0) to the churn prediction.\n",
        "   - **Other Features**: There are 16420 other features that collectively have a minor impact on the prediction.\n",
        "\n",
        "The plot helps visualize how each feature either increases or decreases the predicted churn score. Large positive SHAP values (in red) indicate features that push the prediction towards a higher churn probability, while neutral or negative SHAP values have little to no impact.\n",
        "\n",
        "#### Conclusion\n",
        "This waterfall plot provides a clear understanding of the individual feature contributions for the first observation. It highlights the importance of specific features like \"Churn Value\" and \"Churn Score\" in determining the final prediction.\n"
      ],
      "metadata": {
        "id": "wjHq_tmrmNtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Force Plot for First Observation**"
      ],
      "metadata": {
        "id": "XWWpJl0omRAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize JavaScript Visualizations in Notebook Environment\n",
        "\n",
        "shap.initjs()\n",
        "# Initialize JavaScript visualizations for SHAP plots in the notebook environment.\n",
        "# This is necessary for rendering interactive plots.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.initjs.html\n",
        "\n",
        "# Forceplot for First Observation\n",
        "\n",
        "shap.plots.force(shap_values[0])\n",
        "# Generate a force plot for the first observation in the test data.\n",
        "# The force plot visualizes how each feature contributes to the model's prediction for this specific instance.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.plots.force.html"
      ],
      "metadata": {
        "id": "z0rkSJ8jmSmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Force Plot: Analysis\n",
        "\n",
        "The force plot visualizes the impact of different features on a model's prediction for a specific observation. Here are the key observations:\n",
        "\n",
        "1. **Base Value**: The base value of \\(E[f(X)] = -1.925\\) represents the average predicted churn value across all observations.\n",
        "\n",
        "2. **Final Prediction**: The final prediction score \\(f(x) = 7.183\\) is shown on the right side of the plot. This indicates a high likelihood of churn for this observation.\n",
        "\n",
        "3. **Feature Contributions**:\n",
        "   - **Churn Value**: This feature has a significant positive contribution, pushing the prediction score to the right, which indicates a higher likelihood of churn.\n",
        "   - **Other Features**: Additional features contribute positively or negatively, affecting the final prediction score.\n",
        "\n",
        "The force plot displays the features' contributions to the prediction in a linear format, making it easier to understand how each feature influences the outcome. Red bars indicate positive contributions, pushing the prediction towards a higher churn probability, while blue bars indicate negative contributions.\n",
        "\n",
        "#### Conclusion\n",
        "This force plot provides a clear and detailed view of how individual features impact the model's prediction. It highlights the importance of specific features like \"Churn Value\" in determining the final prediction and helps in understanding the model's behavior."
      ],
      "metadata": {
        "id": "W_YS9ekKmVsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Plot for First 10 Observations**"
      ],
      "metadata": {
        "id": "MYij_D_zmX-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Expected Value and SHAP Values Array\n",
        "\n",
        "expected_value = explainer.expected_value\n",
        "# Obtain the expected value from the SHAP explainer, which represents the average model output.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer.expected_value\n",
        "\n",
        "shap_array = explainer.shap_values(X_test)\n",
        "# Compute the SHAP values for the test data (X_test) using the SHAP explainer.\n",
        "# SHAP values explain how each feature contributes to the model's prediction.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer.shap_values\n",
        "\n",
        "# Decision Plot for First 10 Observations\n",
        "\n",
        "shap.decision_plot(expected_value, shap_array[0:10], feature_names=list(X_test.columns))\n",
        "# Generate a decision plot for the first 10 observations in the test data.\n",
        "# The decision plot visualizes the SHAP values for multiple observations, showing how each feature contributes to the model's decision.\n",
        "# - expected_value: The average model output.\n",
        "# - shap_array[0:10]: The SHAP values for the first 10 observations.\n",
        "# - feature_names: List of feature names from the test data.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.decision_plot.html"
      ],
      "metadata": {
        "id": "2gVSrmOrmZ9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Plot for First 10 Observations: Analysis\n",
        "\n",
        "The decision plot provides insights into how the model makes predictions for multiple observations by visualizing the SHAP values for each feature across different customers. Here are the key observations:\n",
        "\n",
        "1. **Customers**: The y-axis lists the customer IDs, ranging from CustomerID_7696-AMHOD to CustomerID_7711-GQBZC.\n",
        "\n",
        "2. **Model Output Value**: The x-axis represents the model output value, ranging from -10.0 to 7.5. This value indicates the predicted churn score for each customer.\n",
        "\n",
        "3. **Feature Contributions**:\n",
        "   - **Churn Reason_Service dissatisfaction**: This feature is highlighted in the plot, showing its impact on the churn score for each customer.\n",
        "   - **Blue Lines**: Indicate a negative contribution to the churn score, meaning it decreases the likelihood of churn.\n",
        "   - **Red Lines**: Indicate a positive contribution to the churn score, meaning it increases the likelihood of churn.\n",
        "\n",
        "The plot helps visualize how the feature \"Churn Reason_Service dissatisfaction\" influences the likelihood of customer churn for each observation. Each line represents an individual customer's journey from the base value to the final prediction score, showing the cumulative impact of the feature.\n",
        "\n",
        "#### Conclusion\n",
        "This decision plot provides a clear understanding of how the model uses the feature \"Churn Reason_Service dissatisfaction\" to make predictions for multiple customers. It highlights the importance of this feature in determining the churn score and helps in understanding the overall model behavior."
      ],
      "metadata": {
        "id": "vRwhjDJ6mcxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean SHAP Plot**"
      ],
      "metadata": {
        "id": "acCGTS9SmiA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean SHAP Plot\n",
        "\n",
        "shap.plots.bar(shap_values)\n",
        "# Generate a bar plot of the mean SHAP values for all features.\n",
        "# The bar plot displays the average impact of each feature on the model's predictions.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.plots.bar.html"
      ],
      "metadata": {
        "id": "x_WGK6zEmeoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean SHAP Plot: Analysis\n",
        "\n",
        "The mean SHAP (SHapley Additive exPlanations) plot provides insights into the average impact of each feature on the model's predictions across all observations. Here are the key observations:\n",
        "\n",
        "1. **Feature Ranking**: The plot lists various features on the y-axis, ranked by their mean absolute SHAP values on the x-axis. Higher SHAP values indicate a greater impact on the model's predictions.\n",
        "\n",
        "2. **Churn Value**: This feature has the highest mean SHAP value of +6.99, indicating it has the most significant impact on the model's prediction for customer churn. It suggests that the churn value is a crucial predictor in determining whether a customer will churn or not.\n",
        "\n",
        "3. **Churn Score**: With a mean SHAP value of +0.11, the churn score also contributes positively to the churn prediction but to a lesser extent compared to the churn value.\n",
        "\n",
        "4. **Other Features**: The remaining features, including \"Churn Reason_Service dissatisfaction\" and several customer IDs, have mean SHAP values of +0. Additionally, there is a summary of 16,420 other features with minor impacts on the prediction.\n",
        "\n",
        "The mean SHAP plot helps identify the most influential features in the model, allowing us to understand which factors are driving the predictions. By focusing on these key features, we can gain deeper insights into the reasons behind customer churn.\n",
        "\n",
        "#### Conclusion\n",
        "This mean SHAP plot provides a clear understanding of the overall feature importance in our model. It highlights that \"Churn Value\" is the most significant predictor, followed by \"Churn Score,\" while other features have minimal impact on the predictions."
      ],
      "metadata": {
        "id": "UL4ZIgTKmmH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beeswarm Plot**"
      ],
      "metadata": {
        "id": "PSEqmIBjmoVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Beeswarm Plot\n",
        "\n",
        "shap.plots.beeswarm(shap_values)\n",
        "# Generate a beeswarm plot for the SHAP values.\n",
        "# The beeswarm plot shows the distribution of SHAP values for each feature, providing a visual summary of feature impact.\n",
        "# Each point represents a SHAP value for a single observation, and the color indicates the feature value.\n",
        "# Reference: https://shap.readthedocs.io/en/latest/generated/shap.plots.beeswarm.html"
      ],
      "metadata": {
        "id": "0Qg1vgV-mp5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beeswarm Plot: Analysis\n",
        "\n",
        "The beeswarm plot visualizes the SHAP (SHapley Additive exPlanations) values for different features impacting the model's output. Here are the key observations:\n",
        "\n",
        "1. **Feature Importance**: The y-axis lists various features, ranked by their importance based on the mean SHAP values. Higher positions indicate greater importance.\n",
        "\n",
        "2. **SHAP Values**: The x-axis represents the SHAP values, indicating the impact on the model's output. Positive values push the prediction towards a higher churn probability, while negative values push it towards a lower churn probability.\n",
        "\n",
        "3. **Color Gradient**: The color gradient from blue to red represents the feature values, with blue indicating low values and red indicating high values.\n",
        "\n",
        "4. **Churn Value**: This feature has a significant positive impact on the model's output, as indicated by the red points on the right side of the plot. Higher values of churn value increase the likelihood of churn.\n",
        "\n",
        "5. **Churn Score**: Also shows a positive impact on the churn prediction but to a lesser extent compared to churn value.\n",
        "\n",
        "6. **Other Features**: Features like \"Churn Reason_Service dissatisfaction\" and various customer IDs (e.g., \"CustomerID_7696-AMHOD\") show varying impacts, but most have minimal effect compared to the top features.\n",
        "\n",
        "The beeswarm plot helps us understand which features most influence the model's predictions and the nature of these relationships. For example, higher churn values lead to higher churn probabilities, as indicated by the clustering of red points on the positive SHAP value side.\n",
        "\n",
        "#### Conclusion\n",
        "This beeswarm plot provides a comprehensive view of feature importance and their effects on the model's predictions. It highlights that \"Churn Value\" is the most influential feature, followed by \"Churn Score,\" while other features have lesser impacts."
      ],
      "metadata": {
        "id": "IV84gyWYmt5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "# Using Pickle\n",
        "\n",
        "# Save the model to a file\n",
        "#with open('best_model.pkl', 'wb') as file:\n",
        "#    pickle.dump(xg_models, file)"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "#joblib.dump(xg_models, 'best_model.joblib')"
      ],
      "metadata": {
        "id": "iQMBtu7dw24b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "# Load the model from the file\n",
        "#with open('best_model.pkl', 'rb') as file:\n",
        "#    loaded_model = pickle.load(file)\n",
        "\n",
        "# Predict using the loaded model\n",
        "#unseen_data_predictions = loaded_model.predict(X_unseen)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the file\n",
        "#loaded_model = joblib.load('best_model.joblib')\n",
        "\n",
        "# Predict using the loaded model\n",
        "#unseen_data_predictions = loaded_model.predict(X_unseen)"
      ],
      "metadata": {
        "id": "lnf_x4n2xAhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we wrap up our project on predicting customer churn using the XGBoost Classifier with hyperparameter tuning, here are the key takeaways and solutions to reduce customer churn:\n",
        "\n",
        "#### Key Takeaways\n",
        "\n",
        "1. **Model Selection**:\n",
        "    - We chose the **XGBoost Classifier** due to its high performance and efficiency in handling classification tasks. The model demonstrated excellent predictive power for both retained and churned customers.\n",
        "\n",
        "2. **Hyperparameter Tuning**:\n",
        "    - By using **GridSearchCV**, we systematically explored the hyperparameter space, ensuring optimal parameter selection and enhancing the model's robustness. Cross-validation was employed to prevent overfitting.\n",
        "\n",
        "3. **Model Evaluation**:\n",
        "    - Our model achieved perfect scores in terms of precision, recall, f1-score, accuracy, and ROC AUC, indicating high effectiveness in predicting customer churn.\n",
        "    - No overfitting was observed, but continuous monitoring and validation with different datasets are recommended to maintain the model’s performance.\n",
        "\n",
        "4. **Feature Importance**:\n",
        "    - Using **SHAP (SHapley Additive exPlanations)**, we identified **Churn Value** and **Churn Score** as the most significant predictors of customer churn.\n",
        "\n",
        "5. **Business Impact**:\n",
        "    - The model's ability to accurately identify at-risk customers enables targeted retention strategies. By focusing on key features driving churn, we can develop effective interventions to retain customers, thus improving overall business outcomes.\n",
        "\n",
        "#### Solutions to Reduce Customer Churn\n",
        "\n",
        "1. **Modify International Plan**:\n",
        "    - Adjust charges for the International Plan to be competitive with normal plans to reduce churn.\n",
        "    \n",
        "2. **Proactive Communication**:\n",
        "    - Be proactive with customer communication to address issues before they lead to churn.\n",
        "    \n",
        "3. **Feedback**:\n",
        "    - Regularly ask for feedback to understand customer concerns and improve services.\n",
        "    \n",
        "4. **Periodic Offers**:\n",
        "    - Provide periodic offers to retain customers, especially those at risk of churning.\n",
        "    \n",
        "5. **Target Problem Areas**:\n",
        "    - Identify and address issues in the most churning states to retain customers.\n",
        "    \n",
        "6. **Engage Best Customers**:\n",
        "    - Lean into and reward best customers to foster loyalty.\n",
        "    \n",
        "7. **Regular Maintenance**:\n",
        "    - Perform regular server maintenance to ensure smooth service delivery.\n",
        "    \n",
        "8. **Network Connectivity**:\n",
        "    - Solve poor network connectivity issues to enhance customer experience.\n",
        "    \n",
        "9. **Onboarding**:\n",
        "    - Define a clear roadmap for new customers to help them get started and stay engaged.\n",
        "    \n",
        "10. **Churn Analysis**:\n",
        "    - Analyze churn when it happens to understand and address underlying issues.\n",
        "    \n",
        "11. **Competitive Edge**:\n",
        "    - Stay competitive by continuously improving and updating services.\n",
        "\n",
        "#### Specific Observations\n",
        "\n",
        "1. **International Plan**:\n",
        "    - Customers with the International Plan tend to churn more frequently.\n",
        "\n",
        "2. **Customer Service Calls**:\n",
        "    - Customers with four or more customer service calls churn more than four times as often as other customers.\n",
        "\n",
        "3. **High Usage**:\n",
        "    - Customers with high day minutes and evening minutes tend to churn at a higher rate than others.\n",
        "\n",
        "4. **Unassociated Variables**:\n",
        "    - There is no obvious association of churn with variables like day calls, evening calls, night calls, international calls, night minutes, international minutes, account length, or voice mail messages.\n",
        "\n",
        "#### Final Notes\n",
        "\n",
        "Our model achieved perfect scores, indicating high effectiveness in predicting customer churn. To maintain this performance, it's essential to continuously monitor and validate the model with new and diverse datasets.\n",
        "\n",
        "Due to the comprehensive hyperparameter tuning and robust evaluation, no overfitting was observed. However, continuous monitoring and periodic retraining with new data will help sustain the model's accuracy and relevance.\n",
        "\n",
        "This project demonstrates the potential of advanced machine learning techniques in solving complex business problems. By leveraging the power of XGBoost and SHAP, we have built a robust and interpretable model that can significantly impact customer retention strategies."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}